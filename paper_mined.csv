,FileName,Abstract,Introduction,Conclusion
0,2015-A Multi-Modal Neuro-Physiological Study of Phishing Detection and Malware Warnings.pdf, detecting phishing attacks (identifying fake vs.real websites) and heeding security warnings represent classical user-centered secu- rity tasks subjected to a series of prior investigations.however  our understanding of user behavior underlying these tasks is still not fully mature  motivating further work concentrating at the neuro- physiological level governing the human processing of such tasks.we pursue a comprehensive three-dimensional study of phishing detection and malware warnings  focusing not only on what users’ task performance is but also on how users process these tasks based on: (1) neural activity captured using electroencephalogram (eeg) cognitive metrics  and (2) eye gaze patterns captured using an eye- tracker.our primary novelty lies in employing multi-modal neuro- physiological measures in a single study and providing a near re- alistic set-up (in contrast to a recent neuro-study conducted inside an fmri scanner).our work serves to advance  extend and sup- port prior knowledge in several signiﬁcant ways.speciﬁcally  in the context of phishing detection  we show that users do not spend enough time analyzing key phishing indicators and often fail at de- tecting these attacks  although they may be mentally engaged in the task and subconsciously processing real sites differently from fake sites.in the malware warning tasks  in contrast  we show that users are frequently reading  possibly comprehending  and eventu- ally heeding the message embedded in the warning.our study provides an initial foundation for building future mech- anisms based on the studied real-time neural and eye gaze features  that can automatically infer a user’s “alertness” state  and determine whether or not the user’s response should be relied upon.∗work done while being a student at uab  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full cita- tion on the ﬁrst page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or re- publish  to post on servers or to redistribute to lists  requires prior speciﬁc permission and/or a fee.request permissions from permissions@acm.org.ccs’15  october 12–16  2015  denver  colorado  usa.© 2015 acm.isbn 978-1-4503-3832-5/15/10 $15.00.doi: http://dx.doi.org/10.1145/2810103.2813660.categories and subject descriptors k.4.1 [computer and society]: public policy issues—privacy; d.4.6 [operating system]: security and protection—authentica- tion; h.1.2 [information systems]: user/machine systems —hu- man factors; human information processing general terms security and privacy  human-centered computing keywords phishing detection; malware warnings; eeg; eye tracking; neu- roscience 1,  cyber-security is undoubtedly a topic of national importance.while some cyber-attacks exploit the ﬂaws in the system design or implementation itself  others are successful due to the potential negligence or mistakes of end users.this latter aspect of com- puter systems security  commonly referred to as “user-centered se- curity ” forms the central focus of our work.there exists a num- ber of attacks and vulnerabilities underlying user-centered security systems.for example  users are frequently subject to phishing at- tacks (i.e.presented with malicious websites which may look very similar to real websites)  which they may fail to detect  eventually undermining the privacy of their sensitive information.similarly  warnings are regularly shown to users in order to alert them against potential security risks (e.g.while connecting to a potentially mali- cious site)  which they may not read or comprehend  or may simply ignore.there exists a large body of recent literature focusing on user- centered security (e.g.[9  17  18  , and future work we pursued a triangular study of phishing detection and malware warnings  measuring users’ neural activity  eye gaze patterns  task performance  and inter-relationships thereof.in the realm of phish- ing detection  our results showed that users do not spend enough time looking at key phishing indicators and often fail at detecting these attacks  although they may be highly engaged in the task and subconsciously processing real sites differently than fake sites.in the malware warning tasks  on the other hand  our results demon- strated that users frequently read and eventually heed the message embedded within the warning.we also found that a user’s person- ality traits (speciﬁcally  attention control) directly impact his/her phishing detection accuracy.this suggests that users may detect phishing attacks better if they could be trained to exercise attention control (beyond phishing awareness training).further work is nec- essary to understand the effect of such interventional training on users’ performance in the phishing detection task.based on our work  we suggested the possibility of building fu- ture automated mechanisms applying a fusion of real-time neural and eye gaze features that can infer users’ “alertness” state  and de- termine whether or not users’ responses should be relied upon.the proposed mechanism may be used to “sanitize” a user’s response and enhance the credibility of human decisions in a user-centered security system  serving as a closed-loop between humans and ma- chines.future research is needed to design and validate such mech- anisms in different security domains.acknowledgments the authors thank: cali fidopiastis and pankaj mahajan for their help with data collection  rajesh kana for his feedback in the early stages of this work  john sloan for feedback on a previous draft  and the abm team  especially stephanie korszen  for valuable guidance on eeg data analysis.we also thank konstantin (kosta) beznosov (our shepherd) and ccs’15 anonymous reviewers for their constructive comments and guidance.11
1,2015-Weakly Supervised Extraction of Computer Security Events from Twitter.pdf, twitter contains a wealth of timely information  however staying on top of breaking events requires that an informa- tion analyst constantly scan many sources  leading to infor- mation overload.for example  a user might wish to be made aware whenever an infectious disease outbreak takes place  when a new smartphone is announced or when a distributed denial of service (dos) attack might aﬀect an organization’s network connectivity.there are many possible event cate- gories an analyst may wish to track  making it impossible to anticipate all those of interest in advance.we therefore propose a weakly supervised approach  in which extractors for new categories of events are easy to deﬁne and train  by specifying a small number of seed examples.we cast seed- based event extraction as a learning problem where only pos- itive and unlabeled data is available.rather than assuming unlabeled instances are negative  as is common in previ- ous work  we propose a learning objective which regular- izes the label distribution towards a user-provided expecta- tion.our approach greatly outperforms heuristic negatives  used in most previous work  in experiments on real-world data.signiﬁcant performance gains are also demonstrated over two novel and competitive baselines: semi-supervised em and one-class support-vector machines.we investigate three security-related events breaking on twitter: dos at- tacks  data breaches and account hijacking.a demonstration of security events extracted by our sys-  tem is available at: http://kb1.cse.ohio-state.edu:8123/events/hacked  ∗this work was conducted at carnegie mellon  copyright is held by the international world wide web conference com- mittee (iw3c2).iw3c2 reserves the right to provide a hyperlink to the author’s site if the material is used in electronic media.www 2015  may 18–22  2015  florence  italy.acm 978-1-4503-3469-3/15/05.http://dx.doi.org/10.1145/2736277.2741083 .categories and subject descriptors i.2.7 [natural language processing]: language pars- ing and understanding; h.2.8 [database management]: database applications—data mining general terms algorithms  experimentation 1,  social media presents a rich and timely source of informa- tion on events taking place in the world  enabling applica- tions such as earthquake detection [41] or identifying the lo- cation of missing persons during natural disasters [,  motivated by the wide variety of event categories which might be of interest to track  we proposed a weakly super- vised seed-based approach to event extraction from twitter.we showed how this leads to an unusual learning problem where only a small number of positive seeds and a sam- ple of unlabeled candidate events are available.a number of approaches were investigated to address this challenge  including label regularization  constrained semi-supervised em and one-class svms.we applied this approach to de- tect several security-related events including dos attacks and account hijacking incidents  and demonstrated that a large number of security-related events are mentioned on twitter.9.acknowledgments  this research has been supported in part by darpa (un- der contract number fa8750-13-2-0005) and the depart- ment of defense under contract no.fa8721-05-c-0003 with carnegie mellon university for the operation of the software engineering institute  a federally funded research and devel- opment center.10
2,2016-A Simple Generic Attack on Text Captchas.pdf,—text-based captchas have been widely deployed across the internet to defend against undesirable or malicious bot programs.many attacks have been proposed; these ﬁne prior art advanced the scientiﬁc understanding of captcha robustness  but most of them have a limited applicability.in this paper  we report a simple  low-cost but powerful attack that effectively breaks a wide range of text captchas with distinct design features  including those deployed by google  microsoft  yahoo!  amazon and other internet giants.for all the schemes  our attack achieved a success rate ranging from 5% to 77%  and achieved an average speed of solving a puzzle in less than 15 seconds on a standard desktop computer (with a 3.3ghz intel core i3 cpu and 2 gb ram).this is to date the simplest generic attack on text captchas.our attack is based on log-gabor ﬁlters; a famed application of gabor ﬁlters in computer security is john daugman’s iris recognition algorithm.our work is the ﬁrst to apply gabor ﬁlters for breaking captchas.i,  captcha allows websites to automatically distinguish com- puters from humans.this technology  in particular text-based captchas  has been widely deployed on the internet to curb abuses introduced by automated computer programs mas- querading as human beings.although many text captchas have been broken  the most recent studies  such as one by a uc berkeley team [,
3,2016-Attack Patterns for Black-Box Security Testing of Multi-Party Web Applications.pdf,—the advent of software-as-a-service (saas) has led to the development of multi-party web applications (mpwas).mpwas rely on core trusted third-party systems (e.g.payment servers  identity providers) and protocols such as cashier-as-a- service (caas)  single sign-on (sso) to deliver business services to users.motivated by the large number of attacks discovered against mpwas and by the lack of a single general-purpose application-agnostic technique to support their discovery  we propose an automatic technique based on attack patterns for black-box  security testing of mpwas.our approach stems from the observation that attacks against popular mpwas share a number of similarities  even if the underlying protocols and services are different.in this paper  we target six different replay attacks  a login csrf attack and a persistent xss attack.firstly  we propose a methodology in which security experts can create attack patterns from known attacks.secondly  we present a security testing framework that leverages attack patterns to automatically generate test cases for testing the security of mpwas.we implemented our ideas on top of owasp zap (a popular  open-source penetration testing tool)  created seven attack patterns that correspond to thirteen prominent attacks from the literature and discovered twenty one previously unknown vulnerabilities in prominent mpwas (e.g.twitter.com  developer.linkedin.com  pinterest.com)  including mpwas that do not belong to sso and caas families.i,  an increasing number of business critical  online applica- tions leverage trusted third parties in conjunction with web- based security protocols to meet their security needs.for instance  many online applications rely on authentication as- sertions issued by identity providers to authenticate users using a variety of web-based single sign-on (sso) protocols (e.g.saml sso v,  we presented an approach for black-box security testing of mpwas.the core of our approach is the concept of application-agnostic attack patterns.these attack patterns are inspired by the similarities in the attack strategies of the previ- ously discovered attacks against mpwas.the implementation of our approach is based on owasp zap  a widely-used open- source legacy penetration testing tool.by using our approach  we have been able to identify serious drawbacks in the sso and caas solutions offered by linkedin  paypal and stripe  previously unknown vulnerabilities in a number of websites leveraging the sso solutions offered by facebook and in- stagram and automatically generate test cases that reproduce previously known attacks against vulnerable integration of the 2checkout service.acknowledgment  this work has been partly supported by the eu under grant  317387 secentis (fp7-people-2012-itn)
4,2016-AUROR- Defending Against Poisoning Attacks in Collaborative Deep Learning Systems.pdf, deep learning in a collaborative setting is emerging as a corner- stone of many upcoming applications  wherein untrusted users col- laborate to generate more accurate models.from the security per- spective  this opens collaborative deep learning to poisoning at- tacks  wherein adversarial users deliberately alter their inputs to mis-train the model.these attacks are known for machine learning systems in general  but their impact on new deep learning systems is not well-established.we investigate the setting of indirect collaborative deep learning — a form of practical deep learning wherein users submit masked features rather than direct data.indirect collaborative deep learn- ing is preferred over direct  because it distributes the cost of com- putation and can be made privacy-preserving.in this paper  we study the susceptibility of collaborative deep learning systems to adversarial poisoning attacks.speciﬁcally  we obtain the follow- ing empirical results on 2 popular datasets for handwritten images (mnist) and trafﬁc signs (gtsrb) used in auto-driving cars.for collaborative deep learning systems  we demonstrate that the at- tacks have 99% success rate for misclassifying speciﬁc target data while poisoning only 10% of the entire training dataset.as a defense  we propose auror  a system that detects mali- cious users and generates an accurate model.the accuracy un- der the deployed defense on practical datasets is nearly unchanged when operating in the absence of attacks.the accuracy of a model trained using auror drops by only 3% even when 30% of all the users are adversarial.auror provides a strong guarantee against evasion; if the attacker tries to evade  its attack effectiveness is bounded.1,  deep learning techniques have brought a paradigm shift in data driven applications  from speech recognition (e.g.apple’s siri [4]  google now [7]  microsoft’s cortana [6] and recently facebook m [3])  to image identiﬁcation (e.g.google’s photos [5]  face- book’s moments [,
5,2016-Cracking Classifiers for Evasion- A Case Study on the Google_s Phishing Pages Filter.pdf,  various classifiers based on the machine learning techniques have  been widely used in security applications.meanwhile  they also  became  an  attack  target  of  adversaries.many  existing  studies  have  paid  much  attention  to  the  evasion  attacks  on  the  online  classifiers  and  discussed  defensive  methods.however   the  security of the classifiers deployed in the client environment has  not got the attention it deserves.besides  earlier studies have only  concentrated  on  the  experimental  classifiers  developed  for  research purposes only.the security of widely-used commercial  classifiers still remains unclear.in this paper  we use the google’s  phishing pages filter (gppf)  a classifier deployed in the chrome  browser and with over one billion users  as a case to investigate  the security challenges for the client-side classifiers.a new attack  methodology  targeted  to  client-side  classifiers   called  classifiers  cracking   is  presented.according  to  the  methodology   we  successfully  crack  the  classification  model  of  gppf  and  extract  sufficient  knowledge  from  it  for  performing  effective  evasion  attacks   including  the  classification  algorithm   scoring  rules  and  features   etc.most  importantly   we  completely  reverse  engineer  84.8% scoring rules  covering most of high-weighted rules.based  on  the  cracked  information   we  perform  two  kinds  of  evasion  attacks  to  gppf   using  100  real  phishing  pages  as  the  target  of  evaluation.the  experiments  show  that  all  the  phishing  pages  (100%)  can  be  easily  manipulated  to  bypass  the  detection  of  gppf.our  study  demonstrates  that  the  existing  client-side  classifiers are very vulnerable to classifiers cracking attacks.categories and subject descriptors  d.4.6  [artificial intelligence]: learning   [security  and  protection]:   invasive  software;   i.2   general terms  security   keywords  phishing  detection   machine  learning   classifiers   cracking   collision attacks  evasion attacks   1,  machine  learning  techniques  have  been  commonly  adopted  in  security  applications.various  classifiers  were  trained  for  detecting  malicious  web  pages  [,  in this paper  we presented a new attack methodology  classifier  cracking   for  evading  the  client-side  classifier.our  approach  is  different  from  existing  attack  methods  is  that  various  reverse  engineering techniques are leveraged to directly extract desirable  knowledge  from  client-side  classifier  for  launching  evasion  attacks.our study took gppf  a learning-based filter for phishing  pages deployed in chrome as a case to study  which owns over  one  billion  users.employing  various  reverse  engineering  techniques   we  successfully  crack  the  gppf  model  and  completely  recovered  84.8%  encrypted  scoring  rules.based  on  the information  we developed two kinds of evasion attacks: good  features  insertion  and  bad  features  elimination.the  latest  100  real phishing pages collected from phishtank were taken as the  target of evaluation.the attack experiments showed that we can  easily  manipulate  all  the  phishing  pages  (100%)  to  make  them  successfully evade the detection of gppf in the latest version of  chrome.additionally   a  potential  defense  strategy  was  also  discussed.we  believe  that  the  deep  learning  method  can  be  employed to build client-side classifiers for essentially increasing  the complexity of cracking.our  research  revealed  an  important  fact  that  the  client-side  classifiers have a larger attack surface and hence larger number of  potential attacks.in the future  we will further research potential  defense techniques  especially based on the deep learning method   to develop more robust client-side classifier framework.9.acknowledgments  the  authors  would  like  to  thank  the  anonymous  reviewers  for  their  insightful  comments.the  work  is  supported  by  national  natural  science  foundation  of  china  (nsfc)  under  grants  61170240   91418206  and  61472429   and  national  science  and  technology  major  project  of  china  under  grant  2012zx01039- 004.10
6,2016-HERCULE- Attack Story Reconstruction via Community Discovery on Correlated Log Graph.pdf, advanced cyber attacks consist of multiple stages aimed at being stealthy and elusive.such attack patterns leave their footprints spatio-temporally dispersed across many diﬀer- ent logs in victim machines.however  existing log-mining intrusion analysis systems typically target only a single type of log to discover evidence of an attack and therefore fail to exploit fundamental inter-log connections.the output of such single-log analysis can hardly reveal the complete attack story for complex  multi-stage attacks.additionally  some existing approaches require heavyweight system instrumenta- tion  which makes them impractical to deploy in real produc- tion environments.to address these problems  we present hercule  an automated multi-stage log-based intrusion analysis system.inspired by graph analytics research in social network analysis  we model multi-stage intrusion anal- ysis as a community discovery problem.hercule builds multi-dimensional weighted graphs by correlating log entries across multiple lightweight logs that are readily available on commodity systems.from these  hercule discovers any “attack communities” embedded within the graphs.our eval- uation with 15 well known apt attack families demonstrates that hercule can reconstruct attack behaviors from a spectrum of cyber attacks that involve multiple stages with high accuracy and low false positive rates.1,  emerging cyber attack campaigns (e.g.enterprise-wide apt) exhibit “low-and-slow” attack patterns: attackers con- duct reconnaissance and move laterally within a network via many stealthy multi-stage payloads.one annual report published by fireeye [18] highlighted that attackers may reside in a victim’s environment for up to ,
7,2016-PhishEye- Live Monitoring of Sandboxed Phishing Kits.pdf, phishing is a form of online identity theft that deceives un- aware users into disclosing their conﬁdential information.while signiﬁcant eﬀort has been devoted to the mitigation of phishing attacks  much less is known about the entire life-cycle of these attacks in the wild  which constitutes  however  a main step toward devising comprehensive anti- phishing techniques.in this paper  we present a novel ap- proach to sandbox live phishing kits that completely protects the privacy of victims.by using this technique  we perform a comprehensive real-world assessment of phishing attacks  their mechanisms  and the behavior of the criminals  their victims  and the security community involved in the process – based on data collected over a period of ﬁve months.our infrastructure allowed us to draw the ﬁrst compre- hensive picture of a phishing attack  from the time in which the attacker installs and tests the phishing pages on a com- promised host  until the last interaction with real victims and with security researchers.our study presents accurate measurements of the duration and eﬀectiveness of this pop- ular threat  and discusses many new and interesting aspects we observed by monitoring hundreds of phishing campaigns.1,  despite the large eﬀort and the numerous solutions pro- posed by the security community  phishing attacks remain today one of the main threats on the internet [1].they usually aim at deceiving users into visiting fake web pages that mimic the graphic appearance of real and authentic websites [18].the main goal of an attacker  also known as phisher  is to collect sensitive user data such as login cre- dentials  banking information  or credit cards numbers.the stolen data can then be monetized by leveraging hijacked ac- counts and performing fraudulent online transactions  or in- directly through the resale of the stolen information to other cyber-criminals  mostly on the internet black market [16].phishing attacks constitute a major challenge for inter- net service providers (isps)  as well as for email providers   permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page.copyrights for components of this work owned by others than the author(s) must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior speciﬁc permission and/or a fee.request permissions from permissions@acm.org.ccs’16  october ,  in this paper we present the design and implementation of a honeypot system especially designed to analyze and disarm phishing kits.using this infrastructure  we conducted a ﬁve- month experiment to understand and measure the entire life cycle of this type of attack.diﬀerent from previous works  our approach is able to measure the eﬀective lifetime of phishing kits  which starts immediately after the kit installation.we are also the ﬁrst to clearly distinguish the victims from the attackers and the other third party visitors.our results show that less vic- tims divulge their credentials compared to previous studies conducted in 2009 [39]  maybe due to an increased user ed- ucation in the past seven years against this threat.9.acknowledgments  we would like to thank the reviewers for their valuable comments that allowed us to improve the quality of this pa- per.this research was partly funded by the french ministry of education and research under cifre grant given to xiao han  and by the european union’s horizon 2020 project supercloud under grant agreement 643964.10
8,2016-POSTER- Phishing Website Detection with a Multiphase Framework to Find Visual Similarity.pdf, most phishing pages try to convince users that they are legitimate sites by imitating visual signals like logos from the websites they are targeting.visual similarity detection methods look for these imitations between the screen-shots of the suspect pages and an image database of the most tar- geted websites.existing approaches  however  are either too slow for real-time use or not robust to manipulation.in this work  we design a multi-phase framework for visual similar- ity detection.the ﬁrst phase of the framework should rule out the bulk of websites quickly  but without introducing false negatives and with resistance to attacker manipula- tions.later phases can use more heavyweight operations to decide whether or not to warn the user about possible phishing.in this abstract  we focus on the ﬁrst phase.in experiments  our proposed method rules out more than half of the test cases with zero false negatives with less than 5 ms of processing time per page.keywords web security  phishing  visual similarity  1,  in the us alone  hundreds of millions of dollars are lost to phishing attacks each year [4].thus  despite substantial research into anti-phishing mechanisms  attackers continue to ﬁnd the attacks to be successful and proﬁtable.while a variety of information could be used to detect phishing sites  such as dns whois records and ip address of the server  much of it can be manipulated by an intel- ligent adversary.one aspect that is harder to manipulate  however  is the visual branding that helps the attacker con- vince the user that the site is the legitimate target site  such as a particular online bank.without these visual cues  or with heavily modiﬁed cues  the user may ﬁnd the site to be suspicious  and the attacker’s success rate will be lower.thus  a well-studied approach to combating phishing at- tacks is to look for visual similarity between an unknown  permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page.copyrights for third-party components of this work must be honored.for all other uses  contact the owner/author(s).ccs’16 october ,
9,2016-Real-Time Detection of Malware Downloads via Large-Scale URL -_ File -_ Machine Graph Mining  .pdf,  in this paper we propose mastino  a novel defense system to detect malware   download events.a download event is a 3-tuple that identies the action of   downloading  a  file  from  a  url  that  was  triggered  by  a  client  (machine).mastino  utilizes  global  situation  awareness  and  continuously  monitors   various network- and system-level events of the clients’ machines across   the internet and provides real time classification of both files and urls to   the clients upon submission of a new  unknown file or url to the system.to   enable detection of the download events  mastino builds a large download   graph that captures the subtle relationships among the entities of download   events  i.e.files  urls  and machines.we implemented a prototype version   of  mastino  and  evaluated  it  in  a  large-scale  real-world  deployment.our   experimental evaluation shows that mastino can accurately classify malware   download events with an average of 95.5% true positive (tp)  while incurring   less than 0.5% false positives (fp).in addition  we show the mastino can   classify a new download event as either benign or malware in just a fraction   of a second  and is therefore suitable as a real time defense system.1,  6  overview  9  system details  13  experimental setup  18  evaluation  ,
10,2017-A formal foundation for secure remote execution of enclaves.pdf, recent proposals for trusted hardware platforms  such as intel sgx and the mit sanctum processor  offer compelling security features but lack formal guarantees.we introduce a verification method- ology based on a trusted abstract platform (tap)  a formalization of idealized enclave platforms along with a parameterized adver- sary.we also formalize the notion of secure remote execution and present machine-checked proofs showing that the tap satisfies the three key security properties that entail secure remote execution: integrity  confidentiality and secure measurement.we then present machine-checked proofs showing that sgx and sanctum are refine- ments of the tap under certain parameterizations of the adversary  demonstrating that these systems implement secure enclaves for the stated adversary models.ccs concepts • security and privacy → formal methods and theory of se- curity; security in hardware; trusted computing; informa- tion flow control;  keywords enclave programs; secure computation; formal verification; con- fidentiality; integrity; remote attestation  1, an application executing on a typical computing platform con- tains large privileged software layers in its trusted computing base (tcb)  which includes the operating system (os)  hypervisor and firmware.vulnerabilities in these privileged software layers have been exploited to enable the execution of privileged malware with disastrous consequences [1  ,
11,2017-Authenticated Garbling and Efficient Maliciously Secure Two-Party Computation.pdf, we propose a simple and efficient framework for obtaining efficient constant-round protocols for maliciously secure two-party compu- tation.our framework uses a function-independent preprocessing phase to generate authenticated information for the two parties; this information is then used to construct a single “authenticated” garbled circuit which is transmitted and evaluated.we also show how to efficiently instantiate the preprocessing phase with a new  highly optimized version of the tinyot protocol by nielsen et al.our protocol outperforms existing work in both the single- execution and amortized settings  with or without preprocessing: • in the single-execution setting  our protocol evaluates an aes circuit with malicious security in 37 ms with an online time of 1 ms.previous work with the best overall time re- quires 62 ms (with 14 ms online time); previous work with the best online time (also 1 ms) requires 124 ms overall.• if we amortize over 1024 executions  each aes computation requires just 6.7 ms with roughly the same online time as above.the best previous work in the amortized setting has roughly the same total time but does not support function- independent preprocessing.our work shows that the performance penalty for maliciously se- cure two-party computation (as compared to semi-honest security) is much smaller than previously believed.ccs concepts • theory of computation → cryptographic protocols;  keywords two-party computation; secure computation; garbled circuit  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than the author(s) must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.ccs ’17  october 30-november 3  2017  dallas  tx  usa © 2017 copyright held by the owner/author(s).publication rights licensed to associa- tion for computing machinery.acm isbn 978-1-4503-4946-8/17/10...$15.00 https://doi.org/10.1145/3133956.3134053  1, protocols for secure two-party computation (,
12,"2017-Data Breaches, Phishing, or Malware_ Understanding the Risks of Stolen Credentials.pdf", in this paper  we present the first longitudinal measurement study of the underground ecosystem fueling credential theft and assess the risk it poses to millions of users.over the course of march  2016–march  2017  we identify 788 000 potential victims of off-the- shelf keyloggers; 12.4 million potential victims of phishing kits; and 1.9 billion usernames and passwords exposed via data breaches and traded on blackmarket forums.using this dataset  we explore to what degree the stolen passwords—which originate from thousands of online services—enable an attacker to obtain a victim’s valid email credentials—and thus complete control of their online iden- tity due to transitive trust.drawing upon google as a case study  we find 7–25% of exposed passwords match a victim’s google ac- count.for these accounts  we show how hardening authentication mechanisms to include additional risk signals such as a user’s his- torical geolocations and device profiles helps to mitigate the risk of hijacking.beyond these risk metrics  we delve into the global reach of the miscreants involved in credential theft and the blackhat tools they rely on.we observe a remarkable lack of external pressure on bad actors  with phishing kit playbooks and keylogger capabilities remaining largely unchanged since the mid-2000s.1, as the digital footprint of internet users expands to encompass social networks  financial records  and data stored in the cloud  often a single account underpins the security of this entire identity— an email address.this root of trust is jeopardized by the exposure of a victim’s email password or recovery questions.once subverted  a hijacker can reset a victim’s passwords to other services as a stepping stone attack; download all of the victim’s private data; remotely wipe the victim’s data and backups; or impersonate the victim to spew out spam or worse.highly visible hijacking incidents include attacks on journalists such as mat honan and the associated press [,
13,2017-How Effective is Anti-Phishing Training for Children_.pdf, user training is a commonly used method for preventing victimization from phishing attacks.in this study  we focus on training children  since they are active online but often overlooked in interventions.we present an experiment in which children at dutch primary schools received an anti- phishing training.the subjects were subsequently tested for their ability to distinguish phishing from non-phishing.a control group was used to control for external eﬀects.fur- thermore  the subjects received a re-test after several weeks to measure how well the children retained the training.the training improved the children’s overall score by 14%.the improvement was mostly caused by an increased score on the questions where they had to detect phishing.the score on recognizing legitimate emails was not aﬀected by the train- ing.we found that the improved phishing score returned to pre-training levels after four weeks.conversely  the score of recognition of legitimate emails increased over time.af- ter four weeks  trained pupils scored signiﬁcantly better in recognizing legitimate emails than their untrained counter- parts.age had a positive eﬀect on the score (i.e.older children scored higher than younger ones); but sex had no signiﬁcant inﬂuence.in conclusion  educating children to improve their ability to detect phishing works in the short term only.however  children go to school regularly  making it easier to educate them than adults.an increased focus on the cybersecurity of children is essential to improve overall cybersecurity in the future.1, fraudsters use phishing to convince victims to give out per- sonal information.commonly  the fraudsters want creden- tials that are used to access online services  such as online banking.even though the impersonated brands that are misused in phishing are predominately ﬁnancial institutions and payment providers  there has been a recent shift towards retailers and service-oriented companies [3  4].several coun- termeasures are currently in use to prevent phishing victim- ization: blocking phishing messages and websites  improving  copyright is held by the author/owner.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee.symposium on usable privacy and security (soups) , children need to understand digital risks to reduce the risk of victimization on the internet.understanding digital risks is important for children as well as adults.however  the majority of children are self-taught when it comes to the in- ternet [7]  making it unlikely they will systematically learn how to act safely.to learn about the abilities of children in detecting phishing emails and websites  researchers had  usenix association  thirteenth symposium on usable privacy and security    237  children aged 8–13 take in a phishing recognition test.half of the children received training before the test  and the other half did not.both trained and untrained children were tested for the ability to distinguish phishing emails and websites from legitimate ones.several schools partici- pated in the study.a ﬁrst indicator of the practical need for such training arose while performing the experiment.dur- ing the training  as more pupils started sharing their stories  they became very enthusiastic and asked lots of questions.in most classes  at least one child knew a phishing victim.these victims were mostly relatives or neighbors.the most common situation in the stories that were told was a victim losing money due to ﬁlling in banking credentials on a phish- ing website.hearing stories from their peers impacted the children and provided them with a warning message stronger than the presenters could ever give.until novel anti-phishing techniques are developed and de- ployed on a large scale  user training seems to be important.for adults as well as children  that means creating an im- proved knowledge of the subject for as many individuals as possible.in many countries  all children aged 9 or older attend some form of education.potentially  this makes it feasible to embed a cybersecurity training in their curricu- lum  eﬀectively training the entire population of children.in our experience  both schools and parents are very will- ing to embed lessons about cybersecurity in the curriculum.our request to give a training was well received.in particu- lar  incidents with phishing  cyberbullying  and other cyber- threats are often in the news.teachers and parents reported being worried about those issues.at the same time  teachers at schools where we gave a training  found the course highly informative for themselves as well.techniques for establish- ing the validity of an email were unknown to them.several teachers mentioned that hovering over a hyperlink or check- ing the sender address were valuable approaches for them.training teachers should  therefore  be the ﬁrst step in cy- bersecurity education.where needed  universities and prac- titioners (e.g.it security ﬁrms) could provide help.there are existing initiatives  such as the (isc)2 safe and secure online1 where security professionals visit schools.such ini- tiatives should be extended to more countries and expanded in size  and new ones should be developed.training children increased their short-term ability to distin- guish phishing from legitimate correctly.speciﬁcally  their ability to recognize phishing increases signiﬁcantly after an in-class training.however  this increased ability is subject to decay.after four weeks  the ability to recognize phishing for trained children diminished to the level of their non-trained counterparts.this suggests that the training created knowl- edge  but that this knowledge only lasted through the short term.on the positive side  trained children did continue to perform better in recognizing legitimate emails as such.this increases the odds of legitimate communications reach- ing the end user.increasing the ability to recognize phishing requires good awareness.all in all  we believe that researchers and practitioners in the ﬁeld of cybersecurity should not only focus on adults  but that material for children should be developed in paral- lel.phishing  speciﬁcally  is too often seen as an adult-only  1see also https://iamcybersafe.org/  crime.the children of today are the victims of the future.6.acknowledgments we would like to thank brinda badarinath hampiholi  joey de vries  lorena montoya  and jan-willem bull´ee for their valuable advice and feedback.we would also like to thank the reviewers for their constructive feedback and shepherd elizabeth stobert for her helpful comments.7
14,2017-Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification.pdf, deep neural networks (dnns) have transformed several arti(cid:128)cial intelligence research areas including computer vision  speech recog- nition  and natural language processing.however  recent studies demonstrated that dnns are vulnerable to adversarial manipu- lations at testing time.speci(cid:128)cally  suppose we have a testing example  whose label can be correctly predicted by a dnn classi(cid:128)er.an a(cid:138)acker can add a small carefully cra(cid:137)ed noise to the testing example such that the dnn classi(cid:128)er predicts an incorrect label  where the cra(cid:137)ed testing example is called adversarial example.such a(cid:138)acks are called evasion a(cid:136)acks.evasion a(cid:138)acks are one of the biggest challenges for deploying dnns in safety and security critical applications such as self-driving cars.in this work  we develop new dnns that are robust to state-of- the-art evasion a(cid:138)acks.our key observation is that adversarial examples are close to the classi(cid:128)cation boundary.(cid:140)erefore  we propose region-based classi(cid:128)cation to be robust to adversarial ex- amples.speci(cid:128)cally  for a benign/adversarial testing example  we ensemble information in a hypercube centered at the example to predict its label.in contrast  traditional classi(cid:128)ers are point-based classi(cid:128)cation  i.e.given a testing example  the classi(cid:128)er predicts its label based on the testing example alone.our evaluation results on mnist and cifar-10 datasets demonstrate that our region-based classi(cid:128)cation can signi(cid:128)cantly mitigate evasion a(cid:138)acks without sacri(cid:128)cing classi(cid:128)cation accuracy on benign examples.speci(cid:128)cally  our region-based classi(cid:128)cation achieves the same classi(cid:128)cation ac- curacy on testing benign examples as point-based classi(cid:128)cation  but our region-based classi(cid:128)cation is signi(cid:128)cantly more robust than point-based classi(cid:128)cation to state-of-the-art evasion a(cid:138)acks.1, deep neural networks (dnns) are unprecendentedly e(cid:130)ective at solving many challenging arti(cid:1,
15,2017-Orpheus- Enforcing Cyber-Physical Execution Semantics to Defend Against Data-Oriented Attacks.pdf, recent studies have revealed that control programs running on embedded devices suffer from both control-oriented attacks (e.g.code-injection or code-reuse attacks) and data-oriented attacks (e.g.non-control data attacks).unfortunately  existing detection mechanisms are insufficient to detect runtime data-oriented ex- ploits  due to the lack of runtime execution semantics checking.in this work  we propose orpheus  a security methodology for de- fending against data-oriented attacks by enforcing cyber-physical execution semantics.we address several challenges in reasoning cyber-physical execution semantics of a control program  including the event identification and dependence analysis.as an instantia- tion of orpheus  we present a new program behavior model  i.e.the event-aware finite-state automaton (efsa).efsa takes advantage of the event-driven nature of control programs and incorporates event checking in anomaly detection.it detects data-oriented exploits if physical events and efsa’s state transitions are inconsistent.we evaluate our prototype’s performance by conducting case studies under data-oriented attacks.results show that efsa can success- fully detect different runtime attacks.our prototype on raspberry pi incurs a low overhead  taking 0.0001s for each state transition integrity checking  and 0.063s∼0.211s for the cyber-physical con- textual consistency checking.ccs concepts • security and privacy → software and application security; intrusion/anomaly detection and malware mitigation; • com- puter systems organization → embedded and cyber-physical systems; keywords anomaly detection; cyber-physical systems; data-oriented attacks; control programs; execution semantics; event awareness; 1, embedded control and monitoring systems are becoming widely used in a variety of cps (cyber-physical systems) and iot (internet  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.acsac ,
16,2017-POSTER- A PU Learning based System for Potential Malicious URL Detection.pdf, this paper describes a pu learning (positive and unlabeled learning) based system for potential url attack detection.previous machine learning based solutions for this task mainly formalize it as a su- pervised learning problem.however  in some scenarios  the data obtained always contains only a handful of known attack urls  along with a large number of unlabeled instances  making the su- pervised learning paradigms infeasible.in this work  we formalize this setting as a pu learning problem  and solve it by combining two different strategies (two-stage strategy and cost-sensitive strategy).experimental results show that the developed system can effectively find potential url attacks.this system can either be deployed as an assistance for existing system or be employed to help cyber-security engineers to effectively discover potential attack mode so that they can improve the existing system with significantly less efforts.keywords url attack detection  machine learning  pu learning  1, with the rapid development of internet  more and more kinds of url attacks have arisen  becoming a serious threat to cyber-security.traditional url attack detection systems are mainly constructed through the use of blacklists or rule lists.these lists will gradually become much longer  and it is impracticable to cover all attacks by these ways.more severely  these kinds of methods lack the ability of detecting potential attacks  making it awkward for cyber-security engineers to efficiently discover newly generated url attacks.to provide better generalization performance  machine learn- ing based approaches have been employed to this task.these ap- proaches mainly fall into two categories: most formalize it as a supervised learning problem  in which labeled data are needed [6]  while the rest try to solve the problem in an unsupervised manner  e.g.by anomaly detection techniques [5]  with no label informa- tion required.when the labeled data can be obtained  supervised learning methods can always provide better generality.however  in  permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for third-party components of this work must be honored.for all other uses  contact the owner/author(s).ccs ’17  october 30-november 3  , in this work  we develop a potential url attack detection system based on pu learning.compared to supervised learning based ap- proaches  our method only needs a handful of malicious urls  along with the unlabeled urls  which is suitable for the real situation that we encounter.the developed system mainly contains three parts: firstly  a fea- ture extraction process is executed to transfer the original urls into numerical feature vectors; secondly  two-stage strategy and cost-sensitive strategy are employed to train the classification mod- els; finally  each new-coming url will be first transformed into numerical feature vector and then be fed into the learned models  those urls with high scores will be regarded as potential malicious urls with high probability.empirical results show that our developed system can effectively discover potential url attacks.this system can either be deployed as an assistance for the existing system or be employed to help cyber-security engineers to effectively discover potential attack mode.acknowledgments partially supported by nsfc (61333014) and the collaborative inno- vation center of novel software technology and industrialization
17,2017-PT-Rand- Practical Mitigation of Data-only Attacks against Page Tables.pdf,—kernel exploits constitute a powerful attack class allowing attackers to gain full control over a system.various kernel hardening solutions have been proposed or deployed in practice to protect the kernel against code injection (e.g.dep) or code-reuse exploits (e.g.cfi).however  the security of all these hardening techniques relies heavily on the assumption that kernel page tables cannot be manipulated  e.g.by means of data- only attacks.ensuring kernel page tables integrity is not only essential for kernel security but also a challenging task in practice since existing solutions require hardware trust anchors  costly hypervisors  or inefﬁcient integrity checks.in this paper  we ﬁrst motivate the importance of protecting kernel page tables by presenting a data-only attack against page tables to bypass the recently released cfi-based (linux) kernel hardening technique rap.thereafter  we present the design and implementation of pt-rand  the ﬁrst practical solution to protect kernel page tables that does not suffer from the mentioned deﬁ- ciencies of previous proposals.pt-rand randomizes the location of page tables and tackles a number of challenges to ensure that the location of page tables is not leaked.this effectively prevents the attacker from manipulating access permissions of code pages  thereby enabling secure enforcement of kernel exploit mitigation technologies such as cfi.we extensively evaluate our prototype implementation of pt-rand for the current linux kernel on the popular linux distribution debian and report a low overhead of 0.22% for common benchmarks.moreover  we combine rap with pt-rand to protect rap against data-only attacks on kernel page tables.i,  operating system kernels are essential components in mod- ern computing platforms since they provide the interface be- tween user applications and hardware.they also feature many important services such as memory and disk management.typ- ically  the kernel is separated from user applications by means of memory protection  i.e.less-privileged user applications can only access the higher-privileged kernel through well-deﬁned interfaces  such as system calls.attacks against kernels are gaining more and more prominence for two reasons: ﬁrst  the kernel executes with high privileges  often allowing the attacker to compromise the entire system based on a single  permission  to  freely  reproduce  all  or  part  of  this  paper  for  noncommercial  purposes is granted provided that copies bear this notice and the full citation  on the ﬁrst page.reproduction for commercial purposes is strictly prohibited  without the prior written consent of the internet society  the ﬁrst-named author  (for  reproduction  of  an  entire  paper  only)   and  the  author’s  employer  if  the  paper  was  prepared  within  the  scope  of  employment.ndss  ’17   ,
18,2017-QUASAR- Quantitative Attack Space Analysis and Reasoning.pdf, computer security has long been an arms race between attacks and defenses.while new defenses are proposed and built to stop specific vectors of attacks  novel  sophisticated attacks are devised by at- tackers to bypass them.this rapid cycle of defenses and attacks has made it difficult to strategically reason about the protection offered by each defensive technique  the coverage of a set of defenses  and possible new vectors of attack for which to design future defenses.in this work  we present quasar  a framework that systematically analyzes attacks and defenses at the granularity of the capabilities necessary to mount the attacks.we build a model of attacks in the memory corruption domain  and represent various prominent defenses in this domain.we demonstrate that quasar can be used to compare defenses at a fundamental level (what they do instead of how they do it)  reason about the coverage of a defensive con- figuration  and hypothesize about possible new attack strategies.we show that of the top five hypothesized new attack strategies  in fact  four have been published in security venues over the past two years.we investigate the fifth hypothesized vector ourselves and demonstrate that it is  in fact  a viable vector of attack.acm reference format: richard skowyra  steven r.gomez  david bigelow  james landry  and hamed  okhravi.2017.quasar: quantitative attack space analysis and reasoning.in proceedings of 2017 annual computer security applications conference   orlando  fl  dec 2017 (acsac’17)  11 pages.doi: 10.1145/3134600.3134633  1, computer security has long been an arms race.while defenders work diligently to propose and build new defenses that prevent various avenues of attack  attackers work just as hard to conceive  distribution statement a.approved for public release: distribution unlimited.this material is based upon work supported by the department of defense under air force contract no.fa87, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the department of defense.permission to make digital or hard copies of all or part of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full citation  on the first page.copyrights for components of this work owned by others than acm  must be honored.abstracting with credit is permitted.to copy otherwise  or republish   to post on servers or to redistribute to lists  requires prior specific permission and/or a  fee.request permissions from permissions@acm.org.acsac’17  orlando  fl.usa © 2017 acm.978-1-4503-5345-8/17/12...$15.00 doi: 10.1145/3134600.3134633  hamed okhravi  mit lincoln laboratory hamed.okhravi@ll.mit.edu  and build new and clever means to bypass such defenses.the dy- namics of this situation is exemplified by the state of attacks and defenses in the memory corruption domain.although memory cor- ruption attacks have been studied for more than four decades [3]  increasingly complex defenses are continually built to guard against equally sophisticated attacks capable of bypassing the newest de- fenses.a few examples of such evolution in the community include: the deployment of w ⊕ x [26] followed by the advent of code reuse attacks to bypass it [34]  code randomization and diversification techniques [23] followed by information leakage attacks [32  38]  fine-grained memory randomization [19] followed by just-in-time rop (jit rop) attacks [37]  control flow integrity (cfi) [2] followed by control jujutsu [16] and control flow bending attacks [8]  and code re-randomization [7] followed by data-oriented programming (dop) [18].this rapid defense/attack development cycle has made it difficult for both security planners and researchers to focus on the strate- gic view.on the one hand  defenses tend to focus on the narrow artifacts of an attack rather than on the attack’s fundamental re- quirements  allowing motivated attackers to modify those artifacts and bypass the defense.on the other hand  security planners can- not easily understand the actual benefits offered by a defensive technique  compare incongruent defenses  or identify the types of attacks that are/are not covered by a particular defensive configu- ration.there are some existing approaches for analyzing attacks and de- fenses  including low-level frameworks such as attack graphs [35] and high-level risk modeling and simulation frameworks [24  41].we argue that while valuable  these approaches are insufficient for strategic analysis of attacks  defense planning  and defense compar- ison.at the more detailed end of the scale  attack graphs analyze a network of computers at the granularity of individual software applications and their vulnerabilities.while such analysis provides high fidelity  the results are often only valid for a short duration be- cause regular changes in the network such as (un)installation of an application  discovery of a new vulnerability  and even small modi- fications to the network topology or reachability  may significantly change or completely invalidate the analysis results.moreover  attack graphs cannot easily compare various defenses since that would require solving the attack graph for different subsets of de- fenses  a problem that quickly becomes intractable as the number of possible defenses grows.in comparison  risk modeling and sim- ulation approaches can analyze attack and defense interaction at a higher granularity  but such approaches usually require quan- titative inputs such as attack arrival rates and defensive success  68probabilities.such quantitative inputs are often not available or obtainable is a repeatable fashion in practice.this work presents a framework called quantitative attack space analysis and reasoning (quasar) to analyze attacks and de- fenses at a strategic level.quasar represents each attack class with a set of fundamental capabilities necessary for an attacker to suc- cessfully carry out that attack.those capabilities may themselves rely on other finer-grained capabilities  combined using and/or operations in a graph-like structure  which we call an attack capa- bility graph (acg).for example  the attack type “code injection” requires that (1) the attacker knows the instruction set that is used on the target machine  and (2) the attacker has the capability to write to executable memory pages.in turn  the capability to write to executable memory pages requires (1) a write location  and (2) a write type.each of these capabilities is in turn subdivided into further and more specific capabilities (e.g.the write location may be the stack or the heap  and the write type may be a direct write or an overflow write).we represent a defense in quasar as a set of constraints on the acg that disables some combination of attack capabilities.for example  stack canaries would put a constraint on the code injection sub- graph that prevents the simultaneous usage of overflowing (for the write type) and stack (for the write location).however  with this defense  note that other combination of capabilities such as direct writing to stack are still possible.this approach allows us to represent defenses with a high degree of flexibility (unlike attack trees [21]) and can be viewed as a declarative way of specifying defenses: what defenses do instead of how they do it.quasar can be used in defense planning to compare various defenses at a fundamental level and to analyze the coverage of a defensive configuration.it can also be used as a research planning tool to identify the capabilities most prevalently used in attacks (the critical capabilities)  which currently lack coverage from existing defenses.finally  it can be used to anticipate new high-impact attack strategies that attackers may begin to employ once effective defenses for current attacks are deployed.to demonstrate these use cases  we create the entire acg for the memory corruption domain  capturing all publicly disclosed attack types and their capabilities from 1972 to 2017  resulting in 140 fundamental attack capabilities.we also represent 20 promi- nent memory protection defenses including control-flow integrity [2]  code pointer integrity [22]  readactor [11]  tasr [7]  binary stirring [40]  ilr [17]  as well as widely deployed defenses such as dep [26]  stack canaries [10]  and aslr [27].naïvly calculating defense coverage and attack criticality in such a large model quickly becomes intractable because an np-hard problem must be solved an exponential number of times.instead  we leverage a recent de- velopment in the formal methods community  called #sat  that can quantify these measures without repeatedly solving the full satisfiability problem.this approach allows us to use quasar to meaningfully com- pare several seemingly-incongruent defenses to code reuse attacks against each other.furthermore  quasar allows us to identify the top high-impact attack strategies that attackers may next target given the state of the best proposed defenses today.as validation for this latter use  during the time that this project has been under  development  four of the top five high-impact attack strategies iden- tified by quasar have been published in various security venues.we briefly study the fifth high-impact strategy (software-based malicious dma attacks) ourselves and construct a proof-of-concept attack against sftp to illustrate its practicality and impact  and fur- thermore show that the opportunities for such attacks are prevalent in various popular server applications.our contributions are as follows:  • we propose a framework to analyze attacks and defenses at the granularity of attack capabilities  not too low-level to impede strategic reasoning and not too high-level to require hard-to-quantify values.• we build an entire attack capability graph (acg) for the memory corruption domain  comprising 140 fundamental attack capabilities  and also represent various prominent memory protection defenses.• we leverage recent advancements from the formal meth- ods community to compute informative measures such as defense coverage and attack capability criticality in the memory corruption acg.• we demonstrate how acgs can be used to specify the • we identify the top five new high-impact attack strategies that we anticipate attackers use in response to recent de- fenses proposed in the community.we further illustrate that four of these attack techniques have in fact been pub- lished in the past 2 years.• we investigate the fifth high-impact attack strategy (software-  precise impact of each defense and compare defenses.based malicious dma)  build an example exploit against a real-world application  and otherwise study its applicabil- ity and prevalence.our results indicate that analysis of attacks and defenses at the granularity of attack capabilities allow better strategic reasoning for defense planners and researchers alike.2 modeling attacks and defenses quasar is a framework that helps quantify how critical partic- ular attack techniques might be for attackers  and measures the coverage of defenses with respect to these attack techniques.to compute these measures  it is necessary to produce an encoding of existing knowledge about attacks and defenses in a specific security domain.we call this a knowledge base  which is manually curated and constructed by security experts  and ultimately represented as a set of logical expressions.this allows quasar to leverage #sat solving for its quantitative measures  which is the counting variant of boolean satisfiability.at a high level  a quasar model compiles to boolean formulas whose solutions correspond to suc- cessful attack methods.by counting the number of solutions in the presence of specific defenses  we can create measures that enable defenses to be analyzed and compared.this section will cover the model creation and compilation process  while sections 3 and 4 will go into detail on measures for comparison and gap analysis.2.1 attack capabilities in order to compare defenses both against each other  and against the space of attacker strategies  we need to choose a granularity  69figure 1: linux getaddrinfo attack  of abstraction that captures the impact of defenses on attacks  but is neither so specific that conclusions are not generalizable (e.g.depending on specific software versions) nor so abstract that it requires substantial parameterization from unavailable datasets (e.g.monte carlo simulations).we elected to use an attack tree-like representation of attacker capabilities  which represents features of the target system that must be present in order for an attack method to succeed despite the presence of deployed defenses.consider the example in figure 1  which describes the getaddrinfo  attack disclosed in february 2016 [1] against linux’s libc library.this attack enabled malicious dns servers to achieve remote code execution on vulnerable systems.the left-most box contains a summary of detailed implementation data: specific function names  specific memory offsets  and specific side channels.this level of granularity is helpful for understanding the artifacts of an attack  but is tightly coupled to specific system configurations.upon ab- straction into large-scale components (center box)  these details resolve themselves into mechanisms for memory corruption  mem- ory leakage  and code reuse.the right-most box depicts what we call attack capabilities  which are a formalized  taxonomic approach to characterizing an attack.the getaddrinfo attack uses  for ex- ample  memory corruption on the stack via an overflow.this level of granularity is helpful for two reasons.first  the right-most depiction of the attack is agnostic of specific imple- mentation details.it focuses purely on what features the attacker requires  not how they are implemented.this enables reasoning about large classes of attack and how defenses may impact their critical requirements  rather than analysis of artifacts that attack- ers could potentially bypass.second  because attack capabilities are shared across many implementations of attacks  a relatively small number of such capabilities are needed to fully capture all disclosed attack methods.this allows quasar to avoid scalability challenges that would naturally arise in finer-grained modeling approaches like attack graphs.attacker capabilities are dependency relationships  beginning with a high-level goal (e.g.“control flow hijacking”) and contin- uing into increasingly-refined sub-goals.each attack capability can have either an “all-of” or “any-of” relationship with other capabilities on which it depends  or it can be an atom with no further dependencies.this can be represented in an attack capa- bility graph (acg).for example  consider the acg depicted in  figure 2  which is a small subgraph of the entire memory cor- ruption acg  capturing code injection attacks.the top-level goal  control_flow_hijackinд  has an any-of relationship with its sole dependency  code_injection.that has an all-of relationship with both thew rite_to_executable_memory and instruction_set_known capabilities; both dependencies must be satisfied in order for the attacker to be able to use code injection.these dependencies have their own dependencies  which must themselves be satisfied.ca- pabilities that represent the lowest level units of an attack (e.g.over f low in the example above) are considered attack atoms and are not dependent on other capabilities; however  such atoms can easily be extended with new requirements if it should become nec- essary by the identification of a new subdivision in capabilities or defenses.in addition to extending atoms  an acg can be further expanded by adding new high-level capabilities and dependencies.control-flow hijacking  for example  can also be accomplished via code reuse (e.g.return-oriented programming).a depen- dency model can be added to the existing any-of dependency for code_reuse  and form a new subgraph of the larger acg.the acg representation makes model construction easy for hu- man operators  but is otherwise equivalent to a logical formulation of attacker capabilities that is amenable to automated analysis by quasar.formally  an attack capability is defined as a propositional logic formula with one of the following structures:  all-of: an → (ai ∧ ...∧ aj ) any-of: an → (ai ∨ ...∨ aj ) atom: an  (1)  where:  • an is the name of the attack capability.this atom may be constrained by defenses  and is only available if its require- ments (if any) are met.• ai ...aj are the attack capabilities on which an depends.note that the acg could be represented in datalog  but we choose to use propositional logic notation because datalog cannot be used to encode the entire model.specifically  defenses may impose constraints that negate particular attack capabilities.to create a unified attack capability model  a single formula ϕatk is created as the conjunction of all attack capabilities  along with the  1.new connection triggers fork()2.stack-based buffer overflows in send_dg and send_vc functions3.overwrite pointer to code4.tcp connection loss if memory error5.tcp connect success if valid address6.compute rop offsets and send payload1.new connection triggers fork()2.stack-based buffer overflows in send_dg and send_vc functions3.overwrite pointer to code4.tcp connection loss if memory error5.tcp connect success if valid address6.compute rop offsets and send payload•buffer on stack with wrong size•memory layout preserved acrossfork().guess pointer valuebyte-by-byte.•construct a rop chain andcall system()•buffer on stack with wrong size•memory layout preserved acrossfork().guess pointer valuebyte-by-byte.•construct a rop chain andcall system()•memory corruption•stack-based•overflow•memory leakage•remote side channel•fault-based•code reuse•known gadget loc.•chained•syntactic•memory corruption•stack-based•overflow•memory leakage•remote side channel•fault-based•code reuse•known gadget loc.•chained•syntacticattack implementation detailsattack capabilitiesattack  componentsspecificgeneral  logical70(cid:94)  assertion of the top-level goal: χatk = ϕatk = χatk ∧ aд  a∈a  a  where:  • a is the set of all attack capabilities • χatk is the entire attack capability graph • aд is the top-level goal of the attack capability graph  this structure creates a formula with a key property: it is satis- fiable only if there exists a set of attack capabilities which permit the top-level goal aд to satisfy all of its dependencies.thus  every solution to the formula is a unique attack method  by virtue of being a unique set of truth values that satisfy the formula and make aд achievable by the attacker.2.2 defenses defenses in the knowledge base serve to constrain an attacker’s ability to use one or more capabilities.in general  defenses are encoded in a manner similar to attack capabilities; however  where attack capabilities must be attack atoms or have dependencies on other capabilities  defenses can arbitrarily constrain capabilities.this flexibility is a novel contribution of our work and does not exist in similar approaches  such as attack/defense trees [21].for example  data execution prevention (dep) constrains an at- tacker’s ability to write to executable memory.we can encode dep as assuring that the attack capabilityw rite_to_executable_memory is disabled as far as any upstream capabilities are concerned  regard- less of whether the dependencies ofw rite_to_executable_memory are satisfied.a defense is formalized as a logical formula of the following structure:  dn ∧ ϕr → ϕa  (3)  where:  • dn is the name of the defense.note that the constraint it imposes over the attack space is enforced only if dn is asserted.• ϕr is a requirement that the defense has over some set of system environment atoms  such as being limited to deployment on certain platforms (e.g.linux vs.windows).• ϕa is the constraint imposed by the defense on available attack capabilities.this may be an arbitrary logical con- straint  and is not limited to simple negation.this structure has two useful properties.first  defenses can be added to the model without actually deploying them (i.e.enforcing their constraint).this is useful  as it enables quasar to decouple model creation from model querying.that is  we can efficiently count formula solutions (and thus the number of viable attack meth- ods) for any combination of defenses  which need not be chosen in advance at model compilation time.second  defenses impose arbitrary constraints over attack ca- pabilities  which is particularly critical in the memory corruption domain.coarse-grained control-flow integrity  for example  does not directly remove any capabilities from an attacker’s arsenal but instead limits their combination by imposing a policy: dereferenc- ing of a return address may only go to a call site  and dereferencing an indirect branch may only go to a function prologue.(2)  because defenses can impose arbitrary constraints which may be difficult to write by hand  we developed a small domain-specific lan- guage to express constraints more compactly than by using boolean expressions over capabilities.these are automatically expanded by quasar into the full boolean formula.for example  we represent coarse-grained control flow integrity in the following way:  dcap control_flow_integrity_coarse_grained assures  (function_return.dereference_event implies  (entry_point.code_pointer_dereference i  {call_site.entry_point}))  and  (explicit_dereference.dereference_event implies  (entry_point.code_pointer_dereference i  {function.entry_point}))  the language has two features to note beyond simple boolean operations: postfix-matched names and set operations.the for- mer servers to disambiguate attack capabilities which may have the same name.stack appears in several locations of the attack capabil- ity graph  for example  with different contexts and semantics.rather than demand unique names  defenses can specify which capability is constrained by postfixing it with a chain of its parents  separated by dots.for example  function_return.dereference_event refers to the function_return attack capability with dereference_event as its parent.quasar will not compile the model if any ambiguity is present  which must be resolved by extending the length of the postfix until it is unique.the second feature is the use of two simple set operations over the children of an attack capability: intersection (i) and comple- ment (c).each non-terminal attack capability can be viewed as the name of a set whose members are its children.operations over this set can allow defenses to more clearly and easily constrain which children of that attack capability are constrained or avail- able.for example  (entry_point.code_pointer_dereference i call_site.entry_point)) refers to the entry_point set  which denotes the means by which a code pointer de-reference can point to the entry point to a code region.its members are function  call_site  aligned_instruction and unaligned_instruction.the intersection of entry_point with call_site restricts the at- tacker from using any other entry point but a call site.this notation allows defenses to avoid explicitly listing which alternatives they restrict.it allows more terse  readable defenses  as well as somewhat insulating defenses from needing to be re- written if the attacker capability graph is updated to include another alternative in an already-restricted set.once all defenses have been defined  they are conjoined into a single boolean formula  which is itself conjoined with the space of attack capabilities captured by ϕatk:  (cid:94) d∈d ϕf ull = ϕdef ∧ ϕatk  ϕdef =  d  (4)  using the formalism defined above  we created an acg for mem- ory corruption  comprising 140 fundamental attack capabilities.71figure 2: code injection attack capability graph  all publicly disclosed attack methods (e.g.return-oriented pro- gramming (rop)  ret-to-libc  counterfeit object-oriented pro- gramming  jit-rop) are included as sets of capabilities that may be chosen by an attacker.this acg is shown in figure 3.we also modeled twenty prominent defenses  including control- flow integrity [2]  code pointer integrity [22]  readactor [11]  tasr [7]  binary stirring [40]  ilr [17]  as well as widely deployed de- fenses such as dep [26]  stack canaries [10]  and aslr [27].2.3 automated solution counting via #sat the formula ϕf ull representing the model of defenses and attack strategies has an important property: every solution to that formula constitutes a successful attack method  and every successful attack method constitutes a solution to the formula.this means that the ability to count formula solutions enables the counting of attack methods.furthermore  defenses are not explicitly enabled.as log- ical implications  the constraint they impose on the selection of attack capabilities are enforced only if the defense itself is asserted.if the number of solutions to the formula can be counted given some set of defense literals that are enabled or disabled  however  we can perform quantitative reasoning about how many unique attack methods are possible in presence of a specific defense (or set of defenses).unfortunately  traditional sat solvers cannot be usefully lever- aged for counting the number of unique solutions to a formula.sat solvers rapidly find and return individual solutions  whereas we wish to count (but not necessarily enumerate) the total number of formula solutions.a technique known as #sat  or propositional model counting  does provide the ability to count formula solutions [6].specif- ically  an approach known as “knowledge compilation” trans- forms a boolean formula into an efficient representation known as smoothed deterministic decomposable negation normal form (sddnnf) [12  13].this representation has several useful proper- ties.first  although the compilation step is itself np-hard  queries against the sddnnf are performed as linear-time graph traversals.because our graph is guaranteed to be polynomial with respect to number of variables in the input formula  the expensive com- putational step need only be performed upon model creation or modification  rather than during the solution counting phase.second  the sddnnf allows solution counting to be imple- mented as a function g (s ) which takes a constraint set as a pa- rameter s  and returns the solution count under those constraints.constraints in this context are a set of truth values for atoms in the formula.this can be used to conduct an analysis of attack meth- ods conditioned on a specific set of defenses being asserted (i.e.deployed) and all others negated.we leverage this in section 3 to compare defenses by measuring their coverage over the space of attack methods.finally  the partial derivative of the solution counting function can be computed with respect to each atom of the formula.this enables us to compute  for every atom  the number of solutions when that atom is either asserted or negated.computation for each atom can be done simultaneously  in a second linear time graph traversal.we use this capability in section 4 to identify future high-impact attack capabilities.for this work  we used the c2d [14] #sat solver  which outputs a formula in sddnnf.despite having approximately 500 variables covering attack capabilities and defenses  compilation times never exceeded 500ms on a 2.1ghz intel xeon processor with 32gb of ram.3 comparing defenses 3.1 comparing defense constraints representing memory corruption defenses as logical constraints on attack capabilities can be a useful way to identify comparable or — with respect to our model — equivalent defenses.for example  consider two anti-rop defenses: • ilr [17] is a virtualization-based defense which randomizes • binary stirring [40] is a loader-based defense which re- orders and re-writes native code to be semantically equiv- alent on the basic block level.instruction locations in a process.each of these defenses rely on very different mechanisms  work- ing at varying levels of abstraction and varying points in the pro- gram’s lifecycle.how  then  may they be compared against each other? when distilled to the constraints that they impose on avail- able attack capabilities  however  we realize that all three defenses are logically equivalent:  ¬memory_layout_known → ¬syntactic_gadдets  (5)  72figure 3: attack capability graph for memory corruption attack methods.attack analysisdata_valuedata_pointerfunction_pointersemantic_objectinitialized_datadata_valuedata_pointerfunction_pointersemantic_objectuninitialized_dataexception_handlerheap_metadatavirtual_function_tablereturn_addressprocess_control_datamemory_typestackheaplocationpartialfullwrite_granularitydirectoverﬂowwrite_typememory_corruptionmmudmaaccess_methodstackheaplocationdirecttrampolinefunction_pointerdirecttrampolinedata_valuedata_pointerreturn_addresstypedatabinarylibrarylocationfunction_addressgadget_addressbase_addresstypetextmemory_accessedmemory_readablefault_channeltiming_channeli/o_side_channelmultiple_disclosuressingle_disclosurenumberbuffer_overreadmemory_readdirect_writebuffer_overﬂowmemory_writetypepartialfullgranularitysecondary_memory_corruptionmemory_disclosureleakage_typememory_image_knowngeneric_binary_inspectionspeciﬁc_binary_inspectioninspection_typefunction_addressgadget_addressimage_readabledisclosed_codedisk_image_knownmemory_layout_knownsingle_code_sequenceheapstacklocationsemantic_loop_gadgetindirect_jmpindirect_callreturn_addresstypechained_code_sequence  trampolinereuse_lengthcounterfeit_procedurescounterfeit_objectssemanticfunctiongadgetgranularitysyntacticabstraction_levelcode_reusewrite_to_executable_memoryinstruction_set_knownoverﬂowdirectwrite_typestackheapwrite_locationcode_injectionpayload_typeunaligned_instructiongenuinearbitrarypossiblecall_sitegenuinearbitrarypossiblefunctionentry_pointbinarylibrarytextstackheapdatapoints_to_locationobject_freedexception_triggeredexplicit_dereferencefunction_returnindirect_callindirect_jmpdereference_eventcode_pointer_dereferencepayloadcfhijackcompromisedefense optimizationinstruction_set_knownwrite_to_executable_memorycode_injectionbuffer_overreadsingle_code_sequence0102030405060708090100criticality92.787.685.371.967.3best possible defensescurrent defenses(lower is better)possibleattacksdefensesdata executionpreventiongadgetsmashingstackcookiesreadactorexecuteonly memoryaslr fine grainedaslr coarse grainedmulticompilertimely address spacererandomizationcode pointer integritycontrolflowintegritycoarse grainedcontrolflowintegrityfine grainedcontrolflowintegrityidealsubmitquery(cid:48)emo(cid:85)y (cid:38)o(cid:85)(cid:85)(cid:88)ptionattack analysisdefense optimizationdefenses(cid:38)onst(cid:85)aintsenvironmentcost all-of  dependency any-of  dependencyatom capability73that is  all three defenses apply only when the memory layout is not known to an attacker.in that case  they prevent the attacker from using syntactic (i.e.traditional rop) gadgets.it is also im- portant to note that we are considering the “ideal” version of each defense as claimed by the authors; the actual implementation may have other weaknesses not included in this formulation.by modeling defenses at this level  we gain the ability to iden- tify comparable or logically equivalent defenses.this enables  for example  identification of common threat model assumptions (e.g.attackers not knowing memory layout) that might indicate a shared point of weakness across several defenses.it also allows researchers or defense planners to avoid spending resources on developing or deploying a defense which provides no added protection beyond already existing techniques.3.2 comparing defense coverage another strategy for comparing defenses to one another is through analysis of the degree to which they constrain attack methods.re- call that defenses in quasar are represented as logical constraints over attack capabilities.these constraints limit what capabilities can be simultaneously selected as part of an attack method.by counting how many distinct successful attack methods are possible in the presence of a deployed defense  we can compute a measure of that defense’s coverage over the space of attack methods.specif- ically  for each defense in the model we can compute the following defense coverage measure:  ∀d ∈ d : coverage(d ) = 1 − g (sd )g (sof f )  −1  (6)  where:  in the model.• d is the set of literals corresponding to all defenses included • g is the solution counting function which takes a set of constraints and returns the number of formula solutions under those constraints.• sd = {d}∪{¬d′ ∈ d|d′ (cid:44) d} is a constraint set in which the defense d is enabled  and all other defenses are disabled.• sof f = {¬d′ ∈ d} is a constraint set in which all defenses are disabled.that is  defense coverage is a proportional measure which com- pares the number of attack methods available when that defense is deployed  to those available when no defenses are deployed.this is normalized to be in the [0  1] interval  where a higher value corre- sponds to higher coverage  meaning a lower number of successful attack methods.with respect to our model of defenses and attack capabilities  coverage never exceeds approximately 0.4 for any sin- gle existing defense.this is because no single defense can stop the majority of attack methods.it is possible that for strong defenses  the remaining attack methods may be qualitatively harder.this  however  is difficult to quantify and is currently not a part of our analysis.the approach used to compare single defenses can also be used to compare sets of defenses against each other by enabling them in sd.for example  consider two memory corruption defenses: readac- tor [11] and stack cookies [4].the constraints imposed on the attack capabilities for each defense are shown in table 1  as are their  defensive coverages.readactor is a recent memory corruption de- fense that relies on fine-grained randomization and non-executable memory and with respect to our attack capability model  it con- strains attack methods such that: the text segment of a process is unreadable to an attacker  the victim’s memory image must be known (i.e.an attacker’s local copy cannot be used to identify gad- get locations)  the only indirect branches are trampolines (rather than direct pointers to the branch target)  and code reuse on the se- mantic level cannot rely on counterfeit objects.due to constraining many attack capabilities  often forcing the attacker to alternatives which themselves rely on complex dependencies  readactor pro- vides a high defense coverage.attacks are still possible  but are generally restricted to those with many complex dependencies or which rely on multi-stage attacks [29].in comparison  stack canaries protect against buffer overflows on the stack  assuming no memory disclosures are present.this is captured in our defense constraint.if the memory layout of the victim is unknown  and the memory corruption happens via an overflow on the stack  then the attacker’s corruption of process control data cannot include corruption of return addresses.this is a much weaker constraint than readactor  as is clear when comparing their respective defense coverages.4 new high-impact attacks 4.1 attack criticality in addition to its ability to compare defenses against one another  quasar can also be used to compare attack capabilities against each other.the intuition behind our attack criticality analysis is this: for a given set of deployed defenses  there is a set of attack methods that will succeed despite the presence of those defenses.these attack methods rely upon specific capabilities  and some of those capabilities may be leveraged more than others.thus  attack criticality analysis is a per-capability measurement showing what proportion of successful attack methods rely on that capability being available.computation of attack criticality is based on the partial derivative of the solution-counting formula:  −1  g (d)  (7)  where:  ∀a ∈ a : criticality(a  d) =  ∂g (d)  ∂a • a is the set of all attack capabilities • g is the solution counting function which takes a set of constraints and returns the number of formula solutions under those constraints.• d is a constraint set denoting enabled defenses.computing the partial derivative of g (d) with respect to attack capability a returns two values: the number of solutions when a is enabled and the number when a is disabled [13].for this analysis  we are interested in the case where a is enabled  meaning it is used by successful attack methods.this is then divided by the total number of attack methods given the deployed defenses  creating a proportional attack criticality score in the [0  1] interval.high attack criticality indicates attack capabilities which are used by many successful attack methods  as well as those which have very few of their own dependencies and are thus more generalizable.a lower criticality indicates attack capabilities that are blocked or  74table 1: comparing defense coverage  defense  readactor  stack canaries  constraint memory_accessed → (data ∧ ¬t ext ) ∧ memory_layout_known → (memory_imaдe_known ∧ ¬disk_imaдe_known) ∧ function_pointer_type → (trampoline ∧ ¬direct ) ∧ return_address_type → (trampoline ∧ ¬direct ) ∧ semantic_abstraction_level → (¬counter f eit_objects ∧ counter f eit_procedures) ¬memory_layout_known → (over f low → process_control_data → (¬return_address ∧ (exception_handler ∨ heap_metadata ∨ v irtual_function_t able)))  coverage  0.32  0.046  limited by deployed defenses  as well as those capabilities which have a large number of required dependencies.attack criticality can be used to identify  for some set of deployed defenses  the highest-impact attack capabilities that can be used against that defensive posture.that is  these are the areas where attackers are most incentivized to focus their attacks  and thus in turn where defense planners and researchers should prioritize mitigations.4.2 anticipating high-impact attacks in april 2015 we conducted an experiment to test the real-world utility of attack criticality measures.we further added attack ca- pabilities to our model which were hypothesized in the literature  but had not necessarily been proved feasible in practice.we then enabled a combination of defenses that are ubiquitously deployed (dep and aslr)  as well as the following state-of-the-art defenses: timely address space re-randomization (tasr) [7]  control-flow integrity [2]  readactor [11]  code-pointer integrity (cpi) [22]  and gadget-smashing defenses [17  25  40].this defensive configuration is likely to be impractical in real- world  due to both imposed overhead and potential incompatibilities among defenses.it does  however  represent an ideal best case for defenders and worst case for attacks.the highest-criticality capa- bilities would thus represent where attackers are most incentivized to concentrate their development efforts.we then computed attack criticality against this defensive pos- ture  and identified the five highest-criticality hypothesized capa- bilities as well as the defenses they would bypass  were they to be realized.the results are depicted in table 2.one month after our initial analysis  schuster et al.[31] released an attack against c++ programs using object semantics to con- duct code reuse attacks without relying on any rop gadgets being present.over the next several months  as documented in table 2  four of the five high-impact attack capabilities were developed and published either in the academic literature or as proof-of-concept exploits.the final high-value attack capability that we identified in the top five is a software-based approach to carrying out direct memory access attacks  bypassing the memory permissions en- forced by the memory management unit.hardware variants of this attack have already been published (e.g.attacks against thun- derbolt [33])  but we primarily consider software-based attacks.in order to demstrate the viability of this capability  we created a proof-of-concept attack using software-based forged dma requests to bypass memory protections.5 forged dma attacks as described in section 4  quasar identified the bypassing of memory permissions in software as a high-criticality  undefended  and as-yet unused attack capability in the memory corruption do- main.the key enabler is found in the “access_method” portion of the acg in figure 3  where memory is reachable by either di- rect memory access (dma) or the memory management unit (mmu).memory permissions are enforced by the mmu and our model contains the implicit assumption that the mmu is handing those permissions appropriately and correctly; therefore  bypassing memory permissions must instead be achieved by dma.a slightly different model might split the mmu into multiple categories to highlight other potential access vectors and capabilities  but the overall capability of bypassing memory permissions in software would remain valid and of high criticality.5.1 dma as an attack vector direct memory access (dma) has long been recognized as an alternate path into memory for both malicious [5] and benign [9] purposes.dma is an architectural feature that removes the cpu from the critical path when transferring data between memory and other external hardware devices such as disk drives and network interface cards.this has the dual benefit of both speeding up the data transfer  and allowing the cpu to perform useful work while waiting for the transfer to finish.however  this type of access bypasses the memory management unit  which translates virtual addresses to physical addresses and ensures that page permissions are enforced.attacks that exploit this bypass method have been carried out in both physical and virtualized environments  either by exploiting i/o controllers and firmware in externally connected devices (e.g.usb  firewire  thunderbolt) or simply by building malicious hardware and physically connecting it to a system [30].the need to protect against this type of malicious dma attack led  in part  to the introduction of the iommu (input/output mmu) in x86 and other architectures.if we assume that the mmu is properly configured  an attcker must bypass the mmu to be able to bypass memory permissions in software.however  dma is difficult to perform in a userspace context.an unprivileged userspace process cannot issue a dma request directly to hardware  and must therefore convince the ker- nel to make such a request on its behalf.unfortunately  such a userspace-accessible functionality does exist that can be exploited to trick the kernel into executing a malicious dma request on be- half of an unprivileged process (the “confused deputy” problem).75table 2: anticipating development of attack capabilities  attack capability  partial pointer overwrites software-based dma attacks data-based memory disclosure code reuse inside control-flow graph semantic code reuse  attack criticality 0.663 0.651 0.595 0.543 0.531  defenses bypassed  first appearance in the wild  tasr dep  readactor  cpi aslr  tasr  cpi cfi gadget smashing  nov 2016 [15] section 5 mar 2016 [18] july 2015 [8] may 2015 [31]  we dub such an attack a forged dma (fdma) attack (not to be confused with hardware-based malicious dma attacks [5  30]).the primary example we will use here is the o_direct flag in linux  but other examples exist such as opencl’s cl_mem_use_host_ptr flag.the o_direct flag is used as an argument to block i/o devices to transfer data directly between userspace buffers and the device without going through the kernel.in an fdma attack  a remote at- tacker can corrupt a program’s call to open() which uses o_direct or has a flags argument that can be modified by the attacker.the attacker then subverts a write() call to point to an area of memory he/she does not have permissions to access (e.g.to read a code page marked as executable-only by a defense like readactor [11]).thus  such an attack can bypass memory permissions because the memory access is serviced as a dma request  rather than an mmu access.at a high level  such an attack is equivalent to the attacker being able to run the following code:  1 2 3 4 5  fd = open( code.bin   (o_direct | o_wronly)); write(fd  page_aligned_program_code_pointer  4096); close(fd) fd = open( code.bin   o_rdonly); read(fd  outgoing_buffer  4096);  this copies a portion of the program code to a file regardless of memory permissions  and then re-reads the contents of the file which has not preserved the original permissions.5.2 example exploit against sftp as a proof-of-concept  we have developed an exploit making use of the fdma attack against the standard sftp program  which is part of the openssh suite of applications.the attack targets two functions that are executed when the client uploads a file: process_open and process_write.at a high-level these functions implement the following functionality (simplified for illustration purposes):  1 2 3 4 5 6 7 8 9 10 11  int process_open() {  int fd  flags  mode; char *name; fd = open(name  flags  mode); handle_fd(fd);  } int process_write() {  int fd  data_length; char *data; write(fd  data  data_length);  }  the attack is carried out as follows.• we first leak stack (data memory) to find a code pointer.• we convert the code pointer into a page-aligned code  pointer.• we connect to the sftp server and send a file  code.bin.• the process_open() function is called to write the file to disk.we corrupt the flags variable to include o_direct before the call to open().• the process_write() function is called shortly afterwards.we corrupt the data variable to point to the calculated code address and the length to be block aligned.this causes code from the running sftp process to be written to the specified file regardless of memory permissions.• we retrieve the code.bin file by corrupting a read request.at the end of this process  the file code.bin contains code from the sftp process  having been retrieved without regard to the memory permissions of that piece of memory.for the sake of brevity  we do not include the entire exploit payload in the paper  but we have successfully tested this exploit against a system protected by dep  aslr  and readactor.5.3 evidence of exploitable real-world  conditions  the o_direct flag has been available on linux since kernel version 2.4.10 and  as previously mentioned  is used for block device i/o.the option is used for certain specialized applications such as databases  which handle their own caching and i/o scheduling  but can be used for any appropriate file.from the attacker’s standpoint  this attack is thus best suited to situations where a disk can be used as an intermediate location to store and retrieve a memory dump via a file  but the attack would also work for other block devices where o_direct is applicable.attacks can be carried out on file i/o that already make use of the o_direct flag by corrupting the filename and the data buffer; alternately  attacks can make use of any open() call where the flags are specified in a variable rather than directly coded by also corrupting that flags variable in memory.we investigated the usage of o_direct and of variable-resident flags in a variety of popular webservers (apache  nginx  lighttpd  boa  aolserver  openssh  and squid) and database managers (mon- godb  mysql  postgresql  redis  sqlite  memcached  mariadb  hypertable  and firebird)  both of which make good targets when file i/o is involved.we discovered that use of o_direct was rare by default  being enabled by default in only two out of 16 example programs (firebird and mongodb); however  11 out of those 16 programs (nginx  aolserver  openssh  squid  firebird  hyper- table  mysql  postgresql  momgodb  mariadb  and sqlite) made use of open() with a variable-based flags setting that could be cor- rupted and thus turn on use of o_direct.based on these results   76the opportunity for the fdma attack exits in many real-world applications.6 discussion and lessons learned in this paper  we demonstrated how quasar can help analyze the extent to which attack capabilities may be constrained in the context of an overall attack method  and allow us to quantitatively assess the coverage provided by any particular set of defenses.quasar is well-suited for defense planning and research allocations.however  we also discovered a number of limitations to the general approach of modeling attack capabilities and defenses for satisfiability testing.the effort needed to model attacks at useful levels of abstraction is high  taking large amounts of time and requiring a high degree of domain expertise.during the process of model construction  we had to walk a fine line between defining overly broad capabilities and too finely-grained capabilities.if the model grows too large  the np-hard process of compiling into sddnnf begins to take longer; simultaneously  a model that is too abstract and lacks sufficient atomicity is unlikely to be constrained by realistic defenses  which would reduce the value of quasar as an analysis tool.the definition of defenses in the knowledge base is a similarly challenging problem  requiring an in-depth understanding of the specific defense  of the existing set of attack capabilities  and the ability to express the constraints that the defense puts on those capabilities.as discussed in section 3  we defined a domain-specific language to do so.this language is expressive and allowed us to encode most defenses for memory corruption using only a few statements.in theory  a defense could be significantly complicated enough as to make the encoding task difficult in even a special- ized language  but we found this to be rare in the case of memory corruption.these limitations are largely mitigated by the fact that most of the utility of quasar comes from its use after the acg con- struction has been completed.however  we must also stress that maintenance of attack capabilities and defenses in the knowledge base is important  as the nature of attack classes can change over time as new offensive and defensive paradigms are developed.in many cases  portions of the model may need to have additional layers of atomicity added to fully cover recent developments.7 limitations quasar provides a mechanism for encoding and automatically reasoning about domain knowledge bases.while we find this ap- proach useful for defense planning and research  it does have a number of limitations that makes it unsuitable for solving certain kinds of problem.formal proofs of security  for example  are im- possible using quasar for two reasons.first  the soundness of a conclusion (e.g.that an attack strategy actually bypasses a defense) can only be verified empirically.absent a formal mathematics of cybersecurity which captures the dynamics of all possible defenses and attacks  the most quasar can do is provide recommendations for which attack strategies are likely worth testing empirically.second  the model is necessarily incomplete.quasar formalizes existing domain knowledge.it does not  and cannot  provide any formal guarantees about what domain experts do not know.this is  analogous to the problem of ‘unknown unknowns’  and will persist as a problem for as long as the memory corruption domain remains outside the realm of a formal mathematics of cybersecurity.for example  defense coverage can only be assessed against the space of attack strategies known to the domain expert who creates the model.entirely novel attacks are not considered  and if developed may well bypass a defense.rowhammer [20] is one such example.it operates outside the normal dynamics of memory corruption  and was not anticipated by our analysis.another limitation of quasar is that its measures of attack criticality and defense coverage do not cleanly map to the real world.attack criticality measures how many distinct strategies rely on a shared capability  but not how difficult that capability is to acquire  the probability of it working  the cost of implementation  etc.in reality  attackers may prefer to use a set of low-criticality capabilities because they are easier to acquire  and in the end only one successful strategy is needed to compromise a system.the same limitations apply to defense coverage  which measures the number of attack strategies blocked by a set of defenses.in addition to being incomplete with respect to attack strategies  it also does not provide any indication of how ‘hard’ it is to attack a system in the real world given those strategies which remain.8 related work the closest related work to quasar is attack/defense trees (adtrees) [21]  which is also based on modeling attacker strategies as depen- dency relations rooted in a high-level goal.defenses in this formal- ism  however  act as counter-actions to attack actions in a game played between the attacker and defender  rather than constraints over attack capabilities.the game is won when either the attacker or defender succeeds by preventing the other from achieving their top-level goal.this all-or-nothing approach means that adtrees do not support quantitative analysis such as defense coverage or attack criticality.attack countermeasure trees [28] are a stochastic approach to modeling attacks and mitigation strategies.they support single- and multi-objective optimization for determining returns on the attacker and defender investments.defenses cannot arbitrarily con- strain attacker options  however  nor can they be easily compared against one another.szekeres et al.[39] provide a comprehensive semi-formal taxon- omy of memory corruption attackers and defenses.this taxonomy is not intended for automated analysis  however  and does not pro- vide a way to quantitatively analyze attacks or defenses.the work by skowyra et al.[36] is a precursor to quasar  which is based on qualitative analysis of attacks and defenses us- ing boolean sat solvers.thus  it does not support measures for comparing defenses or identifying high-impact attacks.9 conclusion the framework and methodology as described in this paper and im- plemented via quasar  allows us to quantify the extent to which different attack capabilities are constrained with respect to an over- all attack method  determine the coverage of a defense with respect to constraints on attack capabilities  and identify high-impact pri- ority areas for both attackers and defenders.we have done so by  77docs/aslr.txt  [25] onarlioglu  k.bilge  l.lanzi  a.balzarotti  d.kirda  e.: g-free: defeating return-oriented programming through gadget-less binaries.in: proc.of acsac (2010)  [26] openbsd: openbsd 3.3 (2003)  http://www.openbsd.org/33.html [27] pax: pax address space layout randomization (2003)  http://pax.grsecurity.net/  (2013)  [28] roy  a.kim  d.s.trivedi  k.s.: cyber security analysis using attack counter- measure trees.in: proceedings of the sixth annual workshop on cyber security and information intelligence research (2010)  [29] rudd  r.skowyra  r.bigelow  d.dedhia  v.hobson  t.crane  s.liebchen  c.larsen  p.davi  l.franz  m.sadeghi  a.r.okhravi  h.: address-oblivious code reuse: on the effectiveness of leakage-resilient diversity.in: ndss (2017) [30] sang  f.l.nicomette  v.deswarte  y.: i/o attacks in intel pc-based architectures  and countermeasures.in: syssec workshop (syssec)  2011 first (2011)  [31] schuster  f.tendyck  t.liebchen  c.davi  l.sadeghi  a.r.holz  t.: counterfeit object-oriented programming: on the difficulty of preventing code reuse attacks in c++ applications.in: proc.of ieee s&p (2015)  [32] seibert  j.okhravi  h.soderstrom  e.: information leaks without memory disclosures: remote side channel attacks on diversified code.in: proc.of acm ccs (2014)  [33] sevinsky  r.: funderbolt: adventures in thunderbolt dma attacks.black hat usa  [34] shacham  h.: the geometry of innocent flesh on the bone: return-into-libc  without function calls (on the x86).in: proc.of acm ccs.pp.552–561 (2007)  [35] sheyner  o.haines  j.jha  s.lippmann  r.wing  j.m.: automated generation  and analysis of attack graphs.in: proc.of ieee s&p (2002)  [36] skowyra  r.casteel  k.okhravi  h.zeldovich  n.streilein  w.: systematic analysis of defenses against return-oriented programming.in: proc.of raid (2013)  [37] snow  k.z.monrose  f.davi  l.dmitrienko  a.liebchen  c.sadeghi  a.r.: just-in-time code reuse: on the effectiveness of fine-grained address space layout randomization.in: proc.of ieee s&p (2013)  [38] strackx  r.younan  y.philippaerts  p.piessens  f.lachmund  s.walter  t.: breaking the memory secrecy assumption.in: proc.of eurosec’09.pp.1–8 (2009) [39] szekeres  l.payer  m.wei  t.song  d.: sok: eternal war in memory.in: proc.of ieee s&p (2013)  [40] wartell  r.mohan  v.hamlen  k.w.lin  z.: binary stirring: self-randomizing  instruction addresses of legacy x86 binary code.in: proc.of acm ccs (2012)  [41] zou  c.c.towsley  d.gong  w.: modeling and simulation study of the propaga-  tion and defense of internet e-mail worms.ieee tdsc 4(2) (2007)  fully specifying a model for the memory corruption domain and have run an analysis on it  including the identification of the top five high-priority areas.this identification was subsequently con- firmed by the recent publication of various attacks in four out of five of those areas  and our own proof-of-concept exploit against the fifth.we believe that quasar can serve as a tool for defense planning and strategic thought about defensive capabilities in large organizations and in research planning and priority allocations.future work includes expanding the knowledge base for addi- tional attack domains (e.g.web-based attacks)  defining and com- puting improved measures of functional defenses comparisons  and the operationalization of the tool for use in different settings.press (2009) 
19,2017-Replication- Challenges in Using Data Logs to Validate Phishing Detection Ability Metrics.pdf,  the security behavior observatory (sbo) is a longitudinal field- study of computer security habits that provides a novel dataset for  validating  computer  security  metrics.this  paper  demonstrates  a  new  strategy  for  validating phishing  detection  ability  metrics  by  comparing performance on a phishing signal detection task  with  data logs found in the sbo.we report: (1) a test of the robustness  of  performance  on  the  signal  detection  task  by  replicating  canfield   fischhoff   and  davis  (2016)   (2)  an  assessment  of  the  task s  construct  validity   and  (3)  evaluation  of  its  predictive  validity using data logs.we find that members of the sbo sample  had similar signal detection ability compared to  members of the  previous  mturk  sample  and  that  performance  on  the  task  correlated  with  the  security  behavior  intentions  scale  (sebis).however   there  was  no  evidence  of  predictive  validity   as  the  signal  detection  task  performance  was  unrelated  to  computer  security outcomes in the sbo  including the presence of malicious  software   urls   and  files.we  discuss  the  implications  of  these  findings and the challenges of comparing behavior on structured  experimental tasks to behavior in complex real-world settings. 1,  maintaining security on a home computer requires knowing which  security practices are most important [18] and implementing those  practices  even when they may be inconsistent with users’ mental  models  of  computer  security  [3   43   44].users  are  expected  to  keep their software up to date (both individual programs and their  operating  system)   avoid  suspicious  links  and  attachments  (i.e.phishing  attacks)   choose  secure  passwords   and  install  security  programs  (e.g.anti-virus).many  struggle  to  understand  and  follow all these recommendations  despite good intentions.meanwhile  cyberattacks are becoming more varied and pervasive  [39  40]  where about 1 in every ,  that the study replicated even in the face of large differences  and  high statistical power would lead to conclusions that the study did  not replicate even if the differences in effect sizes were trivial.our  second  test  (2)  assesses  the  consistency  of  the  regression  coefficients in the replication study with the null hypothesis that  the regression coefficient is exactly zero.the p-value on the t-test  of  each  this  measure  of  consistency [45].if the p-value is below the .05 alpha level  we  conclude that the regression coefficient from the replication study  is  inconsistent  with  zero   and  that  the  study  successfully  replicated.the limitation of this second approach is the opposite  of  the  first   where  lower  statistical  power  would  lead  to  fewer  conclusions  that  the  study  successfully  replicated  even  if  the  regression coefficient was large  and high statistical power would  lead  to  more  conclusions  that  the  study  successfully  replicated  even if the regression coefficient was small.third  we assess (3) the region of the parameter space ruled out by  confidence  intervals.in  the  original  and  replication  studies   we  construct 95% confidence intervals.each  interval either does  or  does not cover the population parameter  and if we conclude that  it does include the population parameter  then we will be wrong  5%  of  the  time  (i.e.the  population  parameter  falls  outside  the  interval).therefore   a  successful  replication  would  find  similar  conclusions about the population parameter (i.e.that the region of  the  parameter  space  outside  the  interval  in  the  two  studies  is  “similar”).we  operationalize  this  similarity  as  having  a  non- empty union of the two intervals  or that the intervals overlap.in  other words  we judge that a study replicated the first if the two  studies do not rule out all of the parameter space.this approach  has  the  same  limitations  as  the  first   of  always  concluding  successful  replication  with  a  low  sample  size   and  never  concluding successful replication with a large sample size.fourth   we  assess  (4)  a  combined  regression  analysis.we  assessed whether the replication was successful by combining the  two studies into a single linear regression analysis.a successful  replication  is  then  drawing  the  same  conclusion  using  the  combined  data  as  the  original  data.this  analysis  improves  the  power  of  the  statistical  tests  due  to  the  increased  sample  size  achieved by combining the two samples.when considered together  these tests provide insight into whether  the replication was successful.one of the primary challenges in  assessing  whether  a  replication  is  successful  is  accounting  for  type ii error (i.e.incorrectly accepting the null hypothesis).in the  context of replication  this is the probability of incorrectly finding  that the replication is successful  when in truth it is not.in this  study   the  sample  size  of  the  replication  is  constrained  by  the  existing  sbo  participant  pool   which  limited  our  ability  to  perform a higher-powered test and increases the chance of type ii  errors.to account for this  we interpret a failure to reject the null  hypothesis (i.e.finding that there is no difference in effect size or   we would expect to find at least one false positive (55% chance).is  conservative  for  type  i  errors   but  not  type  ii  errors. hypothesis test result) as a lack of evidence of a difference  rather  than  evidence  that  there  is  no  difference. similarly   confidence  intervals  tend  to  be  larger  when  the  sample  size  and  statistical  power  are  lower   increasing  the  likelihood  that  our  replication  meets  our  definition  of  success.therefore   it  is  critical  to  not  over-interpret  these  results.rather   this  is  a  first  attempt  to  use  data logs for validation.as more data is collected  the strength of  replication studies using this approach will improve.3.6  analysis  in  the  analysis  that  follows   we  first  reproduce  the  phishing  detection experiment by canfield et al.[4] to assess whether sbo  participants  perform  differently  than  amazon  mechanical  turk  [32]  participants  (mturk).we  assess  differences  between  the  samples using t-tests (t)  chi-squared tests (χ2)  and 2-sided mann- whitney-wilcoxon  (w)  tests  where  appropriate.given  the  large  number of statistical tests across disparate analyses  we generally   use   = .01 as a threshold for interpretation  rather than applying  in the regression analysis  with 11 independent tests and   =.05   using   =.01 reduces this chance to 11%.however  using   =.01  therefore   we  interpret  significance  using    =.05  for  the  replication (where type ii error matters most) and   =.01 for the   separate corrections to groups of tests.we replicate the estimation  of  the  sdt  parameters  and  the  linear  regression  analysis  to  determine  any  differences  in  which  factors  predict  performance.remaining analysis (where type i error matters most).second  we assess the experimental measures’  construct validity  with  the pearson  correlation between  the  sdt  parameters and  a  validated  measure  of  security  intentions   the  security  behavior  intentions scale (sebis) [6].third   we  assess  predictive  validity  by  whether  the  sdt  parameters  improve  the  fit  of  logistic  models  for  predicting  observations  of  negative  computer  security  outcomes  for  sbo  participants  (i.e.observations  of  malicious  urls   files   and  software).for  each  outcome   we  construct  a  logistic  regression  model comprised of the sdt parameters and other predictors of  exposure and risky behavior.this serves to test two hypotheses.we  expect  users  who  are  more  susceptible  to  phishing  on  the  experimental  measure  to  experience  more  negative  computer  security outcomes in real life.thus  our first hypothesis is:   h1:  users  who  are  more  susceptible  to  phishing  in  the  sdt  experiment  (i.e.are less able to  detect and avoid threats) are  more  likely  to  visit  malicious  urls  and  have  malware  and  malicious files on their computer.we test h1 using a likelihood ratio test  which compares goodness  of  fit  for nested logistic regression models  with and without the  sdt parameters.the likelihood ratio test is the most efficient test  of the null hypothesis that the sdt measures do not increase the  likelihood  of  the  data  given  the  sdt  measures  [15   16].the  second hypothesis we test is:   h2:  users  who  use  their  computers  more  (i.e.have  greater  exposure) or engage in more risky behavior are more likely to  visit  malicious  urls  as  well  as  have  malware  and  malicious  files on their computer.we test h2 in the construction of the logistic regression models   following the procedure recommended by hosmer et al.[16].usenix association  thirteenth symposium on usable privacy and security    275  4.results  4.1  sample  we recruited 132 sbo participants to participate in the phishing  detection  experiment.of  those   121  started  the  survey  and  98  finished (= 74% response rate).we excluded 5 participants who  sent the sbo less than 7 days of data.the final sample (see sbo  sample  in  table  2)  represents  44%  (=  93/213)  of  all  the  sbo  participants at that time (all sbo in table 2).as shown in table  2  the sbo sample was older  t(121) = 4.52  p < .001  cohen’s d =  0.69  and had a higher proportion of college-educated individuals      (1) = 6.83  p = .009    = 0.17  than did the mturk sample in  canfield  et  al.[4].1  there  was  no  difference  in  gender    (1)  =  0.05   p  =  .823     =  0.01.within  the  sbo  sample   older   participants tended to be better educated  in part because some of  the  younger  participants  were  in  college  (thus  had  not  finished  their  educations)   r(92)  =  0.37   p  <  .001.our  sbo  sample  resembled the wider sbo population on these variables (table 2).table 2.comparison of mturk and sbo demographics.the  mturk sample is from canfield et al.[4].variable  female   bachelors+   mturk  58%  45%   sbo sample  all sbo   60%  63%   61%  58%   46 [19  87]   age  n   32 [19  59]   41 [19  81]   93   152   213  4.2  comparison of experimental results  (replication)  there was little difference between how much attention the sbo  and  mturk  participants  paid  to  instructions.of  the  93  sbo  participants  16 failed at least 1 of the 3 attention checks.users  who  failed  the  attention  checks  were  not  excluded  from  the  sample  but attention was included as a variable in the regression  analysis in order to increase statistical power [30].there were no  significant  differences  in  performance  on  the  attention  checks    17% failed for sbo vs.10% failed for mturk    (1) = 2.18  p =  .14    = 0.09.the median time spent on the introductory phishing   information was slightly higher for the mturk participants  sbo =  0.74 minutes (m = 1.16  sd = 1.79) vs.mturk = 0.95 minutes (m  = 3.17  sd = 11.51)  w = 5018  z = 2.25  p = .02  r = 0.14.however   sbo  participants   particularly  the  older  ones   spent  more  time  on  the  individual  email  stimuli.the  median  time  to  complete the experiment was 47 minutes  including breaks (m =  59  min   sd  =  49  min).this  estimate  excludes  seven  outliers   participants  who  appeared  to  stop  working  and  leave  the  experiment open on their browser for 19 hours to almost 2 weeks.sbo participants spent more time per email  sbo = 0.94 minutes  (m = 1.13  sd = 0.72) vs.mturk = 0.48 minutes (m = 0.53  sd =  0.24)   w  =  11850   z  =  8.88   p  <  .001   r  =  0.57  in  a  mann- whitney-wilcoxon  sample   older  participants spent more time per email  r(92) = 0.46  p < .001.first   we  assess  whether  the  results  of  the  sdt  parameter  estimation replicate.since these are point estimates  there are no  hypothesis tests to replicate.there was no evidence of significant  differences  between  the  mturk  and  sbo  samples  on  any  sdt  parameters  for either the detection or the behavior task  p > .05.however  the point estimates differ by 0.12 for detection c  which   test.within   the  sbo   exceeds  our  meaningful  difference  threshold.when  comparing  the confidence intervals  the replicated point estimate is within the  original  study’s  confidence  interval  for  d’  and  behavior  c.for  detection c  the replicated point estimate is outside of the original  confidence interval   but the  confidence intervals  still  overlap.in  general   there  is  no  evidence  that  the  sdt  estimates  differ  between  the  studies   although  the  evidence  is  weakest  for  detection  c.table  3  shows  the  mean  statistics  for  the  sdt  parameters  and  accuracy  (for  comparison).figure  1  shows  the  distribution of d’ and c for each task and sample.there was no  evidence of learning over the course of the experiment  as d’ and c  were equal when calculated separately for the first and second half  of  the  emails.this  suggests  that  the  performance  parameters  estimated in canfield et al.[4] are not unique to mturk and can be  generalized  to  the  sbo  population   which  was  an  older   potentially less tech-savvy group.we also replicated the regression analysis from canfield et al.[4]  to determine whether there were any differences in the factors that  predicted phishing vulnerability for the two samples.tables 4 and  5 show the results for both samples to compare the results of the  hypothesis tests.figure 2 compares the 95% confidence intervals.in  general   the  sbo sample’s coefficients had  larger confidence  intervals   due  to  the  lower  sample  size   but  overlap  the  mturk  coefficients   suggesting  no  statistically  significant  differences.the results were largely the same  except for the following three  differences.first  unlike canfield et al.’s mturk sample  confidence was not a  significant predictor of response bias (c) for the sbo sample.we  found no systematic differences in mean confidence between the  two samples  m = 0.86 (sd = 0.08) for sbo and mturk  t(181) =  0.04  p  = .97   cohen’s d  = 0.01.second   age and  education  are  predictors  of  c  in  the  sbo  sample   but  were  not  in  the  mturk  sample  perhaps due to the higher variance of age and education in  the  sbo  sample.older  participants  seemed  biased  toward  identifying  emails  as  phishing  (i.e.lower  detection  c).college- educated participants seemed biased toward identifying emails as  legitimate (higher detection c).third  attention and median time  per  email  were  not  significant  predictors  for  the  sbo  sample   perhaps due to reduced variance  as sbo participants were more  likely to fail the attention checks and spent more time per email.as  also  reported  in  tables  4  and  5   the  combined  analysis  is  largely consistent with the original canfield et al.experiment for  sensitivity   but  there  are  differences  for  response  bias.higher  attention  and  higher average confidence predict  higher detection  sensitivity   consistent  with  the  original  canfield  et al.(p  < .01).none  of  the  predictors  are  significant  for  behavior  sensitivity   consistent  with  the  original  canfield  et  al.(p  <  .01).higher  average  confidence   lower perceived  consequences   and  younger  individuals tended to have a higher detection response bias  which  differs  from  the  original  canfield  et  al.study  (p  <  .01).in  the  separate  analysis   age  is  significant  for  the  sbo  sample  but not  the  mturk  sample  and  average  confidence  is  significant  for  the  mturk  sample  but  not  the  sbo  sample.higher  average  confidence and lower perceived consequences are associated with  a higher behavior  response bias   which  differs  from  the  original  canfield et al.study (p < .01).in the separate analysis  the median  time spent per email is significant for the mturk sample and none  of the predictors are significant for the sbo sample.276    thirteenth symposium on usable privacy and security  usenix association  table 3.sdt phishing vulnerability parameter estimates for mturk [4] and sbo samples.  detection task (yes/no)     mturk  m (sd)  [ci]   0.96 (0.64)  [0.86  1.06]  0.32 (0.46)  [0.24  0.39]  0.67 (0.11)  [0.65  0.69]   sbo  m (sd)  [ci]   0.96 (0.66)  [0.83  1.10]  0.20 (0.51)  [0.10  0.30]  0.67 (0.11)  [0.65  0.69]   sensitivity (d’)   response bias (c)   accuracy        t(191) = 0.01    p = .99  d = 0  t(178) = -1.78    p = .08  d = 0.24  t(193) = 0.03    p = 0.98  d = 0      behavior task (multiple choice)   mturk  m (sd)  [ci]   0.39 (0.50)  [0.31  0.47]  -0.54 (0.66)  [-0.64  -0.43]  0.56 (0.08)  [0.55  0.57]   sbo  m (sd)  [ci]   0.42 (0.52)  [0.32  0.53]  -0.62 (0.57)  [-0.74  -0.51]  0.57 (0.09)  [0.55  0.59]         typical   range   t(190) = 0.41    p = .69  d = 0.05  t(216) = -1.07    p = .29  d = 0.14  t(179) = 0.99    p = .32  d = 0.13   0 to 4   -2 to 2   0 to 1   figure 1.plot of d’ vs.c for each task  and sample.the parameter estimates are bounded by  the dotted lines   which represent  extreme performance (no false alarms or no misses).there were no significant differences in performance between the mturk (a   b) [4] and sbo (c  d) samples.         usenix association  thirteenth symposium on usable privacy and security    277  figure 2.comparison of regression coefficients with 95% confidence intervals (ci) for (a) detection d’  (b) detection c  (c) behavior   d’  and (d) behavior c.          278    thirteenth symposium on usable privacy and security  usenix association         table 4.comparison of linear regression analysis of detection and behavior sensitivity (d’) for mturk [4] and community (sbo)   samples.the asterisks indicate statistical significance  where * is p < .05  ** is p < .01  and *** is p < .001.behavior sensitivity (d’b)   detection sensitivity (d’d)   intercept  sample (sbo = 1)  knowledge of base rate  task order (detection = 1)  attention (pass = 1)   log(phish info time)  median time/email  average confidence  average perceived consequences  0.08 (0.08)  log(age)  -0.22 (0.21)  0.11 (0.10)  gender (male = 1)  college (college degree = 1)  0.19 (0.10)  n  adjusted r2  f    142  0.16   3.71***   combined   combined   mturk  b (se)   b (se)  -0.96 (0.64)  -0.04 (0.10)  -0.05 (0.08)  0.09 (0.08)  0.40 (0.13)**  0.05 (0.03)  0.21 (0.09)*   sbo  sbo  mturk  b (se)  b (se)  b (se)  b (se)  0.19 (0.54)  -0.09 (0.83)  0.61 (0.77)  -0.97 (0.92)  -1.32 (0.98)  0.15 (0.08)          0.08 (0.07)  0.05 (0.12)  0.10 (0.08)  -0.22 (0.14)  0.02 (0.10)  -0.06 (0.07)  -0.14 (0.12)  -0.05 (0.09)  0.04 (0.10)  0.15 (0.14)  0.10 (0.11)  0.07 (0.16)  0.12 (0.15)  0.49 (0.18)**  0.33 (0.19)  0 (0.03)  -0.03 (0.03)  0.07 (0.06)  0.02 (0.07)  0.05 (0.04)  -0.08 (0.08)  -0.13 (0.09)  0.48 (0.23)*  0.17 (0.19)  0.23 (0.11)*  0.84 (0.43)  0.71 (0.73)  2.23 (0.67)**  3.46 (0.87)***  2.64 (0.51)***  1.11 (0.57)  0.08 (0.05)  0 (0.08)  0.11 (0.06)  -0.26 (0.11)*  -0.33 (0.17)  -0.16 (0.16)  0.15 (0.12)  0.06 (0.09)  0.10 (0.07)  -0.18 (0.13)  0.02 (0.07)  0.10 (0.09)   0.07 (0.06)  -0.33 (0.13)*  0.04 (0.08)  0.11 (0.08)   0 (0.10)  -0.40 (0.19)*  -0.09 (0.15)  -0.03 (0.16)   84  0.14  2.37*   227  0.15   4.63***   142  0.05  1.68   84  0.05  1.40   227  0.05  2.16*      table 5.comparison of linear regression analysis of detection and behavior response bias (c) for mturk [4] and community   samples.the asterisks indicate statistical significance  where * is p < .05  ** is p < .01  and *** is p < .001.      detection response bias (cd)   behavior response bias (cb)   combined   sbo  b (se)  1.31 (0.62)*    0 (0.10)  0.18 (0.10)  0.07 (0.13)  0.04 (0.05)  0.13 (0.08)  0.62 (0.59)  -0.24 (0.07)***  -0.38 (0.13)**  0.05 (0.10)  0.39 (0.11)***   mturk  b (se)  0.06 (0.70)    0.01 (0.07)  -0.01 (0.07)  0.08 (0.13)  0.01 (0.03)  0.10 (0.16)  1.81 (0.48)***  -0.24 (0.05)***  -0.17 (0.15)  -0.13 (0.07)  0.02 (0.07)  142  0.18   intercept  sample (sbo=1)  knowledge of base rate  task order (detection=1)  attention (pass = 1)  log(phish info time)  median time/email  average confidence  avg perceived consequences  log(age)  gender (male=1)  college (college degree=1)  n  adjusted r2  f     4.3  construct validity  we assessed construct validity as the correlation between the sdt  parameters  and  the  proactive  awareness  subscale  of  the  sebis.one of the four sdt parameters  behavior c (i.e.how suspicious a  link must be before the participant chooses not to click on it)  was  correlated with the sebis proactive awareness subscale  r(83) =    -0.29   p  =  .008.none  of  the  other  sdt  parameters  had  a  correlation  greater  than  0.20.thus   participants  who  reported  looking at the url before clicking on links (in the sebis) were  also more cautious in the experimental task (behavior c).4.4  predictive validity  for  simplicity’s  sake   we  only  report  tests  of  predictive  validity  for the behavior task  as results for the detection task were similar.below  we report our analyses separately for each of the four sbo  computer security negative outcomes.84  0.27   4.16***   4.12***   b (se)  0.81 (0.46)  -0.12 (0.07)  -0.01 (0.06)  0.01 (0.06)  0.07 (0.09)  0.01 (0.02)  0.14 (0.06)*  1.30 (0.36)***  -0.26 (0.04)***  -0.27 (0.09)**  -0.06 (0.06)  0.12 (0.06)*   227  0.21   6.44***   mturk  b (se)  0.10 (0.87)    0.13 (0.09)  0.11 (0.09)  -0.19 (0.16)  0.01 (0.04)  -0.70 (0.20)***  2.38 (0.59)***  -0.42 (0.07)***  -0.22 (0.18)  -0.14 (0.09)  -0.13 (0.09)   142  0.39   9.85***   sbo  b (se)  -0.08 (0.81)    0.07 (0.13)  0.12 (0.13)  -0.13 (0.17)  0 (0.06)  -0.10 (0.10)  0.93 (0.77)  -0.20 (0.09)*  -0.18 (0.16)  0.11 (0.13)  0.18 (0.14)   84  0.07  1.63   combined   b (se)  -0.14 (0.58)  0.13 (0.09)  0.10 (0.07)  0.08 (0.07)  -0.13 (0.12)  0 (0.03)  -0.17 (0.08)*  1.92 (0.47)***  -0.36 (0.05)***  -0.21 (0.12)  -0.05 (0.08)  -0.02 (0.08)   227  0.25   7.81***   4.4.1  malicious urls in browser data  browser  data  were  available  for  86  of  the  93  sbo  users.most  used internet explorer (66/86 = 77%)  followed by chrome (29/86  =  34%)  and  firefox  (12/86  =  14%).some  participants  used  multiple  browsers   so  the  percentages  do  not  sum  to  100%.in  total  9 participants (10%) had visited a malicious url: 2 internet  explorer users (2/66 = 3%)  4 chrome users (4/29 = 14%)  and 3  firefox users (3/12 = 25%).table  6  shows  our  univariate  analyses  [16]  for  the  relationship  between each potential predictor and whether users had visited a  malicious  url.among  these  potential  covariates   only  domains/day  was  related  to  whether  participants  had  visited  malicious  urls.therefore   it  was  included  in  the  regression  model  using a log transformation to normalize the observations.users who visited more domains were more likely to have visited  a  malicious url.table 8  shows  the  regression  analysis  for  the   usenix association  thirteenth symposium on usable privacy and security    279  the  only  significant  browser  data.log(domains/day)  was  predictor.as seen in the likelihood ratio test (reported in the last  row of table 8)  users’ sdt parameter estimates did not improve  the model fit.this indicates that there was no evidence that ability  to  identify  phishing  emails in  the experiment  (as  represented  by  the  sdt  parameters)  was  related  to  whether  participants  had  visited a malicious url in the browser data.4.4.2  malicious urls in network packet data  we also assessed visits to malicious urls in the network packet  data.there  was  much  more  network  packet  data  than  browser  data  (table 6)   since  a single  webpage is assembled  from  many  network packets [17].for 31 of 93 sbo users (33%)  the network  packet  data  indicated  that  they  had  visited  a  malicious  url.univariate  analysis  [16]  found  that  total  urls/day   unique  urls/day   and  domains/day  were  related  to  having  visited  a  malicious url at least once.we then computed a factor analysis   which revealed that these covariates loaded on one factor  alpha =  0.79.we  called  this  factor  browsing  intensity  and  used  a  log  transformation to normalize it.we then used that factor score in  the regression model and likelihood ratio test reported in table 8.the  regression  analysis  shows  that  users  with  higher  browsing  intensity were more likely to have visited a malicious url in the  network packet data.in addition  there was an effect for gender   whereby  men  were  more likely to have visited  malicious urls.this  finding  emerges  after  normalizing  for  exposure  (in  the  regression analysis) and observing no correlation between gender  and exposure  r(90) = .06  p = .57.this suggests that men were  either  more  likely  to  visit  malicious  urls  in  their  browsing  or  worse at detecting malicious urls in this sample.more research  is  needed  to  understand  this  result.in  the  likelihood  ratio  test   users’  sdt  parameter  estimates  did  not  improve  the  model  fit.thus  there was no evidence to  suggest that performance on the  sdt experiment was related to whether participants had visited a  malicious url in the network packet data.table  6.descriptive  statistics  and  factor  analysis  for  the  browser and network packet sensor predictors.network packet   browser   m  (sd)  median  70   median  40   days   total urls   unique urls   22   9   67  (76)  56  (90)  23  (32)  5.7  (4.4)   1 500   670   loading  na   0.73   1   m  (sd)  85  (63)  2 600  (3 600)  990  (1 000)  52  (37)   5   42   0.60   63%  0.80   domains    % of total variance  cronbach s alpha    4.4.3  malware  most users had the windows 10 operating system (53/92 = 58%)   followed  by  windows  8  (22/92  =  24%)   windows  7  (14/92  =  15%)  and windows vista (3/92 = 3%).43 of the 92 (47%) users  with  installed  software  data  had  malware.for  each  operating  system  approximately half of the users had malware.table 7 shows descriptive statistics for viable software covariates.univariate analysis [16] revealed that total software and delayed   software  updates  were  related  to  malware.however   the  factor  analysis  found  that  these  variables  were  only  weakly  related.when  included  in  the  regression  model  separately   delayed  software  updates  were  not  a  significant  predictor   so  it  was  removed from the model.total software was normalized using a  log transformation.users  who  installed  more  software  were  more  likely  to  have  malware  on  their  machine.as  shown  in  table  8   this  variable  predicted  malware.again   the  sdt  parameter  estimates  did  not  improve  the  model  fit.thus   there  was  no  evidence  that  performance on the sdt experiment was related to observations  of malware on a participant’s computer. 4.4.4  malicious files  most users (84/93 = 90%) had malicious files on their computer.in  the  regression  model   we  used  the  same  predictors  as  in  the  malware model  reported in table 7.the  regression  analysis  (table  8)  shows  that  users  who  had  installed  more  software  were  significantly  more  likely  to  have  malicious files on their  computer.the  sdt  parameter estimates  did not improve the model fit.thus  there was no evidence that  performance on the sdt experiment was related to observations  of malicious files on a participant’s computer.table  7.descriptive  statistics  and  factor  analysis  for  the  software predictors.  total software   delayed software updates   % of total variance  cronbach s alpha   244   median  m  (sd)  loading  342  0.44  (316)  2  (1)   0.44   2   20%  0.33   5.discussion  in this study  we reproduced experiment 1 from canfield et al.[4]  in a community sample (sbo).we assessed replicability in terms  of  the  effect  sizes   results  of  the  hypothesis  tests   confidence  intervals   and  combined  analysis.in  general   we  found  similar  distributions of the sdt performance  measures as in the  mturk  sample   suggesting  that  there  was  no  evidence  of  differences  in  performance  between  the  two  samples.however   although  the  performance of the two samples replicated (as defined in section  3.5)   the  regression  analysis  differed  slightly   reflecting  the  differences  between  the  samples  in  terms  of  age  and  education.this analysis suggests that a higher-powered study with a diverse  sample  is  needed  to  assess  demographic  effects.however   the  findings about confidence and perceived consequences are fairly  consistent   suggesting  that  they  may  be  useful  parameters  for  future behavioral interventions and predictive metrics.we  the  experimental behavior  task   consistent  with  it  measuring  what  it  claimed.participants with a greater response bias on the behavior  task  (cb)   or  tendency  to  treat  uncertain  emails  as  phishing   had  higher scores on the sebis proactive awareness subscale   which  elicits  self-reports  of  attention  to  urls.this  suggests  that  participants  were  acting  on  their  computer security  intentions  in  the  sdt  experiment.the  other  sdt  parameters  were  not  correlated with sebis.this suggests that ability (d’) is not related    found  some  evidence  of  construct  validity   for   280    thirteenth symposium on usable privacy and security  usenix association  table 8.logistic regression models and likelihood ratio test (lrt) for each outcome.the lrt compares the full models shown  above with the same models excluding the 2 sdt parameters.the asterisks indicate statistical significance  where * is p < .05  ** is  p < .01  and *** is p < .001.malicious urls  (browser)  -6.43 (2.14)**  -0.06 (0.89)  -0.80 (0.74)  1.93 (0.77)*      0.01 (0.03)  0.90 (0.81)  -0.89 (0.95)   malicious urls  (network packet)  -10.53 (2.83)***  -0.33 (0.55)  0.11 (0.50)    1.39 (0.38)***    -0.03 (0.02)  1.47 (0.55)**  0.16 (0.61)   malware   -5.93 (1.71)***  -0.09 (0.46)  -0.06 (0.44)      0.99 (0.31)**  0 (0.01)  0.07 (0.48)  0.56 (0.53)     (2) =1.29  p = 0.5    (2) = 0.41  p = 0.8    (2) = 0.06  p =1.0    (2) = 4.12  p = 0.13               malicious   files  -6.65 (3.71)  -1.59 (1.04)  -0.90 (1.22)      2.58 (0.87)**  -0.05 (0.03)  -0.64 (0.94)  -1.29 (1.29)      (int)  behavior sensitivity (d b)  behavior response bias (cb)  log(domains/day)  log(browsing intensity)  log(total software)  age  male  college  lrt     to  security  intentions.the  response  bias  (c)  for  the  detection  task  measures  participants’  tendency  to  identify  emails  as  phishing or legitimate.although this could have been related to  security intentions  the behavior task better matched the sebis  scale due to the higher consequences associated with behavior.we  found  no  evidence  of  predictive  validity  for  the  sdt  parameters for any of the four computer security outcomes in the  sbo data: browser visits to malicious urls and network packet  data   malware   and  malicious  files.thus   we  reject  h1.however   those  four  measures  were  robust  enough  to  be  predicted by other observation-based measures  as hypothesized  by  h2.sbo  participants  who  used  their  computers  more  frequently  were  more  likely  to  have  experienced  a  negative  computer security outcome.we  offer  four  possible  reasons  why  the  ability  to  identify  suspicious  messages  in  the  laboratory  task  did  not  predict  the  ability to identify similar suspicious messages in the real world:   1.the  experimental  task  does  not  evoke  true  phishing  behavior    the  experimental  in  an  environment  different  from  sbo  users’  (i.e.lack  of  ecological validity)    the  sbo  measures  are  confounded  by  other  aspects  of  users’ complex real-world experience  or  the  sbo  data  are  too  noisy  to  reveal  the  underlying  correlations without much larger samples.true  behavior   task  evokes   2.3.4.explanation  (1)   that  the  experiment  does  not  evoke  actual  behavior  seems unlikely  as the results of the experiment are in  line  with  other  phishing  susceptibility  research.for  example   participants  who  perceived  worse  consequences  were  more  cautious (negative c) [34  42  47].moreover  performance on the  sdt  experiment  showed  expected  correlations  with  other  variables   such  as  better  performance  being  associated  with  greater security intentions (in the test of construct validity).explanation  (2)   lack  of  ecological  validity  for  the  experiment  environment   seems  more  plausible.one  unrepresentative  feature of the experimental task is that it has a 50% base rate of  phishing  emails   much  higher  than  that  in  everyday  life  [34].that  higher  rate  seems  likely  to  have  influenced  the  sdt  estimates.in a sdt study of baggage screening  artificially high  base rates decreased c (i.e.encouraged participants to be more  biased  toward  identifying items  in  baggage as suspicious)   but      had no effect on d’ (i.e.people’s ability to differentiate between  suspicious  and  benign  items  in  baggage)  [46].analogous  behavior  here  would  have  been  a  greater  propensity  to  treat  messages  as  phishing  in the  experiment  than  in life.a  second  feature of the experimental task is explicitly asking participants  to  evaluate  each  email  for  phishing   thereby  priming  them  to  detect  attacks.research  by  parsons  et  al.[33]  suggests  that  explicitly  mentioning  phishing  artificially  increases  d’  but  has  no effect on c.together  these studies suggest that our estimates  of performance are better than what  would be expected in real  life.however  there is no obvious reason why these differences  should  affect  users’  relative  performance.thus   we  would  expect  users  who  are  good  at  detecting  phishing  to  perform  better  on  the  experiment  than  users  who  are  bad  at  detecting  phishing.as a result  the correlations across measures should be  preserved.in other words  we would not expect users who are  bad  at  detecting  phishing  in  real  life  to  be  better  at  it  in  an  experiment   compared  to  users  who  are  good  at  detecting  phishing in real life.explanation (3)  that the complexity of real-world environments  (for  sbo  participants   among  others)  complicates  the  relationship  between  individuals’  general  propensities  (which  the  sdt  metrics  attempt  their  actual  experiences   is  also  compelling.as  seen  here   negative  experiences (in the sense of visiting malicious urls and having  malicious files)  are strongly related to the amount of exposure  (as measured by browsing intensity and total software).perhaps  individuals’ exposure to threats overwhelms their ability (d’) or  propensity (c) to avoid them.alternatively  the ability to detect  phishing  emails  may  not  translate  to  users’  ability  to  avoid  attack  vectors  in  general.thus   the  effect  of  avoiding  threats  from phishing is washed out by all the other attacks that lead to  malware and  malicious files on users’ computers.participants’  rate  of  negative  experiences  may  also  be  related  to  their  systems’  protections  and  their  attractiveness  as  targets  for  attackers.systems’ vulnerability is partially determined by users  (e.g.abilities   knowledge)  and  partly  by  others  (e.g.browser  blacklists  security software).unfortunately  even with the rich  sbo data set  we lacked the complete picture needed to sort out  these relationships.the sbo collects data on browser warnings   but  there  were  very  few  observations.examining  browser  warnings  would  allow  observation  of  the  urls  that  users  attempted  to  visit   rather  than  being  limited  to  the  successful  ones that were not blocked by browser blacklists.in addition  as  described  in  the  methods  section   we  were  unable  to  measure   to  measure)  and   usenix association  thirteenth symposium on usable privacy and security    281  detections  for  all  security  software.some  of  those  programs   particularly free versions  do not record logs.others have poor  documentation.on those that do provide logs  we observed few  detections.given that security software use did not predict the  presence of  malware or malicious files and that more  malware  and  malicious  files  were  observed  than  were  detected  by  security  software   one  possible  explanation  is  that  many  sbo  users were unable to configure and utilize their security software  effectively.finally  explanation (4)  that the sbo measures are noisy  is to  be expected for real-world observations.there were cases where  data  were  missing  (e.g.a  sensor  malfunctioned  or  was  turned  off)  or  ambiguous  (e.g.multiple  people  using  the  same  computer).as a partial check on one potential source of noise   we  repeated  the  analysis  after  excluding  computers  with  multiple  users   but  found  similar  results.if  data  problems  are  randomly  distributed   then  a  larger  sample  might  reveal  underlying relationships.if they are correlated with individual or  system performance  then those interdependencies will need to  be understood and unraveled.thus   validating  predictive  measures  of  phishing  vulnerability  (including  sdt  and  sebis)  requires  a  much  more  nuanced  picture  than  we  currently  have  of  the  relationship  between  individuals’  ability   propensities   and  experienced  outcomes.the predictive validity of any measure could be undermined by  proper  environmental  safeguards  or  if  people  realize  their  vulnerability  and  restrict  their  behavior.once  available   a  full  picture of the sbo data may provide valuable guidance on these  possibilities.5.1  limitations  this study had several notable limitations.first  it was limited to  windows users.the depth and breadth of sbo data collection  requires custom  software tailored  to  each  os.due to  resource  constraints  the sbo is limited to windows  the most common  os [9].in  the  original  mturk sample  [4]   84% of  participants  used windows and performed similarly to other os users.second  although this study evaluates the generalizability of an  existing method  it leaves some aspects of generalizability open  to further study.although the mturk and sbo samples differed  in some ways (table 2)  extension to other populations would be  valuable.one  within-sample  result  bearing  further  attention  is  the observation that men were more susceptible to phishing.third   both  the  experimental  task  and  the  sbo  study  whether  individuals  visit  a  phishing  website.that  leaves  open  the  question of when they share personal information once there.as  noted  even the simpler outcome of such visits was difficult to  measure in the sbo.we were limited by the data available in  the  google  safe  browsing   shouldiremoveit.com   and  virustotal  datasets.thus   we  missed  attacks  absent  in  these  databases.in  addition   we  observed  more  negative  computer  security  outcomes  related  to  software  (47%  had  malware  and  90% had malicious files) than to browsing (10% in browser data  and  33%  in  network  packet  data).this  lower  rate  may  reflect  limits to the lists of malicious urls  which change over time.for  example   a  legitimate  site  may  be  compromised  and  only  briefly  appear on  the  google safe  browsing blacklist.finally   some  sbo  data  were  missing  for  technical  reasons   which  reduced our ability to observe negative outcomes and correlate  them with other measures. ability   5.2  recommendations   given the novelty of using data logs like those collected by the  sbo  to  validate  performance  tests  like  those  collected  in  canfield et al.we provide recommendations for future work:   1.to the extent possible  use behavioral outcomes that are (a)  as directly related to the outcome of interest as possible and  (b)  rely  on  human  ability  without  intervening technology.for  example   measure  attempts  to  visit  malicious  urls  (via  browser  warning  data)   rather  than  actual  visits   to  distinguish  human  from  browser  blacklist  effectiveness.when  possible   use  security  software  detections of malware and malicious files to assess attempts  to download malicious files.technical constraints and lack  of observations limited our ability to use these outcomes.2.triangulate  between  multiple  data  sources  (e.g.assessing  both  browser  and  network  packet  data)   with  an  understanding of their respective strengths and weaknesses.for  example   there  are  more  network  packet  data   but  browser  data  better  reflect  the  urls  that  users  choose  to  visit.beyond  the  analysis  presented  here   it  may  be  possible  to  crosscheck  events  such  as  security  software  scanning with observed active processes on the machine.3.consider  the  temporal  sequence  of  events   such  as  how  periods without security software protection affect the risk  of acquiring malicious files.the  primary  challenges   6.conclusion  we  assessed  the  validity  of  the  sdt  measures  proposed  by  canfield et al.[4] in three ways: (a) replicating their mturk sdt  experiment  with  sbo  participants   (b)  assessing  construct  validity  via  correlation  with  the  sebis  proactive  awareness  subscale   and  (c)  evaluating  predictive  validity  using  negative  outcomes observed in the sbo data.our results suggest (a) that  the  findings  from  canfield  et  al.[4]  generalize  to  the  sbo  population  and  (b)  the  sdt  measures  have  construct  validity   given  the  correlation  between  participants’  self-reported  tendency to look at urls before clicking links (in the sebis)  and their caution in clicking links in the sdt study (behavior c).however  we found (c) no evidence of predictive validity  as the  sdt  measures  did  not  predict  negative  computer  security  outcomes observed in the sbo.one  of  this  analysis  was  differentiating  between  people’s  ability  to  protect  themselves  (by  knowing  which  urls  to  avoid)  and  technical  safeguards  (such  as  browser  blacklists).future  research   addressing  this  laboratory  and  complication   will  offer  opportunities  for  observational  measures  in  understanding the security ecosystem.7.acknowledgements  we thank the sbo pis: alessandro acquisti  nicolas christin   lorrie cranor  serge egelman  and rahul telang  for providing  access to the sbo data.in addition  we thank lorrie cranor and  serge egelman for comments on an earlier draft.we also thank  rick  wash  for  helpful  comments.the  security  behavior  observatory  was  partially  funded  by  the  nsa  science  of  security  lablet  at  carnegie  mellon  university  (contract  #h9823014c0140);  the  national  science  foundation   grant  cns-1012763  (nudging  users  towards  privacy);  and  the  hewlett  foundation   the  center  for  long-term  cybersecurity (cltc) at the university of california  berkeley.to  complement  one  another   through   for   282    thirteenth symposium on usable privacy and security  usenix association  8
20,2017-Tracking Phishing Attacks Over Time.pdf, the so-called “phishing” attacks are one of the important threats to individuals and corporations in today’s internet.combatting phishing is thus a top-priority  and has been the focus of much work  both on the academic and on the indus- try sides.in this paper  we look at this problem from a new angle.we have monitored a total of 19 066 phishing attacks over a period of ten months and found that over 90% of these attacks were actually replicas or variations of other attacks in the database.this provides several opportunities and insights for the ﬁght against phishing: ﬁrst  quickly and eﬃ- ciently detecting replicas is a very eﬀective prevention tool.we detail one such tool in this paper.second  the widely held belief that phishing attacks are dealt with promptly is but an illusion.we have recorded numerous attacks that stay active throughout our observation period.this shows that the current prevention techniques are ineﬀective and need to be overhauled.we provide some suggestions in this direction.third  our observation give a new perspective into the modus operandi of attackers.in particular  some of our observations suggest that a small group of attackers could be behind a large part of the current attacks.taking down that group could potentially have a large impact on the phishing attacks observed today.keywords phishing detection; clustering  1,  so-called “phishing” has been deﬁned as “a type of com- puter attack that communicates socially engineered mes- sages to humans via electronic communication channels in order to persuade them to perform certain actions for the attacker’s beneﬁt” [18].although this deﬁnition is quite  c(cid:13),
21,2018-A Measurement Study on Linux Container Security Attacks and Countermeasures.pdf, linux container mechanism has attracted a lot of attention and is increasingly utilized to deploy industry applications.though it is a consensus that the container mechanism is not secure due to the kernel-sharing property  it lacks a concrete and systematical evaluation on its security using real world exploits.in this paper  we collect an attack dataset including 223 exploits that are e￿ective on the container platform  and classify them into di￿erent categories using a two-dimensional attack taxonomy.then we evaluate the security of existing linux container mechanism using 88 typical exploits ￿ltered out from the dataset.we ￿nd 50 (56.82%) exploits can successfully launch attacks from inside the container with the default con￿guration.since the privilege escalation exploits can completely disable the container protection mechanism  we conduct an in-depth analysis on these exploits.we ￿nd the kernel security mechanisms such as capability  seccomp  and mac play a more important role in preventing privilege escalation than the container isolation mechanisms (i.e.namespace and cgroup).however  the interdependence and mutual-in￿uence relationship among these kernel security mechanisms may make them fall into the  short board e￿ect  and impair their protection capability.by studying the 11 exploits that still can successfully break the isolation provided by container and achieve privilege escalation  we identify a common 4-step attack model followed by all 11 exploits.finally  we propose a defense mechanism to e￿ectively defeat those identi￿ed privilege escalation attacks.∗also with institute of information engineering  cas  beijing  china.†also with data assurance and communication security research center  cas  bei- jing  china.‡corresponding author.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro￿t or commercial advantage and that copies bear this notice and the full citation on the ￿rst page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior speci￿c permission and/or a fee.request permissions from permissions@acm.org.acsac ’18  december 3–7  2018  san juan  pr  usa © 2018 association for computing machinery.acm isbn 978-1-4503-6569-7/18/12...$15.00 https://doi.org/10.1145/3274694.3274720  ccs concepts • security and privacy → operating systems security; virtu- alization and security; keywords container  privilege escalation  kernel security mechanisms acm reference format: xin lin  lingguang lei  yuewu wang  jiwu jing  kun sun  and quan zhou.2018.a measurement study on linux container security: attacks and countermeasures.in 2018 annual computer security applications conference (acsac ’18)  december 3–7  2018  san juan  pr  usa.acm  new york  ny  usa  12 pages.https://doi.org/10.1145/3274694.3274720  1, container technology is increasingly adopted by the industrial community due to two major advantages.first  the container or- chestration tools such as docker [31] and kubernetes [, based on the above analysis.first  container does not provide much security enhancement for the programs inside it  except the exploits aiming to achieve privilege escalation.second  cgroup mechanism is not e￿ectively utilized by default in defending against the dos attacks.third  the kernel security mechanisms such as capability  seccomp and mac play a more important role in preventing privilege escalation attacks than the container isolating mechanisms (i.e.namespace and cgroup).the former blocks 67.57% privilege escalation attacks while the later blocks only 21.62%.fourth  each kernel security mechanism (including namespace  cgroup  seccomp  capabilities and mac) restricts kernel permissions from di￿erent angles in a fragmented way  while the the relationships among them are intricate and complicate.improper con￿guration of these security mechanisms might lower their protective capability.5 defeating kernel privilege  escalation attacks  we ￿rst derive a general attack model by analyzing the privilege es- calation exploits which are still e￿ective on the container platform  and then propose a defense system.5.1 kernel privilege escalation attack model as illustrated in table 3  with the default security mechanisms enforced by docker  only 4  privilege escalation  exploits work well inside the container.another 7 exploits are blocked as the  cap_net_admin  capability is by default not available inside the container.however  the  cap_net_admin  capability controls the operations such as con￿guring the ip address  routing table  ￿rewall etc.thus is likely to be allowed in practice.therefore  we consider all the 11 exploits as the successful ones on the container platform.by analyzing the 11 exploits  we ￿nd they follow a common 4-step attack model as depicted in figure 2.first  they bypass (or manually disable) the kaslr mechanism to obtain the address of critical kernel static functions whose o￿set to the kernel base address is constant such as native_write_cr4()  commit_creds() and prepare_kernel_cred() etc.second  they exploit the kernel vulnera- bilities such as uaf  race condition  improper veri￿cation  bu￿er over￿ow etc.to enable the overwriting of the pointers of some kernel functions which could be easily triggered to execute.for example  the exploit with edb-id 41994 utilizes the  improper veri- ￿cation  vulnerability of the setsockopt() system call (i.e.cve-2017- 7308) to substitute the content of  retir_blk_timer->func  pointer with the address of native_write_cr4().and the function addressed by  retir_blk_timer->func  will be triggered when the package is received too slowly.third  they overwrite these kernel functions’ pointers so that they point to the addresses of the critical kernel static functions (e.g.native_write_cr4()) or the kernel address of the ￿rst gadget of the crafted rop chain.as such  the smep&smap mechanisms will be disabled when these kernel functions are in- voked.this step could be omitted if the smep&smap mechanisms are manually disabled.finally  they repeat the second step and overwrite the pointer of the kernel function to point to a malicious user space function or shellcode  which invokes the kernel function commit_creds() to apply for root credential (i.e.root privilege with 38 capabilities).as the smep&smap is disabled  the user space function or shellcode could be executed in supervisor mode.in general  all  privilege escalation  exploits share the same at- tack goal  i.e.obtaining the full 38 capabilities.theoretically  the attackers could achieve this by directly overwriting the kernel data associated with a process’s capabilities.however  it is nearly im- possible.first  it is di￿cult for the exploit codes in the user space to locate speci￿c kernel data structures  as they are in di￿erent address space.second  the user programs can only access kernel data through speci￿ed system calls  rather than write any kernel data directly.therefore  almost all  privilege escalation  exploits need to ￿nd a vulnerable system call to enable the overwriting of some speci￿c kernel data (i.e.step 2 in figure 2)  and rely on the supervisor-mode executed user space codes (i.e.step 3) to invoke the privilege-elevating kernel codes (i.e.step 4).5.2 countermeasures according to the attack model  four actions are involved to elevate the privilege  which are bypassing the kaslr mechanism  bypass- ing the smep&smap mechanisms  overwriting pointers of some kernel functions which are easy to be invoked  and invoking the  425  a measurement study on linux container security: a￿acks and countermeasures acsac ’18  december 3–7  2018  san juan  pr  usa  figure 2: kernel privilege escalation attack model  commit_creds() kernel function.therefore  the  privilege escala- tion  attacks could be blocked by disabling any one of the four actions.however  it is di￿cult to ￿nd a general and comprehensive approach to prevent the attackers from bypassing the kaslr mech- anism and overwriting kernel data.the kaslr mechanism has been proclaimed dead by many researchers  as current implemen- tations of kaslr have fatal ￿aws [19].overwriting of the speci￿c kernel functions’ pointers is achieved by exploiting the vulnera- bilities in the linux kernel  such as uaf  race condition  improper verify  bu￿er over￿ow etc.and it is pretty unlikely to patch all vulnerabilities considering the large code size of linux kernel.the cpu mechanisms smap&smep are easy to be disabled if the at- tackers compromise the kaslr mechanism and gain the ability to overwrite the pointers of some kernel functions.therefore  we propose a defense system by forbidding the commit_creds() to be utilized to elevate the privilege inside the container.figure 3: privilege escalation procedure through com- mit_creds()  426  privilege escalation procedure through commit_creds().the procedure to elevate privilege through the commit_creds() is illustrated in figure 3  which is also the detail of the fourth step in figure 2.in linux kernel  the credential associated with a process is stored as two ￿elds inside the task_struct structure  i.e.cred and real_cred.the cred ￿eld represents the current privileges (including capabilities  gid  uid etc.) of the process  and could be temporarily modi￿ed during execution of the process.the real_cred ￿eld rep- resents the highest privileges a process could reach  and normally could not be changed.generally  two static kernel functions whose o￿set to the kernel base address is constant  are used to achieve the privilege escalation.first  the prepare_kernel_cred(0) function is invoked to construct a credential with root privilege (i.e.global_root_uid  global_root_gid  and cap_full_set).then  the commit_creds() function takes the return of the pre- pare_kernel_cred(0) as the parameter  and is invoked to update the real_cred and cred of current process with the parameter (i.e.root privilege).normally  invocation of commit_creds() by a non- privileged process will fail.however  in the attack model illustrated in figure 2  commit_creds() can be executed successfully as the smep&smap mechanisms are disabled (step 3) and the user space program invoking the commit_creds() is triggered in the kernel mode (step 2 & step 4).defense system.we observe that the processes inside the con- tainer usually do not need root privilege (by default  only portions of capabilities are assigned to the container tenants.for example  14 capabilities are assigned to the amazon container tenants [11]  and 9 capabilities are assigned by the openshift container service [32].and a normal container tenant is not allowed to apply for a con- tainer with more capabilities.).however  the privilege escalation attacks inside a container will misuse the commit_creds() to apply for the root privilege (i.e.38 capabilities).therefore  we modify the implementation of the commit_creds() function  and enforce a check before updating the real_cred and cred of the caller pro- cess.speci￿cally  we ￿rst check whether the caller process is inside a container or on the control host.the process is described as a  acsac ’18  december 3–7  2018  san juan  pr  usa  xin lin  lingguang lei  yuewu wang et al.table 4: performance of the defense system  benchmarks dhrystone 2 using register variables double-precision whetstone execl throughput file copy 1024 bufsize 2000 maxblocks file copy 256 bufsize 500 maxblocks file copy 4096 bufsize 8000 maxblocks pipe throughput pipe-based context switching process creation shell scripts (1 concurrent) shell scripts (8 concurrent) system call overhead system benchmarks index score  1 parallel copy  original modi￿ed overhead 3586.3 800.9 726.7 3379.1 2341.8 4930.0 2261.3 77.7 686.1 2498.7 6570.0 3161.3 1681.1  0.77% -3.17% -1.55% -0.39% 0.54% 0.74% -0.14% -0.26% 0.71% -1.67% -0.87% -0.74% -0.49%  3558.6 826.3 738.0 3392.2 2329.2 4893.3 2264.5 77.9 681.2 2540.5 6627.4 3184.8 1689.3  4 parallel copies  original modi￿ed overhead 13308.0 3369.8 4346.2 3857.3 2390.8 6279.1 8339.2 4143.1 2211.3 8947.2 7683.3 6345.5 5183.4  13399.1 3371.7 4450.2 3729.8 2416.7 6204.6 8489.1 4163.9 2312.9 9197.0 8232.6 6517.9 5240.1  -0.68% -0.06% -2.39% 3.31% -1.08% 1.19% -1.80% -0.50% -4.59% -2.79% -7.15% -2.72% -1.09%  task_struct structure in linux kernel which contains a nsproxy ￿eld storing the process’s namespaces information (e.g.namespace id).the processes on the linux control host (e.g.the init process) are also associated with a nsproxy ￿eld whose value is however con- stant  i.e.init_nsproxy.therefore  we can judge whether a process is inside a container or not  by comparing whether its nsproxy is equal to init_nsproxy.if the commit_creds() is invoked from inside a container  we will further check whether it is a privilege escalation operation  and stop modifying the real_cred and cred if it is.we determine whether it is a privilege escalation operation by compar- ing the input parameter (i.e.new credential) of the commit_creds() method and the real_cred of current process.if the uid/gid in the new credential is smaller or the cap_bset in the new credential is larger  it may be a privilege escalation operation.in total  our defense solution needs to add less than 10 lines of codes.5.3 e￿ectiveness and performance we enable the  cap_net_admin  capability for the processes inside the container  besides the default 14 capabilities assigned by docker.we ￿nd all 11 exploits could obtain the root privilege from inside the container before deploying the defense system  and fail to achieve privilege escalation after the deployment.the results prove that our defense system can e￿ectively block the privilege escalation attacks on container platform.we also evaluate the overhead of the defense system on a vmware virtual machine with ubuntu 16.04 lts amd 64 os installed (linux kernel 4.8.0#1)  and con￿gured with quad-core 2.80ghz cpu and 2gb memory.the host machine is dell precision 5520 with 8gb memory and quad-core 2.80ghz cpu.the results are illustrated in table 4 and obtained through the unixbench tool [66]  which show that the defense system introduces negligible overhead.6 discussion on limitation we propose a simple but e￿ective defense mechanism by hardening the commit_creds method  which is misused by existing privilege escalation exploits to modify the credentials bound to a process.however  it is not a complete solution against all potential priv- ilege escalation exploits.first  after completing the third step in figure 2 (i.e.bypassing the smep&smap mechanisms)  the attack- ers may directly modify the kernel data related to real_cred.since real_cred is stored at a ￿xed o￿set to the task_struct data struc- ture  it could be modi￿ed once the task_struct is located.second   427  armored attackers might launch rop attacks that utilize the gad- gets of kernel codes to modify the real_cred without ￿rst bypassing the smep&smap mechanisms.furthermore  attackers may misuse other kernel functions besides commit_creds to launch other types of attacks such as container escape.for instance  the attackers might invoke the switch_task_namespaces() method to achieve container escape.though our defense system can be extended to avoid the misuse of these kernel functions (e.g.forbidding the processes in- side the containers to invoke the switch_task_namespaces() method for container switch)  it is challenging to identify a complete list of all potential kernel functions that might be misused.to defeat those potential attacks  a defense solution is better to be implemented outside linux kernel.for instance  as a hypervisor- based kernel data protection mechanisms  sentry [67] partitions the kernel memory into regions with di￿erent access control poli- cies  which detail when the sensitive data in each region could be accessed by the kernel.allmempro [40] con￿gures the read  write  and execution permissions on a memory page through the extended page tables (ept)  and controls the ept in the hypervisor.alterna- tively  hardware-based kernel protection solutions are promising to prevent those attacks too.for example  privwatcher [9] uti- lizes arm trustzone mechanism to protect the process credentials against memory corruption attacks.we leave it as one future work to develop a more comprehensive defense solution.7 related work 7.1 container security existing research work on container security mainly focuses on three aspects  i.e.security of the container images [20  60  61  65]  security of the container orchestration tools like docker [10  28]  and security of linux container mechanism [7  72].some researches also work on evaluating the security of container mechanism.for example  m.ali babar et al.[3] give a comparative analysis on the isolation mechanisms provided by three container engines  i.e.docker  lxd  and rkt.thanh bui et al.[6] brie￿y compare the security of hardware-based virtualization technology (e.g.xen) and os-level virtualization technology (i.e.container mechanism) from system architecture level.reshetova et al.[62] theoretically analyze the security of several os-level virtualization solutions including freebsd jails  linux-vserver  solaris zones  openvz  lxc and cells etc.however  these works mostly evaluate container security from the system architecture or design principle level  while we evaluate with a measurement approach.some researchers [46  56] also evaluate the container security us- ing potential vulnerabilities against speci￿c container mechanisms such as docker.for example  a.martin et al.[50] do a vulnerability- oriented risk analysis of the container  classify the vulnerabilities into ￿ve categories and perform a vulnerability assessment accord- ing to the security architecture and use cases of docker.a mouat et al.[56] sort the vulnerabilities of container platform into kernel exploits  dos  container breakouts  poisoned images  compromising secrets.z.jian et al.[38] summarize two models to achieve docker container escape  propose a defense tool by inspecting the status of namespaces  and evaluate the tool with 11 cve vulnerabilities.due to the small number of exploits reported in the vulnerability  a measurement study on linux container security: a￿acks and countermeasures acsac ’18  december 3–7  2018  san juan  pr  usa  databases such as cve [12] and nvd [58]  it is challenging to pro- vide a persuasive security evaluation on container mechanism.in this paper  we evaluate security of container in real world using an attack dataset containing 223 exploits.there are also some studies on enhancing the security of con- tainer mechanism through the linux kernel security mechanisms (e.g.capability  seccomp  mac etc.) [41  51] dockerpolicymod- ules [4  8].our defense system is proposed based on analysis of the privilege escalation exploits.7.2 attack taxonomy some researchers work on taxonomy of certain types of linux attacks or vulnerabilities.for example  l.yi et al.[42  70] pro- pose a two-dimensional taxonomy of network vulnerabilities in unix/linux systems.s.hansman et al.[22] propose a taxonomy that consists of four dimensions in the computer and network attack ￿eld.r.sánchez-fraga et al.[17] and j.mirkovic et al.[55] make taxonomy for ddos attacks based on highlight commonalities and important characteristics.n.gruschka et al.[18] focus on taxon- omy on cloud services attacks.d.papp et al.[59] derive a taxonomy for attacks on embedded systems.some researchers just analyze or classify the attacks without making a clear taxonomy.for example  k.ko et al.[39] present the coarse characteristic classi￿cation and correlation analysis of source-level vulnerabilities in linux kernel.in this paper  we propose a two-dimensional taxonomy for the attacks speci￿cally for the container platform.8 conclusion linux container is increasingly utilized by the industrial commu- nity.although it is a consensus that container mechanism is not secure  a concrete and systematical evaluation is absent.in this pa- per  we perform a measurement study on container security using an attack dataset containing 223 exploits.we ￿rst make a taxon- omy from two dimensions.then we evaluate the security of the container with the experiment results of 88 typical exploits ￿ltered out from the dataset.since we ￿nd the privilege escalation exploits can successfully escape outside the container and compromise the host  we give an in-depth analysis on them.under the protection of security mechanisms  there are still 11 exploits can break the container isolation.fortunately  we ￿nd a common attack model that all 11 exploits follows.further more  we propose a defense mechanism to defeat all 11 exploits.in general  we prove that the security of container mechanism depends on the security of the kernel  while the interdependence and mutual-in￿uence relation- ship requires careful con￿gurations to e￿ectively defeat privilege escalation attacks.acknowledgments we would like to thank our shepherd zhiqiang lin and our anony- mous reviewers for their valuable comments and suggestions.this work is supported by the national key research and de- velopment program of china under grant no.2016yfb0800102  the national natural science foundation of china under grant no.61802398  the national cryptography development fund under award no.mmjj20180222  the u.s.onr grants n00014-16-1-3214 and n00014-16-1-3216  and the nsf grants cns-1815650.428 
22,2018-Augmenting Telephone Spam Blacklists by Mining Large CDR Datasets.pdf, telephone spam has become an increasingly prevalent problem in many countries all over the world.for example  the us fed- eral trade commission’s (ftc) national do not call registry’s number of cumulative complaints of spam/scam calls reached 30.9 million submissions in 2016.naturally  telephone carriers can play an important role in the fight against spam.however  due to the extremely large volume of calls that transit across large carrier networks  it is challenging to mine their vast amounts of call detail records (cdrs) to accurately detect and block spam phone calls.this is because cdrs only contain high-level metadata (e.g.source and destination numbers  call start time  call duration  etc.) related to each phone call.in addition  ground truth about both benign and spam-related phone numbers is often very scarce (only a tiny fraction of all phone numbers can be labeled).more importantly  telephone carriers are extremely sensitive to false positives  as they need to avoid blocking any non-spam calls  making the detection of spam-related numbers even more challenging.in this paper  we present a novel detection system that aims to discover telephone numbers involved in spam campaigns.given a small seed of known spam phone numbers  our system uses a combination of unsupervised and supervised machine learning methods to mine new  previously unknown spam numbers from large datasets of call detail records (cdrs).our objective is not to detect all possible spam phone calls crossing a carrier’s network  but rather to expand the list of known spam numbers while aiming for zero false positives  so that the newly discovered numbers may be added to a phone blacklist  for example.to evaluate our system  we have conducted experiments over a large dataset of real-world cdrs provided by a leading telephony provider in china  while tuning the system to produce no false positives.the experimental  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.asia ccs’18  june 4–8 2018  incheon  republic of korea © 2018 association for computing machinery.acm isbn 978-1-4503-5576-6/18/06...$15.00 https://doi.org/10.1145/3196494.3196553  results show that our system is able to greatly expand on the initial seed of known spam numbers by up to about 250%.keywords telephone spam; blacklisting; machine learning; voip acm reference format: jienan liu  babak rahbarinia  roberto perdisci  haitao du  and li su.2018.augmenting telephone spam blacklists by mining large cdr datasets.in asia ccs ’18: 2018 acm asia conference on computer and communications security  june 4–8  2018  incheon  republic of korea.acm  new york  ny  usa  12 pages.https://doi.org/10.1145/3196494.3196553  1, telephone spam has become an increasingly prevalent problem in many countries all over the world [19].for example  the federal trade commission’s (ftc) national do not call registry’s number of cumulative complaints of spam/scam calls in the us reached 30.9 million in , or recommendations expressed in this  material are those of the authors and do not necessarily reflect the views of the national science foundation
23,2018-Do Social Disorders Facilitate Social Engineering_ A Case Study of Autism and Phishing Attacks.pdf, social engineering is a well-established and well-studied threat especially against healthy computer users.little studied  however  is the level of vulnerability to social engineering attacks against people with medical conditions.social disorders in particular may make people more susceptible to such attacks.in this paper  as an initial line of investigation into this understudied research line  we launch a study of phishing  a prominent social engineering attack  against people suffering from autism spectrum disorder  a unique developmental disorder characterized by hampered social skills and communication.we present a study of phishing detection with two groups of participants each with 15 participants  one diagnosed with autism and other without autism  in which they were asked to distinguish real versions of certain websites from their fake counterparts.given the known gullibility and social vulnerability of users with autism  our study is designed to test the hypothesis that individuals with autism will be more prone to such attacks in contrast to healthy participants of prior studies.our results  however  do not support this hypothesis demonstrating that participants with autism are not more vulnerable to phishing attempts.we attribute this result to the unique characteristics of users with autism including attention to detail  strong memory of factual information and diverse way of thinking  which are skills that folklore assumes may actually make users with autism highly qualified for cybersecurity careers.over- all  our work serves to demonstrate that targeted (spear) phishing attacks against internet users suffering from autism may not be more successful compared to untargeted attacks against the user population without autism.it also highlights that social disorders may not necessarily facilitate social engineering attacks.acm reference format: ajaya neupane  kiavash satvat  nitesh saxena  despina stavrinos  and ha- ley johnson bishop.2018.do social disorders facilitate social engineering?  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.acsac ’18  december 3–7  2018  san juan  pr  usa © 2018 association for computing machinery.acm isbn 978-1-4503-6569-7/18/12...$15.00 https://doi.org/10.1145/3274694.3274730  a case study of autism and phishing attacks.in 2018 annual computer se- curity applications conference (acsac ’18)  december 3–7  2018  san juan  pr  usa.acm  new york  ny  usa  11 pages.https://doi.org/10.1145/3274694.3274730  1, phishing is a type of social engineering attack where the attacker directs users to fake websites having the look and feel similar to that of a real website  with the intention of obtaining their sensitive and private credentials.the attackers then exploit the information gathered from such an attack to misuse the victim’s account.social engineering attacks are a well-established and well-studied threat in the literature especially against normal computer users.researchers have conducted a number of human-centered phishing detection studies with such users (e.g.[13–15  ,
24,2018-End-to-End Measurements of Email Spoofing Attacks  .pdf, spear phishing has been a persistent threat to users and organizations  and yet email providers still face key chal- lenges to authenticate incoming emails.as a result  at- tackers can apply spooﬁng techniques to impersonate a trusted entity to conduct highly deceptive phishing at- tacks.in this work  we study email spooﬁng to answer three key questions: (1) how do email providers detect and handle forged emails? (2) under what conditions can forged emails penetrate the defense to reach user inbox? (3) once the forged email gets in  how email providers warn users? is the warning truly effective?  we answer these questions by conducting an end-to- end measurement on 35 popular email providers and ex- amining user reactions to spooﬁng through a real-world spooﬁng/phishing test.our key ﬁndings are three folds.first  we observe that most email providers have the nec- essary protocols to detect spooﬁng  but still allow forged emails to reach the user inbox (e.g.yahoo mail  icloud  gmail).second  once a forged email gets in  most email providers have no warning for users  particularly for mo- bile email apps.some providers (e.g.gmail inbox) even have misleading uis that make the forged email look au- thentic.third  a few email providers (9/35) have imple- mented visual security indicators on unveriﬁed emails.our phishing experiment shows that security indicators have a positive impact on reducing risky user actions  but cannot eliminate the risk.our study reveals a ma- jor miscommunication between email providers and end- users.improvements at both ends (server-side protocols and uis) are needed to bridge the gap.1 ,  despite the recent development of the system and net- work security  human factors still remain a weak link.as a result  attackers increasingly rely on phishing tac- tics to breach various target networks [6, or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of any funding agencies.[2] phishing activity trends  2015.ters apwg trends report q1-q3 2015.pdf.report   3rd quar- http://docs.apwg.org/reports/  1st  [3] postﬁx.http://www.postfix.org.[4] data breach investigations report.verizon inc.2017.http://www.verizonenterprise.com/verizon- insights-lab/dbir/2017/.[5] email statistics report.the radicati group  2017.http://www.radicati.com/wp/wp-content/ uploads/2017/01/email-statistics-report- 2017-2021-executive-summary.pdf.[6] agten  p.joosen  w.piessens  f.and niki- forakis  n.seven months’ worth of mistakes: a lon- gitudinal study of typosquatting abuse.in proc.of ndss (2015).[7] akhawe  d.and felt  a.p.alice in warningland: a large-scale ﬁeld study of browser security warning effec- tiveness.in proc.of usenix security (2013).[8] anderson  b.b.vance  t.kirwan  c.b.ear- gle  d.and howard  s.users aren’t (necessarily) lazy: using neurois to explain habituation to security warnings.in proc.of icis (2014).[9] antin  j.and shaw  a.social desirability bias and self-reports of motivation: a study of amazon mechanical turk in the us and india.in proc.of chi (2012).[10] bilogrevic   i.huguenin  k.mihaila  s.shokri  r.and hubaux  j.-p.predicting users’ mo- tivations behind location check-ins and utility implica- tions of privacy protection mechanisms.in proc.of ndss (2015).[11] blanzieri  e.and bryl  a.a survey of learning- based techniques of email spam ﬁltering.artiﬁcial intel- ligence review 29  1 (2008)  63–92.[12] bravo-lillo  c.cranor  l.and komanduri  s.harder to ignore? revisiting pop-up fatigue and ap- proaches to prevent it.in proc.of soups (2014).[13] bravo-lillo  c.cranor  l.f.downs  j.and komanduri  s.bridging the gap in computer security warnings: a mental model approach.in proc.of ieee s&p (2011).[14] bravo-lillo  c.komanduri  s.cranor  l.f.reeder  r.w.sleeper  m.downs  j.and schechter  s.your attention please: designing security-decision uis to make genuine risks harder to ig- nore.in proc.of soups (2013).[15] constantin   l.yahoo email  anti-spooﬁng pc world  2014.policy breaks mailing lists.https://www.pcworld.com/article/2141120/ yahoo-email-antispoofing-policy-breaks- mailing-lists.html
25,2018-Lemna- Explaining deep learning based security applications.pdf, while deep learning has shown a great potential in various domains  the lack of transparency has limited its application in security or safety-critical areas.existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision.unfortunately  current methods are optimized for non-security tasks (e.g.image analysis).their key assumptions are often violated in security applications  leading to a poor explanation fidelity.in this paper  we propose lemna  a high-fidelity explanation method dedicated for security applications.given an input data sample  lemna generates a small set of interpretable features to ex- plain how the input sample is classified.the core idea is to approx- imate a local area of the complex deep learning decision boundary using a simple interpretable model.the local interpretable model is specially designed to (1) handle feature dependency to better work with security applications (e.g.binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity.we evaluate our system using two popular deep learning applica- tions in security (a malware classifier  and a function start detector for binary reverse-engineering).extensive evaluations show that lemna’s explanation has a much higher fidelity level compared to existing methods.in addition  we demonstrate practical use cases of lemna to help machine learning developers to validate model be- havior  troubleshoot classification errors  and automatically patch the errors of the target models.ccs concepts • security and privacy → software reverse engineering;  keywords explainable ai  binary analysis  deep recurrent neural networks  acm reference format: wenbo guo  dongliang mu  jun xu  purui su  gang wang  xinyu xing.2018.lemna: explaining deep learning based security applications.in ccs âăź18: 2018 acm sigsac conference on computer & communications security  oct.15–19  2018  toronto  on  canada.acm  new york  ny  usa  16 pages.https://doi.org/10.1145/3243734.3243792  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.ccs ’18  october 15–19  2018  toronto  on  canada © 2018 association for computing machinery.acm isbn 978-1-4503-5693-0/18/10...$15.00 https://doi.org/10.1145/3243734.3243792  1, in recent years  deep neural networks have shown a great potential to build security applications.so far  researchers have successfully applied deep neural networks to train classifiers for malware classi- fication [, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies
26,2019-A Decade of Mal-Activity Reporting- A Retrospective Analysis of Internet Malicious Activity Blacklists.pdf, this paper focuses on reporting of internet malicious activity (or mal-activity in short) by public blacklists with the objective of pro- viding a systematic characterization of what has been reported over the years  and more importantly  the evolution of reported activities.using an initial seed of 22 blacklists  covering the period from january 2007 to june 2017  we collect more than 51 million mal-activity reports involving 662k unique ip addresses worldwide.leveraging the wayback machine  antivirus (av) tool reports and several additional public datasets (e.g.bgp route views and inter- net registries) we enrich the data with historical meta-information including geo-locations (countries)  autonomous system (as) num- bers and types of mal-activity.furthermore  we use the initially labelled dataset of ≈ 1.57 million mal-activities (obtained from pub- lic blacklists) to train a machine learning classifier to classify the remaining unlabeled dataset of ≈ 44 million mal-activities obtained through additional sources.we make our unique collected dataset (and scripts used) publicly available for further research.the main contributions of the paper are a novel means of report collection  with a machine learning approach to classify reported activities  characterization of the dataset and  most importantly  temporal analysis of mal-activity reporting behavior.inspired by p2p behavior modeling  our analysis shows that some classes of mal- activities (e.g.phishing) and a small number of mal-activity sources are persistent  suggesting that either blacklist-based prevention systems are ineffective or have unreasonably long update periods.our analysis also indicates that resources can be better utilized by focusing on heavy mal-activity contributors  which constitute the bulk of mal-activities.acm reference format: benjamin zi hao zhao  muhammad ikram  hassan jameel asghar  mo- hamed ali kaafar  abdelberi chaabane  and kanchana thilakarathna.2019.a decade of mal-activity reporting: a retrospective analysis of internet  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.asiaccs ’19  july 9–12  2019  auckland  new zealand © 2019 association for computing machinery.acm isbn 978-1-4503-6752-3/19/07...$15.00 https://doi.org/10.1145/3321705.3329834  malicious activity blacklists.in acm asia conference on computer and com- munications security (asiaccs ’19)  july 9–12  2019  auckland  new zealand.acm  new york  ny  usa  13 pages.https://doi.org/10.1145/3321705.3329834  1, public reports of malicious online activity are commonly used in the form of blacklists by intrusion detection systems  spam filters and alike to determine if a host is known for suspicious activity.however very little is known about the dynamics of the reporting of malicious activities.understanding what has been reported and how the reported activity evolves over time can be of paramount importance to help assess the efficacy of blacklist-based threat pre- vention systems.we conduct a longitudinal measurement study of reporting of malicious online activities (abridged to mal-activities)  over a ten-year period (from january , out of ips reported globally as these are subject to dynamic ip allocation issues (e.g.via dhcp).4.1 evolution of reporting of mal-activities we analyze the daily volume of different classes of reported mal- activities in our dataset over time in figure 5a (note log scale of y-axis).perhaps not surprisingly  we observe that reported mal- activities have been steadily increasing in volume over the last decade  with an interesting spike around 2008-2009 driven by the inception of high-profile fs and exploit kits.one of the earliest kits was mpack [52]  a very popular “user-friendly” exploit kit  3i.e.hosts that guarantee service even after being detected malicious.4ppi services are also used for benign software.(a) evolution  figure 5: evolution and proportion of mal-activities in the dataset.(b) proportion  introduced in 2006.typically  mpack included a collection of php scripts aiming at exploiting browsers’ security holes and commonly used programs (e.g.quicktime).phishing has seen two distinct periods of reporting.first  during 2009 and then in 2013 with an increase in the total volume of reports by two orders of magnitude.this is consistent with a report from kaspersky lab in 2013 [9] which points to the growing popularity of digital payment systems attracting unwanted attention from cybercriminals translated into a dramatic increase in the number of finance-related attacks.we present the relative volume of the reported mal-activity classes over time in figure 5b.we see that malware continues to dominate the proportion of mal-activities.however  phishing has recently undergone an increase in volume: 29% of all mal-activities in the year 2017.in comparison  malware stands at 59% of all mal-activities for the same year.lessons learned.malware is consistently the dominant class over the years.however  interestingly  starting from around 2016-2017  phishing is emerging as one of the major classes of mal-activities  currently consisting of half the volume of malware.notwithstand- ing that data sources may not have immediately reported on novel classes of mal-activities (lower volume of mal-activities  other than malware  in earlier years)  the relative volume serves as a reasonable proxy of the evolution of mal-activities reporting behavior.20072008200920102011201220132014201520162017time0100101102103104105106107108# of mal-activity (log)spammerspupmalwarefraud.serv.phishingexploitstotaldaily20072008200920102011201220132014201520162017time020406080100% of mal-activitymalwarephishingexploitsfraud.serv.pupspammersfigure 6: churn model.ki c is the total number of reports in the c th period of activity of host i.4.2 the churn in mal-activity reporting previous research [57] showed that spammers often quarantine bots for a period of time  waiting for them to be “whitelisted” again.motivated by this  we study the periods of presence of ip addresses  ases and countries (all denoted as hosts for simplicity) in the public reports.the host churn model.consider a malicious ecosystem with n participating hosts  where each host h is either alive (i.e.present in the system) or dead (i.e.logged off/clean/not reported) at any given time t.an active host can be reported one or multiple times as being malicious (denoted m).this behavior can be modeled by an alternating renewal process zi(t) for each host h  similar to the peers churn model in peer-to-peer networks (e.g.yao et al.[62]): zi(t) = 1 if host i has received at least one report at time t  and zi(t) = 0 otherwise  where 1 ≤ i ≤ n  and t is in weeks.our traces are created by binning the reports into weeks per reported host (recall that host refers to an ip  as or cc).c =1) and off-time (i.e.{di c}∞  the model is illustrated in figure 6 where c stands for the cycle number  and durations of host i’s on (life) and off (death) periods are given by variables li c > 0 and di c > 0  respectively.unlike the model in [62]  we empirically evaluate (through our data) all lifetime (i.e.{li c}∞ c =1) durations by averaging over all cycles in our dataset.we denote the average lifetime as li and the average deathtime as di.a high average lifetime would reflect a report of persistent threats (or infection) generally referred to as bulletproof entities  since their involvement in mal-activities is not interrupted for extended durations (even after being reported).a low average deathtime indicates resiliency of the reported host as the mal-activity quickly recovers from a potential shutdown.the reciprocal of mean cycle duration is representative of the rate of arrival of a particular host.it indicates the frequency with which a host participates in  or leaves  a class of mal-activity and is defined as: λi = .consider a scenario where a malicious host is frequently joining and leaving a group of reported botnets (i.e.in bursts)  then both average lifetime and average deathtime would be small  and hence λi would be relatively large.figure 7 displays the cdfs of mean lifetime  mean deathtime and reciprocal of mean duration per ip address  asn and country in the blacklists.figure 7a shows that 86.4% of the ips are short-lived offenders with an average duration of just a week.as mentioned earlier  we refrain from drawing conclusions on the time-based behavior observed at an ip level due to the very likely dynamic ip allocation over time.at an as-level we found that 56.5% of the ases are short-lived with an average of one week duration of presence in the blacklists.this number is drastically reduced to 17.4% for countries  many of which are small african nations  or island states.li +di  1  the long tails observed in the cdf of mean lifetime in figure 7a indicate that there are only a few hosts with an extended lifetime.we report the ip addresses  ases  and countries with the highest lifetimes in table 4(a).we observe that us has the longest mean lifetime of 511 weeks by a large margin (china is ranked second at 55.8 weeks)  showing a much higher persistence of reported mal- activity in the us than any other country.brazil  canada and the uk are the next most persistent countries with the longest average lifetime of 54.8  37.8 and 37.7 weeks  respectively.at an as-level  the most persistent reported as is “china telecom backbone” with 147.0 weeks.figure 7b and table 4(b) suggest that while most ip addresses have a mean deathtime longer than 100 weeks indicating a low participation  the “long head” indicates that only a few ips are recurring participants.again with a focus on the as and country level  we observed that most ases and countries are repeat offenders from the perspective of blacklist reporting.at the country level  in terms of resiliency (low deathtime)  us is ranked first with no deathtime  followed by germany (1.50) and british virgin islands (1.60).for the rate of arrival  we calculate the reciprocal of mean du- ration and rank the countries accordingly.table 4(c) shows that the top 5 countries in terms of arrival rates are colombia  panama  bahamas  norway  and mexico  and constitute the most recurrent countries to be reported in mal-activity involvement.we also analyze the churn with respect to mal-activity classes.from figure 8a  we can observe that exploits tend to have reports with the lowest mean lifetime (one week)  while the rest of the mal-activity classes are similar to each other with a heavier concen- tration at longer weekly durations.in terms of resiliency  phishing has the lowest deathtime (highest resiliency) as shown in figure 8b.due to lower mean deathtime  phishing also has the highest mean rate of arrival indicated in figure 8c  implying highly frequent on-off reporting cycles  i.e.reported (in)active behavior.lessons learned.the analysis shows that a small number of hosts exhibit high renewal of mal-activities  indicating their presence on a blacklist has not deterred their activities.the most recurrent ip has an average report activity cycle of 5.5 weeks.had this host been blocked by blacklists  it would have been removed from said lists in less than 5.5 weeks from the first reports.thus blacklists can consider longer durations prior to delisting a malicious host.phishing has been observed with the highest resiliency to periods of no reporting (on average 54 weeks less than all mal-activities combined)  again suggesting delisting or their ability to circumvent blacklist-based blocking.a overwhelming majority (97.7%) of ip reports cease activities in 2 weeks  with average cycles of 185 weeks  the blacklist provider must tradeoff between potential false positives of hosts which had only been momentarily infected  or curbing the minority of recurrent hosts.4.3 magnitude of reported malicious activities we define a “severity” metric to quantify the magnitude of the reported activity during active periods of malicious hosts in the blacklists.formally  severity is defined as the average number of reports of mal-activities per active cycle as per figure 6.for host i  let ki c denote the total number of reports within the cth period of  𝐿𝐿𝑖𝑖 𝑐𝑐+1𝐿𝐿𝑖𝑖 𝑐𝑐𝐷𝐷𝑖𝑖 𝑐𝑐𝑍𝑍𝑖𝑖(𝑡𝑡)𝑡𝑡𝐾𝐾𝑖𝑖 𝑐𝑐table 4: churn analysis: top 5 ips  ases  and countries (cc) of lifetime  deathtime  and rate of arrival.(a) average lifetime - lt (most persistent)  ip lt asn organization  lt cc lt 4134 chinanet-backbone  cn 147 us 511 209.85.200.132 62 74.125.201.132 52 39 cn 56 4837 china169-backbone  cn 38 br 55 209.85.234.132 48 9800 unicom  cn 74.125.70.132 38 32613 iweb-as  ca 28 ca 38 74.125.202.132 37 28753 leaseweb-de-fra-10  de 26 gb 38  (b) average deathtime - dt (most resilient)  (c) rate of arrival - roa (most frequently active)  ip dt asn organization  103.224.212.222 3.0 36351 softlayer  us 69.172.201.153 3.1 26496 go-daddy  us  dt cc dt 1.5769 us 0 1.6087 de 1.5 204.11.56.48 3.7 40034 confluence-net.us 1.6122 vg 1.6 1.6780 fr 1.8 213.186.33.19 3.9 13335 cloudflare  inc.vg 208.73.211.70 4.2 14618 amazon-aes  us 1.8298 na 2.0  ip roa asn organization  roa cc roa 69.172.201.153 0.183 8001 net-access-corp  us 0.177 co 0.156 0.175 pa 0.148 103.224.212.222 0.176 9931 cat-ap  th 0.173 bs 0.142 0.173 no 0.138 0.169 mx 0.138  208.73.211.70 0.164 46636 natcoweb  us 213.186.33.19 0.150 13649 asn-vins  us 213.186.33.2 0.146 31103 kewweb ag  de  (a)  (b)  (c)  figure 7: churn analysis: cdfs of ips  ases  and countries (cc) of lifetime  deathtime  and rate of arrival.(a)  (b)  figure 8: churn analysis: cdfs of rate of arrival (reciprocal of mean duration)  mean lifetime  and mean deathtime for mal-activities.activity5 and as before let li c denote the active period (in weeks).equal to one indicating a unique malicious report per week.the then severity of host i  is defined as the average of ki c/li c over cdf in figure 10b indicates that only a few hosts are participating in a plethora of mal-activities with as little as 200 ip addresses all cycles of the host i in the dataset.a high severity value indi- reported to be involved in more than 10k malicious activities per cates that whenever a host is active (reported in the blacklists) it is week.figure 10c shows the cdf of the mean severity values for accompanied by a large volume of reported mal-activities.severity allows us to distinguish between long-living persistent reporting each of the mal-activity classes.we observe that fraudulent services are reported in the “low severity” range when compared to the rest of threats and short-living but denser reporting mal-activities.of the categories.we report the results of magnitude analysis in figure 9.observe that 27.4% of ases and 9.45% of countries have a severity value  (c)  5care has been taken to remove duplicate reports  i.e.same (time  ip  url) tuple  from blacklist-07-17.in any case  potential duplicates in the 2m reports from blacklist-07-17 dwarf in comparison to the 49m unique reports obtained from vt.table (a) of figure 9 lists ip addresses  ases  and countries with high values of severity.us has the highest severity of 82 558 re- ports per week.distant second are countries like china  germany  france  and ukraine with severity values of 377  212  149  and 80   100101life mean duration (weeks)0.00.20.40.60.81.0cdfipasncc101102death mean duration (weeks)0.00.20.40.60.81.0cdfipasncc102101mean rate of arrival0.00.20.40.60.81.0cdfipasncc1.01.52.02.53.0life mean duration (weeks)0.800.850.900.951.00cdf1024×1016×1012×1023×102death mean duration (weeks)0.00.20.40.60.81.0cdftotalspammersfruad.serv.malwarephishingexploitspup102mean rate of arrival0.00.20.40.60.81.0cdffigure 9: magnitude analysis of top 5 ips  ases  and countries.asn organization 7276 6762 16509 amazon-02 35994 akamai-as 53684  us university-of-houston 2206 2153 seabone-net  it cn 1817 de 1707 fr 1607 ua  mag.cc mag.82558 377 212 149 80  flashpoint-sc-as  (a) top as  countries (cc) magnitude offenders  (b) hosts  (c) mal-activities  respectively.this is likely due to the fact that the majority of host- ing services and internet users originate from the us.interestingly we observe as7276 (university-of-houston) with 2206 mal- activities per week as the as with the highest severity.they were reported to have participated in all categories of mal-activities ex- cept spammers  with 59785 reports in the dataset.we can observe a large portion of the reported mal-activities originate from potentially misusing cloud provider services (e.g.amazon cloud) as these providers are unlikely to be intentionally propagating their own mal-activity.this inference is consistent with observations made in previous work [58].lessons learned.our analysis shows that malware has been the largest component of reports (90.9%)  see table 2  but when con- sidering the severity of reports  malware on average produces 30.8 reports per week  phishing has the next largest severity  with 9.3 reports per week  despite only consisting of 4.74% of our dataset.on average  malware is approximately 3 times as severe as phishing  despite there being 19 times more malware reports than there is phishing reports.it would be advisable for enforcement agencies to focus on the primary attack vector that is malware  as disabling a malware source would yield the largest reduction of reports per week.not to discount the impact of shutting down a phishing host  it too receives a third of the reports as the most severe mal-activity.5 related work a number of studies have characterized and measured mal-activities  in addition to proposing detection and/or prevention techniques.researchers have also proposed general approaches that rely either on fundamental characteristics of botnet traffic or by correlating meta-datasets.for example  several works detect botnets-based mal-activities by investigating their traffic [34] or typical behav- ior [40  51  60  64].others have investigated multiple datasets includ- ing web resources from suspicious domains [43] host and network information [56  63]  honeypots [54] or dns traffic [61].kuhrer et al.[47] analyze the performance of blacklists  with a forwards- facing collection of data by archiving it for a duration of two years.in this paper  we revisit the blacklists utilized by them and with additional sources  collect a backwards-facing dataset  which is collected post-factum  covering 10 years prior.our analysis of the resulting dataset diverges as we perform the retrospective charac- terization and measurement of (different classes of) mal-activities.using regional internet registry (rir) dataset spanning over a period of 12 years  dhamdhere et al.[38] define two metrics (at- tractiveness and repulsiveness) to describe the relationship among ases.compared to our work  dhamdhere et al.do not focus on mal-activity reporting  instead focusing on the as ecosystem as a whole.antonakakis et al.[33] study the behavior of mirai botnet activity with a dataset collected in 2016 by industrial partners  to ob- serve the resilience of mirai botnet against reverse engineering and takedowns.unfortunately this dataset  is not available for further research  and only focuses on a specific type of mal-activity  whilst our analysis covers six different classes.leita et al.[48] propose “harmur” a system that leverages historical archives of malicious urls collected by symantec to detect mal-activities.in conjunc- tion with publicly available blacklists  dns reports of malicious domains  and symantec’s proprietary malware scanning service to resolve false-positives  for the collection of malicious urls.their proposal retains a large-scale analysis of the collected dataset as future work  however there has been no mention of this dataset to date.it should be noted that harmur leverages the historical information for the purpose of classifying newly observed urls  and thus is considered forwards-facing.by analyzing logs generated by dynamic analysis of malware samples spanning over a period of 5 years  lever et al.[49] inves- tigate the evolution and behavior of the malware and pup cate- gories of mal-activities.in contrast  our study retrieves static data sources that span over 10 years and consists of broader categories of mal-activities in six classes  with an analysis of infrastructure  geo-location and behavior over time.6 conclusion researchers and the industry alike find themselves in a continual arms race to fight against major instances of malicious activity on the internet.although longitudinal datasets like ours do exist  they are mostly proprietary since industries are unable to share it due to reasons of privacy and to maintain a competitive advantage.in this paper  we addressed this gap  with a novel methodology that combined imperfect historical records with machine learning to produce a decade long mal-activity dataset.to assist the research community  we have released our dataset into the public domain for further research:  100101102mean severity (reports/week)0.00.20.40.60.81.0cdfipasncc100101102mean severity (reports/week)0.00.20.40.60.81.0cdftotalpupfruad.serv.malwareexploitsphishingspammershttps://internetmaliciousactivity.github.io/ with our unique dataset  we reflected on the behavior of mal- activity reporting over the last decade in order to gain insights into the continuing presence of malicious activity.our analysis  charac- terized host behavior among other aspects  recurrent periods  and severity of mal-activity reporting in a p2p inspired churn model.our analysis suggests that tracking the heavy mal-activity contrib- utors should be an absolute priority for law-enforcement agencies and major network providers and cloud operators.we found a con- sistent minority of heavy offenders (i.e.ips  ases  and countries) that contribute a majority of mal-activity reports  posing a severe threat to the status-quo of our online ecosystem.we observed a number of hosts with a short renewal cycle of “(in)activity”.their presence on a blacklist has not deterred their activities.had the host been effectively blocked by blacklists  the renewal of their activity indicates the removal of the host from said blacklists suggesting a need to consider longer durations prior to delisting a malicious host.detecting and quickly reacting to the emergence of such heavy mal-activity contributors would arguably significantly reduce the damage inflicted by them
27,2019-Detecting and Characterizing Lateral Phishing at Scale.pdf,  we present the ﬁrst large-scale characterization of lateral phishing attacks  based on a dataset of 113 million employee- sent emails from 92 enterprise organizations.in a lateral phishing attack  adversaries leverage a compromised enter- prise account to send phishing emails to other users  beneﬁt- ting from both the implicit trust and the information in the hijacked user’s account.we develop a classiﬁer that ﬁnds hun- dreds of real-world lateral phishing emails  while generating under four false positives per every one-million employee- sent emails.drawing on the attacks we detect  as well as a corpus of user-reported incidents  we quantify the scale of lat- eral phishing  identify several thematic content and recipient targeting strategies that attackers follow  illuminate two types of sophisticated behaviors that attackers exhibit  and estimate the success rate of these attacks.collectively  these results expand our mental models of the ‘enterprise attacker’ and shed light on the current state of enterprise phishing attacks.1,  for over a decade  the security community has explored a myriad of defenses against phishing attacks.yet despite this long line of work  modern-day attackers routinely and suc- cessfully use phishing attacks to compromise government systems  political ﬁgures  and companies spanning every eco- nomic sector.growing in prominence each year  this genre of attacks has risen to the level of government attention  with the fbi estimating $1,
28,2019-Detecting Violations of Differential Privacy.pdf, the widespread acceptance of differential privacy has led to the publication of many sophisticated algorithms for protecting privacy.however  due to the subtle nature of this privacy definition  many such algorithms have bugs that make them violate their claimed privacy.in this paper  we consider the problem of producing coun- terexamples for such incorrect algorithms.the counterexamples are designed to be short and human-understandable so that the counterexample generator can be used in the development process – a developer could quickly explore variations of an algorithm and investigate where they break down.our approach is statistical in nature.it runs a candidate algorithm many times and uses statistical tests to try to detect violations of differential privacy.an evalua- tion on a variety of incorrect published algorithms validates the usefulness of our approach: it correctly rejects incorrect algorithms and provides counterexamples for them within a few seconds.ccs concepts • security and privacy → privacy protections;  keywords differential privacy; counterexample detection; statistical testing acm reference format: zeyu ding  yuxin wang  guanhong wang  danfeng zhang  and daniel kifer.2018.detecting violations of differential privacy.in 2018 acm sigsac conference on computer and communications security (ccs ’18)  october 15–19  2018  toronto  on  canada.acm  new york  ny  usa  15 pages.https://doi.org/10.1145/3243734.3243818  1, differential privacy has become a de facto standard for extracting information from a dataset (e.g.answering queries  building ma- chine learning models  etc.) while protecting the confidentiality of individuals whose data are collected.implemented correctly  it guarantees that any individual’s record has very little influence on the output of the algorithm.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.ccs ’18  october 15–19  , and future work are discussed in section 6.2 related work  differential privacy.the term differential privacy covers a fam- ily of privacy definitions that include pure ϵ-differential privacy (the topic of this paper) [17] and its relaxations: approximate (ϵ  δ)- differential privacy [16]  concentrated differential privacy [8  20]  and renyi differential privacy [31].the pure and approximate ver- sions have received the most attention from algorithm designers (e.g.see the book [19]).however  due to the lack of availability of easy-to-use debugging and verification tools  a considerable frac- tion of published algorithms are incorrect.in this paper  we focus on algorithms for which there is a public record of an error (e.g.variants of the sparse vector method [12  28]) or where a seemingly small change to an algorithm breaks an important component of  the algorithm (e.g.variants of the noisy max algorithm [5  19] and the histogram algorithm [14]).programming platforms and verification tools.several dynamic tools [21  29  37  39  40] exist for enforcing differential privacy.those tools track the privacy budget consumption at runtime  and terminates a program when the intended privacy budget is ex- hausted.on the other hand  static methods exist for verifying that a program obeys differential privacy during any execution  based on relational program logic [1–7] and relational type sys- tem [24  35  41].we note that those methods are largely orthogonal to this paper: their goal is to verify a correct program or to terminate an incorrect one  while our goal is to detect an incorrect program and generate counterexamples for it.the counterexamples provide valuable guidance for fixing incorrect algorithms for algorithm de- signers.moreover  we believe our tool fills in the currently missing piece in the development of differentially private algorithms: with our tool  immature designs can first be tested for counterexamples  before being fed into those dynamic and static tools.counterexample generation.symbolic execution [9  10  26] is widely used for program testing and bug finding.one attractive feature of symbolic execution is that when a property is being vio- lated  it generates counterexamples (i.e.program inputs) that lead to violations.more relevant to this paper is work on testing rela- tional properties based on symbolic execution [22  30  33].however  those work only apply to deterministic programs  but the differen- tial privacy property inherently involves probabilistic programs  which is beyond the scope of those work.3 background in this section  we discuss relevant background on differential pri- vacy and hypothesis testing.3.1 differential privacy we view a database as a finite multiset of records from some domain.it is sometimes convenient to represent a database by a histogram  where each cell is the count of times a specific record is present.differential privacy relies on the notion of adjacent databases.the two most common definitions of adjacency are: (1) two databases d1 and d2 are adjacent if d2 can be obtained from d1 by adding or removing a single record.(2) two databases d1 and d2 are adja- cent if d2 can be obtained from d1 by modifying one record.the notion of adjacency used by an algorithm must be provided to the counterexample generator.we write d1 ∼ d2 to mean that d1 is adjacent to d2 (under whichever definition of adjacency is relevant in the context of a given algorithm).we use the term mechanism to refer to an algorithm m that tries to protect the privacy of its input.in our case  a mechanism is an algorithm that is intended to satisfy ϵ-differential privacy:  definition 3.1 (differential privacy [17]).let ϵ ≥ 0.a mechanism m is said to be ϵ-differentially private if for every pair of adjacent databases d1 and d2  and every e ⊆ range(m)  we have  p(m(d1) ∈ e) ≤ eϵ · p(m(d2) ∈ e).the value of ϵ  called the privacy budget  controls the level of  the privacy: the smaller ϵ is  the more privacy is guaranteed.session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada476one of the most common building blocks of differentially private algorithms is the laplace mechanism [17]   which is used to answer numerical queries.let d be the set of possible databases.a numer- ical query is a function q : d → rk (i.e.it outputs a k-dimensional vector of numbers).the laplace mechanism is based on a concept called global sensitivity  which measures the worst-case effect one record can have on a numerical query:  definition 3.2 (global sensitivity [17]).the ℓ1-global sensitivity  of a numerical query q is  ∆q = max d1∼d2  ∥q(d1) − q(d2)∥1.the laplace mechanism works by adding laplace noise (having density f (x | b) = 1 2) to query an- 2b swers.the chosen variance depends on ϵ and the global sensitivity.we use the notation lap(b) to refer to the laplace noise.exp(−|x|/b) and variance 2b  definition 3.3 (the laplace mechanism [17]).for any numerical  query q : d → rn  the laplace mechanism outputs  m(d  q  ϵ) = q(d) + (η1  ...ηn)  where ηi are independent random variables sampled from lap(∆q/ϵ).theorem 3.4 ([19]).the laplace mechanism is ϵ-differentially  private.3.2 hypothesis testing a statistical hypothesis is a claim about the parameters of the dis- tribution that generated the data.the null hypothesis  denoted by h0 is a statistical hypothesis that we are trying to disprove.for example  if we have two samples  x and y where x was gener- ated by a binomial(n  p1) distribution and y was generated by a binomial(n  p2) distribution  one null hypothesis could be p1 = p2 (that is  we would like to know if the data supports the conclusion that x and y came from different distributions).the alternative hypothesis  denoted by h1  is the complement of the null hypothesis (e.g.p1 (cid:44) p2).a hypothesis test is a procedure that takes in a data sample z and either rejects the null hypothesis or fails to reject the null hypothesis.a hypothesis test can have two types of errors: type i and type ii.a type i error occurs if the test incorrectly rejects h0 when it is in fact true.a type ii error occurs if the test fails to reject h0 when the alternative hypothesis is true.type i and type ii errors are analogous to false positives and false negatives  respectively.in most problems  controlling type i error is the most important.in such cases  one specifies a significance level α and requires that the probability of a type i error be at most α.commonly used values for α are 0.05 and 0.01.in order to allow users to control the type i error  the hypothesis test also returns a number p – known as the p-value – which is a probabilistic estimate of how unlikely it is that the null hypothesis is true.the user rejects the null hypothesis if p ≤ α.in order for this to work (i.e.in order for the type i error to be below α)  the p-value must satisfy certain technical conditions: (1) a p-value is a function of a data sample z  (2) 0 ≤ p(z) ≤ 1  (3) if the null hypothesis is true  then p(p(z) ≤ α | h0) ≤ α.a relevant example of a hypothesis test is fisher’s exact test [23] for two binomial populations.let c1 be a sample from a binomial(n1  p1)  distribution and let c2 be a sample from a binomial(n2  p2) distribu- tion.here p1 and p2 are unknown.using these values of c1 and c2  the goal is to test the null hypothesis h0 : p1 ≤ p2 against the alter- native h1 : p1 > p2.let s = c1 + c2.the key insight behind fisher’s test is that if c1 ∼ binomial(n1  p1) 1 and c2 ∼ binomial(n2  p2) and if p1 = p2  then the value p(c1 > c1 | c1 + c2 = s) does not depend on the unknown parameters p1 or p2 and can be computed from the cumulative distribution function of the hypergeometric distribution; specifically  it is equal to 1 − hypergeometric.cdf(c1 | n1 + n2  n1  s).when p1 > p2  then p(c1 > c1 | c1 + c2 = s) cannot be computed without knowing p1 and p2.however  it is less than 1 − hypergeometric.cdf(c1 | n1 + n2  n1  s).thus it can be shown that 1 − hypergeometric.cdf(c1 | n1 + n2  n1  s) is a valid p-value and so the fisher’s exact test rejects the null hypothesis when this quantity is ≤ α.4 counterexample detection for a mechanism m that does not satisfy ϵ-differential privacy  the goal is to prove this failure.by definition 3.1  this involves finding a pair of adjacent databases d1  d2 and an output event e such that p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e).thus a counterexample involves finding these two adjacent inputs d1 and d2  the bad output set e  and to show that for these choices  p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e).ideally  one would compute the probabilities p(m(d1) ∈ e) and p(m(d2) ∈ e).unfortunately  for sophisticated mechanisms  it is not always possible to compute these quantities exactly.however  we can sample from these distributions many times by repeatedly running m(d1) and m(d2) and counting the number of times that the outputs fall into e.then  we need a statistical test to reject the null hypothesis p(m(d1) ∈ e) ≤ eϵ p(m(d2) ∈ e) (or fail to reject it if the algorithm is ϵ-differentially private).we will be using the following conventions: • the input to most mechanisms is actually a list of queries q = (q1  ...ql) rather than a database directly.for example  al- gorithms to release differentially private histograms operate on a histogram of the data; the sparse vector mechanism operates on a sequence of queries that each have global sensitivity equal to 1.thus  we require the user to specify how the input query answers can differ on two adjacent databases.for example  in a histogram  exactly one cell count changes by at most 1.in the sparse vector technique [19]  every query answer changes by at most 1.to simplify the discussion  we abuse notation and use d1  d2 to also denote the answers of q on the input adja- cent databases.for example  when discussing the sparse vector technique  we write d1 = [0  0] and d2 = [1  1].this means there are adjacent databases and a list of queries q = [q1  q2] such that they evaluate to [0  0] on the first database and [1  1] on the second database.• we use ϵ0 to indicate the privacy level that a mechanism claims • we use ω for the set of all possible outputs (i.e.range) of the  to achieve.mechanism m.we use ω for a single output of m.1this is read as  c1 is a random variable having the binomial(n1  p1) distribution .session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada477• we call a subset e ⊆ ω an event.we use p1 (respectively  p2) to denote p(m(di) ∈ e)  the probability that the output of m falls into e when executing on database d1 (respectively  d2).• some mechanisms take additional inputs  e.g.the sparse vector  mechanism.we collectively refer to them as args.our discussion is organized as follows.we provide an overview of the counterexample generator in section 4.1.then we incre- mentally explain our approach.in section 4.2 we present the hy- pothesis test.that is  suppose we already have query sequences d1 and d2 that are generated from adjacent databases and an out- put set e  how do we test if p(m(d1) ∈ e) ≤ eϵ p(m(d2) ∈ e) or p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e)? next  in section 4.3  we con- sider the question of output selection.that is  suppose we already have query answers d1 and d2 that are generated from adjacent databases  how do we decide which e should be used in the hy- pothesis test? finally  in section 4.4  we consider the problem of generating the adjacent query sequences d1 and d2 as well as additional inputs args.the details of specific mechanisms we test for violations of dif-  ferential privacy will be given in the experiments in section 5.4.1 overview at a high level  the counterexample generator can be summarized in the pseudocode in algorithm 1.first  it generates an inputlist  a set of candidate tuples of the form (d1  d2  arдs).that is  instead of returning a single pair of adjacent inputs d1  d2 and any auxiliary arguments the mechanism may need  we return multiple candidates which will be filtered later.each adjacent pair (d1  d2) is designed to be short so that a developer can understand the problematic inputs and trace them through the code of the mechanism m.for this reason  the code of m will also run fast  so that it will be possible to later evaluate m(d1  arдs) and m(d2  arдs) multiple times very quickly.algorithm 1: overview of counterexample generator 1 function counterexampledetection(m  ϵ):  ϵ: desired privacy (is m ϵ-differentially private?)  input: m: mechanism // get set of possible inputs: (d1  d2  arдs) inputlist ← inputgenerator(m  ϵ) e  d1  d2  arдs ← eventselector(m  ϵ  inputlist) p⊤  p⊥ ← hypothesistest(m  ϵ  d1  d2  arдs  e) return p⊤  p⊥  2 3 4 5  the next step is the eventselector.it takes each tuple(d1  d2  arдs) from inputlist and runs m(d1  arдs) and m(d2  arдs) multiple times.based on the type of the outputs  it generates a set of candidates for e.for example  if the output is a real number  then the set of candidates is the set of intervals (a  b).for each candidate e and each tuple (d1  d2  arдs)  it counts how many times m(d1  arдs) produced an output ω ∈ e and how many times m(d2  arдs) pro- duced an output in e.based on these results  it picks one specific e and one tuple (d1  d2  arдs) which it believes is most likely to show a violation of ϵ0-differential privacy.finally  the hypothesistest takes the selected e  d1  d2  and args and checks if it can detect statistical evidence that p(m(d1  arдs) ∈ e) > eϵ p(m(d2  arдs) ∈ e) – which corresponds to the p-value p⊤ – or eϵ p(m(d1  arдs) ∈ e) < p(m(d2  arдs) ∈ e) – which corresponds to the p-value p⊥.it is important to note that the eventselector also uses hypothe- sistest internally as a sub-routine to filter out candidates.that is  for every candidate e and every candidate (d1  d2  args)  it runs hypothesistest and treats the returned value as a score.the combi- nation of e and (d1  d2  arдs) with the best score is returned by the evenselector.note that eventselector is using the hypothesistest in an exploratory way – it evaluates many hypotheses and returns the best one it finds.this is why the e and (d1  d2  args) that are finally chosen need to be evaluated again on line 4 using fresh samples from m.interpreting the results.one of the best ways of understanding the behavior of the counterexample generator is to look at the p- values it outputs.that is  we take an mechanism m that claims to satisfy ϵ0-differential privacy and  for each ϵ close to ϵ0  we test whether it satisfies ϵ-differential privacy (that is  even though m claims to satisfy ϵ0-differential privacy  we may want to test if it satisfies ϵ-differential privacy for some other value of ϵ that is close to ϵ0).the hypothesis tester returns two p-values: • p⊤.small values indicate that probably p(m(d1  arдs) ∈ e) > eϵ p(m(d2  arдs) ∈ e).• p⊥.small values indicate that probably p(m(d2  arдs) ∈ e) > eϵ p(m(d1  arдs) ∈ e).for each ϵ  we plot the minimum of p⊤ and p⊥.figure 1 shows typical results that would appear when the counterexample detector is run with real mechanisms m as input.in figure 1a  m correctly satisfies the claimed ϵ0 = 0.7 differen- tial privacy.in that plot  we see that the p-values corresponding to ϵ = 0.2  0.4  0.6 are very low  meaning that the counterexam- ple generator can prove that the algorithm does not satisfy differ- ential privacy for those smaller values of ϵ.near 0.7 it becomes difficult to find counterexamples; that is  if an algorithm satisfies 0.7-differential privacy  it is very hard to statistically prove that it does not satisfy 0.65 differential privacy.this is a typical feature of hypothesis tests as it becomes difficult to reject the null hypothesis when it is only slightly incorrect (e.g.when the true privacy pa- rameter is only slightly different from the ϵ we are testing).now  any algorithm that satisfies 0.7-differential privacy also satisfies ϵ-differential privacy for all ϵ ≥ 0.7.this behavior is seen in figure 1a as the p-values are large for all larger values of ϵ.figure 1b shows a graph that can arise from two distinct sce- narios.one of the situations is when the mechanism m claims to provide 0.7-differential privacy but actually provides more privacy (i.e.ϵ-differential privacy for ϵ < 0.7).in this figure  the counterex- ample generator could prove  for example  that m does not satisfy 0.4-differential privacy  but leaves open the possibility that it satis- fies 0.5-differential privacy.the other situation is when our tool has failed to find good counterexamples.thus when a mechanism is correct  good precision by the counterexample generator means that the line starts rising close to (but before the dotted line)  and worse precision means that the line starts rising much earlier.session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada478(a) expected results for algorithms cor- rectly claiming ϵ0 = 0.7 differential pri- vacy.(b) expected results where counterexam- ples cannot be found or when m satisfies differential privacy for ϵ < ϵ0 = 0.7.(c) expected results where m does not satisfy 0.7-differential privacy (i.e.m has a bug and provides less privacy than advertised).figure 1: interpreting experimental results on hypothesis tests.a hypothetical algorithm m claims to achieve ϵ0-differential privacy.for each ϵ from 0.2 to 2.4 we would evaluate if m satisfies ϵ-differential privacy.we show a typical graph when m does satisfy ϵ0-differential privacy (left)  a graph where m possibly provides more privacy (center) and a graph where m provides less privacy than advertised.figure 1c shows a typical situation in which an algorithm claims to satisfy 0.7-differential privacy but actually provides less privacy than advertised.in this case  the counterexample generator can generate good counterexamples at ϵ = 0.7 (the dotted line) and even at much higher values of ϵ.when an mechanism is incorrect  such a graph indicates good precision by the counterexample generator.limitations.in some cases  finding counterexamples requires a large input datasets.in those cases  searching for the right inputs and running algorithms on them many times will impact the ability of our counterexample generator to find counterexamples.this is a limitation of all techniques based on statistical tests.another important case where our counterexample generator is not expected to perform well is when violations of differential privacy happen very rarely.for example  consider a mechanism m that checks if its input is d1 = [1].if so  with probability e−9 it outputs 1 and otherwise it outputs 0 (if the input is not 1  m always outputs 0).m does not satisfy ϵ-differential privacy for any value of ϵ.however  showing it statistically is very difficult.supposing d1 = [1] and d2 = [0] are adjacent databases  it requires running m(d1) and m(d2) billions of times to observe that an output of 1 is possible under d1 but is at least eϵ times less likely under d2.addressing both of these problems will likely involve incorpo- ration of program analysis  such as symbolic execution  into our statistical framework and is a direction for future work.4.2 hypothesis testing suppose we have a mechanism m  inputs d1  d2 and an output set e (we discuss the generation of d1  d2 in section 4.4 and e in section 4.3).we would like to check if p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e) or if p(m(d2) ∈ e) > eϵ p(m(d1) ∈ e)  as that would demonstrate a violation of ϵ-differential privacy.we treat the p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e) case in this section  as the other case is symmetric.to do this  the high level idea is to: • define p1 = p(m(d1) ∈ e) and p2 = p(m(d2) ∈ e)  algorithm 2: hypothesis test.parameter n: # of iterations 1 function pvalue(c1  c2  n  ϵ): 2 3 4 5 function hypothesistest(m  arдs  ϵ  d1  d2   e):  ˜c1 ← binomial(c1  1/eϵ) s ← ˜c1 + c2 return 1 − hypergeom.cdf(˜c1 | 2n  n  s) input: m: mechanism  arдs: additional arguments for m ϵ: privacy budget to test d1  d2: adjacent databases e: event  6 7 8 9 10 11 12  o1 ← results of running m(d1  arдs) for n times o2 ← results of running m(d2  arдs) for n times c1 ← |{i | o1[i] ∈ e}| c2 ← |{i | o2[i] ∈ e}| p⊤ ←pvalue (c1  c2  n  ϵ) p⊥ ←pvalue (c2  c1  n  ϵ) return p⊤  p⊥  • formulate the null hypothesis as h0 : p1 ≤ eϵ · p2 and the alternative as h1 : p1 > eϵ · p2.• run m with inputs d1 and d2 independently n times each.record the results as o1 and o2.• count the number of times the result falls in e in each case.let c1 = |{i | o1[i] ∈ e}| and c2 = |{i | o2[i] ∈ e}|.intuitively  c1 ≫ eϵ c2 provides strong evidence against the null hypothesis.• calculate a p-value based on c1  c2 to determine how unlikely the null hypothesis is.the challenge is  of course  in the last step as we don’t know what p1 and p2 are.one direction is to estimate them from c1 and c2.however  it is also challenging to estimate the variance of our estimates ˆp1 and ˆp2 (the higher the variance  the less the test should trust the estimates).0.20.40.60.81.01.2test0.00.20.40.60.81.0pvaluetestresultideal0.20.40.60.81.01.21.4test0.00.20.40.60.81.0pvaluetestresultideal0.250.500.751.001.251.501.75test0.00.20.40.60.81.0pvaluetestresultidealsession 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada479instead  we take a different approach that allows us to conduct the test without knowing what p1 and p2 are.first  we note that c1 and c2 are equivalent to samples from a binomial(n  p1) dis- tribution and a binomial(n  p2) distribution respectively.we first consider the border case where p1 = eϵp2.consider sample ˜c1 from a binomial(c1  1/eϵ) distribution.we note that this sample enjoys the following property (which implies that in the border case  ˜c1 has the same distribution as c2):  lemma 4.1.let x ∼binomial(n  p1) and z be generated from x by sampling from the binomial(x   1/eϵ) distribution.the marginal distribution of z is binomial(n  p1/eϵ).dom variables means that x =n 1/eϵ (and set zi = 0 otherwise).then set z =n  proof.the relationship between binomial and bernoulli ran- i =1 xi  where xi is a bernoulli(p1) random variable.generating z from x is the same as doing the following: set zi = 0 if xi = 0.if xi = 1  set zi = 1 with probability i =1 zi.hence  the marginal distribution of zi is a bernoulli(p1/eϵ) random variable: p(zi = 1) = p(zi = 1 | xi = 1)p(xi = 1) + p(zi = 1 | xi = 0)p(xi = 0)  = (1/e ϵ) · p1 + 0 · (1 − p1) = p1/e ϵ  this means that the marginal distribution of z is binomial(n  p1/eϵ).□  thus we have the following facts that follow immediately from  the lemma: • if p1 > eϵp2 then the distribution of ˜c1 is binomial(n  ˜p1) with ˜p1 = p1/eϵ and so has a larger binomial parameter than c2 (which is binomial(n  p2)).we want our test to be able to reject the null hypothesis in this case.• if p1 = eϵp2 then the distribution of ˜c1 is binomial(n  ˜p1) with ˜p1 = p2 and so has the same binomial parameter as c2.we do not want our test to reject the null hypothesis in this case.• if p1 < eϵp2 then the distribution of ˜c1 is binomial(n  ˜p1) with ˜p1 = p1/eϵ and so has a smaller binomial parameter than c2 (which is binomial(n  p2)).we do not want to reject the null hypothesis in this case.thus  by randomly generating ˜c1 from c1  we have (randomly) reduced the problem of testing p1 > eϵp2 vs.p1 ≤ eϵp2 (on the basis of c1 and c2) to the problem of testing ˜p1 > p2 vs.˜p1 ≤ p2 (on the basis of ˜c1 and c2).now  checking whether ˜c1 and c2 come from the same distribution can be done with the fisher’s exact test (see section 3): the p-value is 1 − hypergeom.cdf(˜c1 | 2n  n  ˜c1 + c2).2 this is done in the function pvalue in algorithm 2.to summarize  given c1 and c2  we first sample ˜c1 from the binomial(c1  1/eϵ) distribution and then return the p-value of (1 − hypergeom.cdf(˜c1 | 2n  n  ˜c1 +c2)).since this is a random reduction  we reduce its variance by sampling ˜c1 multiple times and averaging the p-values.that is  we run the p-value function (algorithm 2) multiple times with the same inputs and average the p-values it returns.2here we use a notation from scipy [25] package where hypergeom.cdf means the cumulative distribution function of hypergeometric distribution.4.3 event selection having discussed how to test if p(m(d1) ∈ e) > eϵ p(m(d2) ∈ e) or if p(m(d2) ∈ e) > eϵ p(m(d1) ∈ e) when d1  d2  and e were pre-specified  we now discuss how to select the event e that is most likely to show violations of ϵ-differential privacy.algorithm 3: event selector.parameter n: # of iterations 1 function eventselector(n  m  ϵ  inputlist):  input: m: mechanism  inputlist: possible inputs ϵ: privacy budget to test  searchspace ← search space based on return type pvalues ← [ ] results ← [ ] foreach (d1  d2  arдs) ∈ inputlist do  o1 ← results of running m(d1  arдs) for n times o2 ← results of running m(d2  arдs) for n times foreach e ∈ seachspace do  2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  c1 ← |{i | o1[i] ∈ e}| c2 ← |{i | o2[i] ∈ e}| p⊤ ←pvalue (c1  c2  n  ϵ) p⊥ ←pvalue (c2  c1  n  ϵ) pvalues.append(min(p⊤  p⊥)) results.append(d1  d2  arдs  e)  end  end return results[argmin(pvalues)]  one of the challenges is that different mechanisms could have different output types (e.g.a discrete number  a vector of numbers  a vector of categorical values  etc.).to address this problem  we define a search space s of possible events to look at.the search space depends on the type of the output ω of m  which can be determined by running m(d1) and m(d2) multiple times.(1) the output ω is a fixed length list of categorical values.we first run m(d1) once and ask it to not use any noise (i.e.tell it to satisfy ϵ-differential privacy with ϵ = ∞).denote this output as ω0.now  when m runs with its preferred privacy settings to produce an output ω  we define t(ω) be the hamming distance between the output ω and ω0.the search space is  s = {{ω | t(ω) = k} : k = 0  1  ...l}  where l is the fixed length of output of m.another set of events relate to the count of a categorical value in the output.if there are m values  then define  si = {{ω | ω.count(valuei) = k} : k = 0  1  ...l}   1 ≤ i ≤ m.the overall search space is the union of s and all si.(2) the output ω is a variable length list of categorical val- ues.in this case  one extra set of events e we look at corre- spond to the length of the output.for example  we may check if p(m(d1) has length k) > p(m(d2) has length k).hence  we define  s0 = {{ω | ω.lenдth = k} : k = 0  1  ...}  session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada480(3) the output ω is a fixed length list of numeric values.for the search space s  we use this s0 unioned with the search space from the previous case.in this case  the output is of the form ω = (a1  ...am).our search space is the union of the following:  {{ω | ω[i] ∈ (a  b)} : i = 1  ...m and a < b}  {{ω | avg(ω) ∈ (a  b)} : a < b}  {{ω | min(ω) ∈ (a  b)} : a < b}  {{ω | max(ω) ∈ (a  b)} : a < b}.that is  we would end up checking if p(avд(m(d1)) ∈ (a  b)) > p(avд(m(d2)) ∈ (a  b))  etc..to save time  we often restrict a and b to be multiples of a small number like ±0.2  or ±∞.in the case that the output ω is always an integer array  we replace the condition “∈ (a  b)” with “= k” for each integer k.(4) m outputs a variable length list of numeric values.the search space is the union of case 3 and s0 in case 2.(5) m outputs a variable length list of mixed categorical and numeric values.in this case  we separate out the categorical values from numeric values and use the cross product of the search spaces for numeric and categorical values.for instance  events would be of the form “ω has k categorical components equal to ℓ and the average of the numerical components of ω is in (a  b)” the eventselector is designed to return one event e for use in the hypothesis test in algorithm 1.the way eventselector works is it receives an inputlist  which is a set of tuples (d1  d2  arдs) where d1  d2 are adjacent databases and args is a set of values for any other parameters m needs.for each such tuple  it runs m(d1) and m(d2) for n times each.then for each possible event in the search space  it runs the hypothesis test (as an exploratory tool) to get a p-value.the combination of (d1  d2  arдs) and e that produces the lowest p-value is then returned to algortihm 1.algorithm 1 uses those choices to run the real hypothesis test on fresh executions of m on d1 and d2.the pseudocode for the eventselector is shown in algorithm 3.3  4.4 input generation in this section we discuss our approaches for generating candidate tuples (d1  d2  arдs) where d1  d2 are adjacent databases and args is a set of auxiliary parameters that a mechanism m may need.4.4.1 database generation .to find the adjacent databases that are likely to form the basis of counterexamples that illustrate violations of differential privacy  we adopt a simple and generic approach that works surprisingly well.recalling that the inputs to mechanisms are best modeled as a vector of query answers  we use the type of patterns shown in table 1.the “one above” and “one below” categories are suitable for algorithms whose input is a histogram (i.e.in adjacent databases  at most one query can change  and it will change by at most 1).the rest of the categories are suitable when in adjacent databases every  3in practice  to avoid choosing bad e  we let ce be the total number of times m(d1) and/or m(d2) produced an output in e.then it only executes line 11-14 in algorithm 3 if ce ≥ 0.001 · n · e ϵ   otherwise the selection of e is too noisy.3  algorithm 4: input generator.1 function argumentgenerator(m  d1  d2): 2  4 5 6 function inputgenerator(m  len):  arдs0 ← arguments used in noise generation with values that minimize the noises constraints ← traverse the source code of m and generate constraints to force d1 and d2 to diverge on branches arдs1 ← maxsmt(constraints) return arдs0 + arдs1 input: m: mechanism candidates ← empirical pairs of databases of length len inputlist ← [ ] foreach (d1  d2) ∈ candidates do  len: length of input to generate  7 8 9 10 11 12 13  arдs ← argumentgenerator(m  d1  d2) inputlist.append(d1  d2  arдs)  end return inputlist  table 1: database categories and samples  category one above one below  sample d1 [1  1  1  1  1] [1  1  1  1  1] one above rest below [1  1  1  1  1] [1  1  1  1  1] one below rest above [1  1  1  1  1] all above & all below [1  1  1  1  1] [1  1  0  0  0]  half half  x shape  sample d2 [2  1  1  1  1] [0  1  1  1  1] [2  0  0  0  0] [0  2  2  2  2] [0  0  0  2  2] [2  2  2  2  2] [0  0  1  1  1]  query can change by at most one (i.e.the queries have sensitivity4 ∆q = 1).the design of the categories is based on the wide variety of changes in query answers that are possible when evaluated on one database and on an adjacent database.for example  it could be the case that a few of the queries increase (by 1  if their sensitivity is 1  or by ∆q in the general case) but most of them decrease.a simple representative of this situation is “one above rest below” in which one query increases and the rest decrease.the category “one below rest above” is the reverse.another situation is where roughly half of the queries increase and half decrease (when evaluated on a database compared to when evaluated on an adjacent database).this scenario is captured by the “half half” category.another situation is where all of the queries increase.this is captures by the “all above & all below” category.finally  the “x shape” category captures the setting where the query answers are not all the same and some increase and others decrease when evaluated on one database compared to an adjacent database.these categories were chosen from our desire to allow coun- terexamples to be easily understood by mechanism designers (and to make it easier for them to manually trace the code to understand  4for queries with larger sensitivity  the extension is obvious.for example d1 = [1  1  1  1  1] and d2 = [1 + ∆q  1 + ∆q  ...1 + ∆q]  session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada481the problems).thus the samples are short and simple.we consider inputs of length 5 (as in table 1) and also versions of length 10.4.4.2 argument generation.some differentially-private algorithms require extra parameters beyond the database.for example  the sparse vector technique [19]  shown in algorithm 11  takes as inputs a threshold t and a bound n .it tries to output numerical queries that are larger than t .however  for privacy reasons  it will stop after it returns n noisy queries whose values are greater than t .these two arguments are specific to the algorithm and their proper values depend on the desired privacy level as well as algorithm precision.to find values of auxiliary parameters (such as n and t in sparse vector)  we build argument generator based on symbolic execution [26]  which is typically used for bug finding: it generates concrete inputs that violate assertions in a program.in general  a symbolic executor assigns symbolic values  rather than concrete values as normal execution would do  for inputs.as the execution goes  the executor maintains a symbolic program state at each assertion and generates constraints that will violate the assertion.when those constraints are satisfiable  concrete inputs (i.e.a solution of the constraints) are generated.compared with standard symbolic execution  a major difference in our argument generation is that we are interested in algorithm arguments that will likely maximize the privacy cost of an algorithm.in other words  there is no obvious assertion to be checked in our argument generation.to proceed  we use two heuristics that likely will cause large privacy cost of an algorithm: • the first heuristic applies to parameters that affect noise gener- ation.for example in sparse vector  the algorithm adds lap(2 · n · ∆q/ϵ0) noise.for such a variable  we use the value that results in small amount of noise (i.e.n = 1).small amount of noise is favorable since it reduces the variance in the hypothesis testing (section 4.2).• the second heuristic (for variables that do not affect noise) prefers arguments that make two program executions using two different databases (as described in section 4.4.1) to take as many diverging branches as possible.the reason is that diverging branches will likely use more privacy budget.next  we give a more detailed overview of our customized sym-  bolic executor.the symbolic executor takes a pair of concrete databases as inputs (as described in section 4.4.1) and uses symbolic values for other input parameters.random samples in the program (e.g.a sample from laplace distribution) are set to value 0 in the symbolic execution.then  the symbolic executor tracks symbolic program states along program execution in the standard way [26].for ex- ample  the executor will generate a constraint5 x = y + 1 after an assignment (x ← y+1)  assuming that variable y has a symbolic value y before the assignment.also  the executor will unroll loops in the source code  which is standard in most symbolic executors.unlike standard symbolic executors  the executor conceptually tracks a pair of symbolic program states along program execution (one on concrete database d1  and one on concrete database d2).moreover  it also generate extra constraints  according to the two heuristics above  in the hope of maximizing the privacy cost of an 5for simplicity  we use a simple representation for constraints; z3 has an internal format and a user can either use z3’s apis or smt2 [34] format to represent constraints.algorithm.in particular  it handles two kinds of statements in the following way: • sampling.the executor generates two constraints for a sam- pling statement: a constraint that eliminates randomness in symbolic execution by assigning sample to value 0  and a con- straint that ensures a small amount of noise.consider a state- ment (η ← lap(e)).the executor generates two constraints: η = 0 as well as a constraint that minimizes expression e.• branch.the executor generates a constraint that makes the two executions diverge on branches.consider a branch state- ment (if e then · · · ).assume that the executor has symbolic values e1 and e2 for the value of expression e on databases d1 and d2 respectively; it will generates a constraint (e1 ∧ ¬e2) ∨ (¬e1 ∧ e2) to make the executions diverge.note that unlike other constraints  a diverging constraint might be unsatisfiable (e.g.if the query answers under d1 and d2 are the same).how- ever  our goal is to maximize the number of satisfiable diverging constraints  which can be achieved by a maxsmt solver.the executor then uses an external maxsmt solver such as z3 [13] on all generated constraints to find arguments that maximizes the number of diverged branches.for example  the correct version of the sparse vector algorithm (see the complete algorithm in algorithm 11) has the parameter t (a threshold).it has a branch that tests whether the noisy query answer is above the threshold t :  q + η2 ≥ ˆt  here  η2 is a noise variable  q is one query answer (i.e.one of the components of the input d1 of the algorithm) and ˆt is a noisy threshold ( ˆt = t + η1).suppose we start from a database candidate ([1  1  1  1  1]  [2  2  2  2  2]).the symbolic executor assigns symbolic values to the parameters t and unrolls the loop in the algorithm  where each iteration handles one noisy query.along the execution  it updates program states.for example  statement ˆt ← t + η1 results in ˆt = t + η1.for the first execution of the branch of interest  the executor tracks the following symbolic program state: q1 = 1 ∧ q2 = 2 ∧ η1 = 0 ∧ η2 = 0 ∧ ˆt1 = t + η1 ∧ ˆt2 = t + η2 as well as the following constraint for diverging branches:  (q1 + η1 ≥ ˆt1 ∧ q2 + η2 < ˆt2) ∨ (q1 + η1 < ˆt1 ∧ q2 + η2 ≥ ˆt2) similarly  the executor generates constraints from other itera- tions.in this example  the maxsmt solver returns a value in be- tween of 1 and 2 so that constraints from all iterations are satisfied.this value of t is used as arg in the candidate tuple (d1  d2  arд).5 experiments we implemented our counterexample detection framework with all components  including hypothesis test  event selector and input generator.the implementation is publicly available6.the tool takes in an algorithm implementation and the desired privacy bound ϵ0  and generates counterexamples if the algorithm does not satisfy ϵ0-differential privacy.6 https://github.com/cmla-psu/statdp.session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada482in this section we evaluate our detection framework on some of the popular privacy mechanisms and their variations.we demon- strate the power of our tool: for mechanisms that falsely claim to be differentially private  our tool produces convincing evidence that this is not the case in just a few seconds.5.1 noisy max report noisy max reports which one among a list of counting queries has the largest value.it adds lap(2/ϵ0) noise to each answer and returns the index of the query with the largest noisy answer.the correct versions have been proven to satisfy ϵ0-differential privacy [19] no matter how long the input list is.a naive proof would show that it satisfies (ϵ0 · |q|/2)-differential privacy (where |q| is the length of the input query list)  but a clever proof shows that it actually satisfies ϵ0-differential privacy.algorithm 5: correct noisy max with laplace noise 1 function noisymax(q  ϵ0):  input:q: queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  noisyvector[i] ← q[i] + lap(2/ϵ0)  end return arдmax(noisyvector)  5.1.1 adding laplace noise.the correct noisy max algorithm (al- gorithm 5) adds independent lap(2/ϵ0) noise to each query answer and returns the index of the maximum value.as figure 2a shows  we test this algorithm for different privacy budget ϵ0 at 0.2  0.7  1.5.all lines rise when the test ϵ is slightly less than the claimed privacy level ϵ0 of the algorithm.this demonstrates the precision of our tool: before ϵ0  there is almost 0 chance to falsely claim that this algorithm is not private; after ϵ0  the p-value is too large to conclude that the algorithm is incorrect.we note that the test result is very close to the ideal cases  illustrated by the vertical dashed lines.algorithm 6: correct noisy max with exponential noise 1 function noisymax(q  ϵ0):  input:q: queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  noisyvector[i] ← q[i] + exponential(2/ϵ0)  end return arдmax(noisyvector)  2 3 4 5 6  2 3 4 5 6  algorithm 7: incorrect noisy max with laplace noise  return- ing the maximum value 1 function noisymax(q  ϵ0):  input:q: queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  noisyvector[i] ← q[i] + laplace(2/ϵ0)  end // returns maximum value instead of index return max(noisyvector)  algorithm 8: incorrect noisy max with exponential noise  returning the maximum value 1 function noisymax(q  ϵ0):  input:q: queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  noisyvector[i] ← q[i] + exponential(2/ϵ0)  end // returns maximum value instead of index return max(noisyvector)  algorithm 9: histogram 1 function histogram(q  ϵ0):  input:q:queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  noisyvector[i] ← q[i] + lap(1/ϵ0)  end return noisyvector  algorithm 10: histogram with wrong scale 1 function histogram(q  ϵ0):  input:q: queries to the database  ϵ0: privacy budget.noisyvector ← [ ] for i = 1 ...len(q) do  // wrong scale of noise is added noisyvector[i] ← q[i] + lap(ϵ0)  end return noisyvector  2 3 4 5 6  7  2 3 4 5 6  7  2 3 4 5 6  2 3 4  5 6  5.1.2 adding exponential noise.one correct variant of noisy max adds exponential(2/ϵ0) noise  rather than laplace noise  to each query answer(algorithm 6).this mechanism has also been proven to be ϵ0-differential private[19].figure 2b shows the corresponding test result  which is similar to that of figure 2a.the result indicates that this correct variant likely satisfies ϵ0-differential privacy for the claimed privacy budget.incorrect variants of exponential noise.an incorrect variant 5.1.3 of noisymax has the same setup but instead of returning the index of maximum value  it directly returns the maximum value.we evaluate on two variants that report the maximum value instead of the index (algorithm 7 and 8) and show the test result in figure 2c and 2d.for the variant using laplace noise (figure 2c)  we can see that for ϵ0 = 0.2  the line rises at around test ϵ of 0.4  indicating that this algorithm is incorrect for the claimed privacy budget of 0.2.session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada483(a) correct noisy max with laplace noise.(b) correct noisy max with exponential noise.(c) incorrect variant with laplace noise.it returns the maximum value instead of the index.(d) incorrect variant with exponential noise.it returns the maximum value instead of the index.figure 2: results of noisy max algorithm and its variants.the same pattern happens when we set privacy budget to be 0.7 and 1.5: all lines rise much later than their claimed privacy budget.in this incorrect version  returning the maximum value (instead of its index) causes the algorithm to actually satisfy ϵ0 · |q|/2 dif- ferential privacy instead of ϵ0-differential privacy.for the variant using exponential noise (figure 2d)  the lines rise much later than the claimed privacy budgets  indicating strong evidence that this variant is indeed incorrect.also  we can hardly see the lines for privacy budgets 0.7 and 1.5  since their p-values remain 0 for all the test ϵ ranging from 0 to 2.2 in the experiment.5.2 histogram the histogram algorithm [14] is a very simple algorithm for publish- ing an approximate histogram of the data.the input is a histogram and the output is a noisy histogram with the same dimensions.the histogram algorithm requires input queries to differ in at most one  element.here we evaluate with different scale parameters for the added laplace noise.the correct histogram algorithm adds independent lap(1/ϵ0) noise to each query answer  as shown in algorithm 9.since at most one query answer may differ by at most 1  returning the maximum value is ϵ0-differentially private [14].to mimic common mistakes made by novices of differential privacy  we also evaluate on an incorrect variant where lap(ϵ0) noise is used in the algorithm (algorithm 10).we note that the incorrect variant here satisfies 1/ϵ0-differential privacy  rather the claimed ϵ0-differential privacy.figures 3a and 3b show the test results for the correct and in- correct variants respectively.here  figures 3a indicates that the correct implementation satisfies the claimed privacy budgets.for the incorrect variant  the claimed budgets of 0.2 and 0.7 are cor- rectly rejected; this is expected since the true privacy budgets are 1/0.2 and 1/0.7 respectively for this incorrect version.interestingly   0.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.5session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada484(a) correct histogram algorithm with lap(1/ϵ0) noise.(b) incorrect histogram algorithm with lap(ϵ0) noise.it provides more privacy than advertised when ϵ0 ≥ 1 and less privacy than advertised when ϵ0 < 1.figure 3: results of histogram algorithm and its variants  algorithm 11: svt [28].input:q: queries to the database  ϵ0: privacy budget t : threshold  n : bound of outputting true’s ∆: sensitivity  1 function svt(q  t   ϵ0  ∆  n ): 2 3  out ← [] η1 ← lap(2 ∗ ∆/ϵ0) ˜t ← t + η1 count ← 0 foreach q in q do  η2 ← lap(2 ∗ n ∗ ∆/ϵ0) if q + η2 ≥ ˜t then  out ← true :: out count ← count + 1 if count ≥ n then  break  end out ← false :: out  else  end  end return (out)  the result indicates that for ϵ0 = 1.5  this algorithm is likely to be more private than claimed (the line rise around 0.6 rather than 1.5).again  this is expected since in this case  the variant is indeed 1/1.5 = 0.67-differentially private.5.3 sparse vector the sparse vector technique (svt) [18] (see algorithm 11) is a powerful mechanism for answering numerical queries.it takes a list of numerical queries and simply reports whether their answers are above or below a preset threshold t .it allows the program to output some noisy query answers without any privacy cost.in particular  arbitrarily many “below threshold” answers can be returned  but only at most n “above threshold” answers can be returned.because of this remarkable property  there are many variants proposed in both published papers and practical use.however  most of them turn out to be actually not differentially private[28].we test our tool on a correct implementation of svt and the major incorrect variants summarized in [28].in the following  we describe what the variants do and list their pseudocodes.svt [28].lyu et al.have proposed an implementation of svt 5.3.1 and proved that it satisfies ϵ0-differential privacy.this algorithm (algorithm 11) tries to allocate the global privacy budget ϵ0 into two parts: half of the privacy budget goes to the threshold  and the other half goes to values which are above the threshold.there will not be any privacy cost if the noisy value is below the noisy threshold  in which case the program will output a false.if the noisy value is above the noisy threshold  the program will output a true.after outputting a certain amount (n ) of true’s  the program will halt.figure 4a shows the test result for this correct implementation.all lines rise around the true privacy budget  indicating that our tool correctly conclude that this algorithm is correct.4 5 6 7  8 9 10 11 12 13 14 15 16 17 18  isvt 1 [38].one incorrect variant (algorithm 12) adds no 5.3.2 noise to the query answers  and has no bound on the number of true’s that the algorithm can output.this implementation does not satisfy ϵ0-differential privacy for any finite ϵ0.this expectation is consistent with the test result shown in figure 4b: the p-value never rises at any test ϵ.this result strongly indicates that this implementation with claimed privacy budget 0.2  0.7  1.5 is not private for at least any ϵ ≤ 2.2.0.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.5session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada485algorithm 12: isvt 1 [38].this does not add noise to the query answers  and has no bound on number of true’s to output (i.e.n ).this is not private for any privacy budget ϵ0 .input:q: queries to the database  ϵ0: privacy budget  algorithm 14: isvt 2 [11].this one has no bounds on number of true’s (i.e  n ) to output.this is not private for any finite privacy budget ϵ0.input:q: queries to the database  ϵ0: privacy budget  t : threshold  ∆: sensitivity  1 function isvt1(q  t   ϵ0  ∆): 2 3  out ← [] η1 ← lap(∆/ϵ0) ˜t ← t + η1 // no bounds on number of outputs foreach q in q do  // adds no noise to query answers η2 ← 0 if q + η2 ≥ ˜t then  out ← true :: out out ← false :: out  else  end  end return (out)  1 function isvt3(q  t   ϵ0  ∆  n ): 2 3  out ← [] η1 ← lap(4 ∗ ∆/ϵ0) ˜t ← t + η1 count ← 0 foreach q in q do  // noise added doesn’t scale with n η2 ← lap(4 ∗ ∆/(3 ∗ ϵ0)) if q + η2 ≥ ˜t then  out ← true :: out count ← count + 1 if count ≥ n then  break  end out ← false :: out  else  end  end return (out)  4 5  6  7  8  9 10 11 12 13 14 15  4 5 6 7  8  9 10 11 12 13 14 15 16 17 18 19  4 5  6  7 8  9 10 11 12 13 14 15  4 5 6 7  8 9  10  11 12 13 14 15 16 17 18 19  algorithm 13: isvt 3 [27].the noise added to queries doesn’t scale with n .the actual privacy cost is 1+6n4 input:q: queries to the database  ϵ0: privacy budget t : threshold  n : bound of outputting true’s ∆: sensitivity  ϵ0.algorithm 15: isvt 4 [36].when the noisy query answer is above the threshold  output the actual value of noisy query answer.input:q: queries to the database  ϵ0: privacy budget t : threshold  n : bound of outputting true’s ∆: sensitivity  t : threshold  ∆: sensitivity  1 function isvt2(q  t   ϵ0  ∆): 2 3  out ← [] η1 ← lap(2 ∗ ∆/ϵ0) ˜t ← t + η1 // no bounds on number of outputs foreach q in q do  η2 ← lap(2 ∗ ∆/ϵ0) if q + η2 ≥ ˜t then out ← true :: out out ← false :: out  else  end  end return (out)  1 function isvt4(q  t   ϵ0  ∆  n ): 2 3  out ← [] η1 ← lap(2 ∗ ∆/ϵ0) ˜t ← t + η1 count ← 0 foreach q in q do  η2 ← lap(2 ∗ n ∗ ∆/ϵ0) if q + η2 ≥ ˜t then  break  end out ← false :: out  else  end  end return (out)  // output numerical value instead of boolean value out ← (q + η2) :: out count ← count + 1 if count ≥ n then  isvt 2 [11].another incorrect variant (algorithm 14) has 5.3.3 no bounds on the number of true’s the algorithm can output.with- out the bounds  the algorithm will still output true even if it has  exhausted its privacy budget.so this variant is not private for any finite ϵ0.figure 4c indicates this implementation with privacy budget ϵ0 = 0.2 is most likely not private for any ϵ ≤ 0.5.when ϵ0 = 0.7   session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada486(a) correct implementation of svt [28].(b) isvt 1 [38] adds no noise to query and threshold.(c) isvt 2 [11] no bounds on outputting true’s.(d) isvt 3 [27] query noise does not scale with n .(e) isvt 4 [36] outputs the actual query answer when it is above the threshold.figure 4: results for variants of sparse vector technique  0.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.250.500.751.001.251.501.752.00test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.00.51.01.52.02.53.0test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.00.51.01.52.02.53.0test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.50.00.51.01.52.0test0.00.20.40.60.81.0pvalue0=0.20=0.70=1.5session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada487table 2: counterexamples detected for incorrect privacy mechanisms  mechanism (ϵ0 = 1.5)  incorrect noisy max with laplace noise  incorrect noisy max with exponential noise  incorrect histogram [17]  isvt 1 [38] isvt 2 [11] isvt 3 [27] isvt 4 [36]  event e  ω ∈ (−∞  0.0) ω ∈ (−∞  1.0) ω[0] ∈ (−∞  1.0)  t(ω) = 0 t(ω) = 9 t(ω) = 0  (ω .count(f alse)  ω[9]) ∈ {9} × (−2.4  2.4)  d1  [1  1  1  1  1] [1  1  1  1  1] [1  1  1  1  1]  [1  1  1  1  1  1  1  1  1  1] [1  1  1  1  1  0  0  0  0  0] [1  1  1  1  1  0  0  0  0  0] [1  1  1  1  1  1  1  1  1  1]  d2  [0  0  0  0  0] [0  0  0  0  0] [2  1  1  1  1]  [0  0  0  0  0  2  2  2  2  2] [0  0  0  0  0  1  1  1  1  1] [0  0  0  0  0  1  1  1  1  1] [0  0  0  0  0  0  0  0  0  0]  we have detected counterexamples showing the algorithm is likely not private for any ϵ ∈ (0  2.1].when ϵ0 = 1.5  we have detected counterexamples showing the algorithm is likely not private for any ϵ ∈ (0  3.0].isvt 3 [27].another incorrect variant (algorithm 13) adds 5.3.4 noise to queries but the noise doesn’t scale with the bound n .the actual privacy budget for this variant is 1+6n4 ϵ0 where ϵ0 is the input privacy budget.we note that our tool detects the actual privacy cost  as shown in figure 4d  for this incorrect algorithm.consider privacy budget ϵ0 = 0.2.the corresponding line rises at 0.3  right before the actual budget 1+6n4 ϵ0 = 0.35 (n = 1)  suggesting the precision of our tool.the same happens for ϵ0 = 0.7 and 1.5.the two lines rise at 1.1 and 2.3  which are close to but before the actual budget 1.225 and 2.625  respectively.isvt 4 [36].another incorrect variant (algorithm 15) out- 5.3.5 puts the actual value of noisy query answer when it is above the noisy threshold.the interesting part of this algorithm is that  since it outputs het- erogeneous list of booleans and values  our event selector chooses {9} × (−2.4  2.4).this means we choose an event that consists of 9 booleans (in this case  falses) followed by a number in (−2.4  2.4).figure 4e shows much noise in it because this one is almost cor- rect in the sense that violations of differential privacy happen with very low probability; thus it is hard to detect its incorrectness.but we can still see that the lines all rise later than the corresponding claimed privacy budget ϵ0.hence  our tool correctly concludes that this algorithm does not satisfy ϵ0-differential privacy.5.4 performance we performed all experiments on a double intel® xeon® e5-2620 v4 @ 2.10ghz cpu machine with 64 gb memory.our tool is im- plemented in anaconda distribution of python 3 and optimized for running in parallel environment to fully utilize the 32 logical cores of the machine.for each test ϵ  we set the samples of iteration n to be 500 000 for the hypothesis test and 100 000 for the event selector and query generator.table 3 lists the average time spent on hypothesis test for a specific test ϵ (i.e.the average time spent on generating one single point in the figures) for each algorithm.the results suggest that it is very efficient to run a test for an algorithm against one privacy cost: all tests finish within 23 seconds.the time difference between noisy max  histogram and sparse vector technique is due to the nature of the algorithms.for svt  the  table 3: time spent on running tool for different algorithms  mechanism  correct laplace noisy max[15] incorrect laplace noisy max  correct exponential noisy max [15] incorrect exponential noisy max  histogram [14]  incorrect histogram  svt [28] isvt 1 [38] isvt 2 [11] isvt 3 [27] isvt 4 [36]  time / seconds  4.32 9.49 4.25 8.70 10.39 11.28 1.99 1.62 4.56 2.56 22.97  parameter n is set to 1  meaning that the algorithm will halt once it hit a true branch.for noisy max and histogram  all noise will be calculated and applied to each query answer  consuming more time to calculate p-values.another factor that will also influence the test time is the search space of events.correct noisy max returns an index which we would have a search space of only integers ranging from 1 to the length of queries.however  the incorrect noisy max will return a real number so the search space would be much larger than the correct one  thus taking more time to find a suitable event e.this also occurs in sparse vector technique.6 conclusions and future work while it is invaluable to formally verify correct differentially-private algorithms  we believe that it is equally important to detect incor- rect algorithms and provide counterexamples for them  due to the subtleties involved in algorithm development.we proposed a novel semi-black-box method of evaluating differentially private algo- rithms  and providing counterexamples for those incorrect ones.we show that within a few seconds  our tool correctly rejects incorrect algorithms (including published ones) and provides counterexam- ples for them.future work includes extensions that detect violations of differ- ential privacy even if those violations occur with extremely small probabilities.this will require additional extensions such as a more refined use of program analysis techniques (including symbolic execution) that reason about what happens when a program is run on adjacent databases.additional extensions include counterex- ample generation for other variants of differential privacy  such  session 3b: differential privacy 2ccs’18  october 15-19  2018  toronto  on  canada488[22] gian pietro farina  stephen chong  and marco gaboardi.2017.relational sym-  bolic execution.http://arxiv.org/abs/1711.08349.(2017).[23] r.a.fisher.1935.the design of experiments.1935.oliver and boyd  edinburgh.[24] marco gaboardi  andreas haeberlen  justin hsu  arjun narayan  and benjamin c.pierce.2013.linear dependent types for differential privacy.in proceedings of the 40th annual acm sigplan-sigact symposium on principles of programming languages (acm symposium on principles of programming languages (popl)).357–370.[25] eric jones  travis oliphant  pearu peterson  et al.2001–.scipy: open source scientific tools for python.(2001–).http://www.scipy.org/ [online; accessed 2018-08-17].[26] james c king.1976.symbolic execution and program testing.commun.acm 19   7 (1976)  385–394.[27] jaewoo lee and christopher w clifton.2014.top-k frequent itemsets via differ- entially private fp-trees.in proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining.acm  931–940.[28] min lyu  dong su  and ninghui li.2017.understanding the sparse vector technique for differential privacy.proceedings of the vldb endowment 10  6 (2017)  637–648.[29] frank d.mcsherry.2009.privacy integrated queries: an extensible platform for privacy-preserving data analysis.in proceedings of the 2009 acm sigmod international conference on management of data.19–30.[30] dimiter milushev  wim beck  and dave clarke.2012.noninterference via sym- bolic execution.in formal techniques for distributed systems.springer  152–168.[31] ilya mironov.2017.rényi differential privacy.in 30th ieee computer security  foundations symposium  csf.[32] prashanth mohan  abhradeep thakurta  elaine shi  dawn song  and david culler.2012.gupt: privacy preserving data analysis made easy.in proceedings of the acm sigmod international conference on management of data.[33] suzette person  matthew b dwyer  sebastian elbaum  and corina s pˇasˇareanu.2008.differential symbolic execution.in proceedings of the 16th acm sigsoft international symposium on foundations of software engineering.acm  226–237.[34] silvio ranise and cesare tinelli.2006.the smt-lib standard: version 1.2.technical report.technical report  department of computer science  the university of iowa  2006.available at www.smt-lib.org.[35] jason reed and benjamin c.pierce.2010.distance makes the types grow stronger: a calculus for differential privacy.in proceedings of the 15th acm sigplan international conference on functional programming (icfp ’10).157–168.[36] a.roth.2011.sparse vector technique  lecture notes for “the algorithmic  foundations of data privacy”.(2011).[37] indrajit roy  srinath setty  ann kilzer  vitaly shmatikov  and emmett witchel.2010.airavat: security and privacy for mapreduce.in nsdi.[38] ben stoddard  yan chen  and ashwin machanavajjhala.2014.differentially private algorithms for empirical machine learning.arxiv preprint arxiv:1411.5428 (2014).[39] michael carl tschantz  dilsun kaynar  and anupam datta.2011.formal verifica- tion of differential privacy for interactive systems (extended abstract).electron.notes theor.comput.sci.276 (sept.2011)  61–79.[40] lili xu  konstantinos chatzikokolakis  and huimin lin.2014.metrics for differ-  ential privacy in concurrent systems.199–215.[41] danfeng zhang and daniel kifer.2017.lightdp: towards automating differential privacy proofs.in acm symposium on principles of programming languages (popl).888–901.as approximate differential privacy  zcdp  and renyi-differential privacy.acknowledgments we thank anonymous ccs reviewers for their helpful suggestions.this work was partially funded by nsf awards #1228669  #1702760 and #1566411
29,2019-Doppelgangers on the Dark Web- A Large-scale Assessment on Phishing Hidden Web Services.pdf, anonymous network services on the world wide web have emerged as a new web architecture  called the dark web.the dark web has been notorious for harboring cybercriminals abusing anonymity.at the same time  the dark web has been a last resort for people who seek freedom of the press as well as avoid censorship.this anonymous nature allows website operators to conceal their iden- tity and thereby leads users to have difficulties in determining the authenticity of websites.phishers abuse this perplexing authen- ticity to lure victims; however  only a little is known about the prevalence of phishing attacks on the dark web.we conducted an in-depth measurement study to demystify the prevalent phishing websites on the dark web.we analyzed the text content of 28 928 http tor hidden services hosting 21 million dark webpages and confirmed 901 phishing domains.we also discovered a trend on the dark web in which service providers perceive dark web domains as their service brands.this trend exacerbates the risk of phishing for their service users who remember only a partial tor hidden service address.our work facilitates a better understanding of the phishing risks on the dark web and encourages further research on establishing an authentic and reliable service on the dark web.ccs concepts • security and privacy → phishing; • information systems → data extraction and integration; • networks → network privacy and anonymity.acm reference format: changhoon yoon  kwanwoo kim  yongdae kim  seungwon shin  and sooel son.2019.doppelgängers on the dark web: a large-scale assessment on phishing hidden web services .in proceedings of the 2019 world wide web conference (www ’19)  may 13–17  2019  san francisco  ca  usa.acm  new york  ny  usa  11 pages.https://doi.org/10.1145/3308558.3313551  1, the web is the most popular  worldwide  and accessible platform for sharing and disseminating information across the globe.however  there is not only a bright side of the web  but there is also a dark ∗corresponding author  this paper is published under the creative commons attribution 4.0 international (cc-by 4.0) license.authors reserve their rights to disseminate the work on their personal and corporate web sites with the appropriate attribution.www ’19  may 13–17  ,
30,2019-ML-Leaks- Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models.pdf,—machine learning (ml) has become a core com- ponent of many real-world applications and training data is a key factor that drives current progress.this huge success has led internet companies to deploy machine learning as a service (mlaas).recently  the ﬁrst membership inference attack has shown that extraction of information on the training set is possible in such mlaas settings  which has severe security and privacy implications.however  the early demonstrations of the feasibility of such attacks have many assumptions on the adversary  such as using multiple so-called shadow models  knowledge of the target model structure  and having a dataset from the same distribution as the target model’s training data.we relax all these key assumptions  thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought.we present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains.in addition  we propose the ﬁrst effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ml model.i,  machine learning (ml) has become a core component of many real-world applications  ranging from image classiﬁca- tion to speech recognition.the success of ml has recently driven leading internet companies  such as google and ama- zon  to deploy machine learning as a service (mlaas).under such services  a user uploads her own dataset to a server and the server returns a trained ml model to the user  typically as a black-box api.despite being popular  ml models are vulnerable to vari- ous security and privacy attacks  such as model inversion [1,
31,2019-Model Inversion Attacks Against Collaborative Inference.pdf, the prevalence of deep learning has drawn attention to the privacy protection of sensitive data.various privacy threats have been presented  where an adversary can steal model owners’ private data.meanwhile  countermeasures have also been introduced to achieve privacy-preserving deep learning.however  most studies only focused on data privacy during training  and ignored privacy during inference.in this paper  we devise a new set of attacks to compro- mise the inference data privacy in collaborative deep learning systems.specifically  when a deep neural network and the corresponding inference task are split and distributed to dif- ferent participants  one malicious participant can accurately recover an arbitrary input fed into this system  even if he has no access to other participants’ data or computations  or to prediction apis to query this system.we evaluate our attacks under different settings  models and datasets  to show their effectiveness and generalization.we also study the charac- teristics of deep learning models that make them susceptible to such inference privacy threats.this provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.ccs concepts • security and privacy → systems security; distributed systems security; • computing methodologies → arti- ficial intelligence.keywords deep neural network  model inversion attack  distributed computation  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.acsac ’19  december 9–13  2019  san juan  pr  usa © 2019 association for computing machinery.acm isbn 978-1-4503-7628-0/19/12...$15.00 https://doi.org/10.1145/3359789.3359824  acm reference format: zecheng he  tianwei zhang  and ruby b.lee.2019.model in- version attacks against collaborative inference.in 2019 annual computer security applications conference (acsac ’19)  december 9–13  2019  san juan  pr  usa.acm  new york  ny  usa  15 pages.https://doi.org/10.1145/3359789.3359824  1, deep learning technology has developed rapidly  especially deep neural networks (dnns).deep learning models out- perform traditional machine learning approaches on various artificial intelligence tasks  e.g.image recognition [19]  natu- ral language processing [31]  speech recognition [16]  anom- aly detection [, are consistent with the visual assessments in figure 3.we set the threshold of ssim as 0.3: a recovered image with an ssim value below this threshold (shaded entries in table 3  and other tables in the following sections) is regarded as being unrecognizable.table 3: psnr (db) and ssim for white-box attacks  mnist  psnr 39.69 ssim 0.9969  conv1 relu2 15.10 0.5998  cifar10  conv11 relu22 relu32 37.59 0.9960  13.38 0.1625  19.47 0.6940  5 black-box attacks next  we consider the black-box setting  where the adversary does not have knowledge of the structure or parameters of fθ1.we assume that the adversary can query the black-box model: he can send an arbitrary input x to p1  and observe the corresponding output fθ1(x ).this assumption applies to the case where the model owner releases prediction apis to end users as an inference service.we further relax this assumption in section 6.model inversion attacks under the black-box setting are more challenging  because without the knowledge of model parameters  the adversary cannot directly perform a gradient descent operation on fθ1 to solve the optimization problem in equation 3(c).one solution is to first recover the model structure and parameters by querying the model  and then recover the inference samples.the possibility of model re- construction has been demonstrated in [33  44  46].we prove that the model inversion attacks can be achieved based on the reconstructed model in section 7.3.we propose a more efficient approach  inverse-network  to directly identify the inversed mapping from output to input  without the need to obtain the model information.our solution is easier to implement  and can recover inputs with higher fidelity.we describe this approach and evaluate it in this section.quantitative comparisons between these two solutions are presented in section 7.3.5.1 inverse-network conceptually  the inverse-network is the approximated in- verse function of fθ1  trained with v = fθ1(x ) as input  and x as output.we show the detailed description of inverse- network approach in algorithm 2.the attack consists of three phases: ① generating a training set for the inverse- network; ② training the inverse-network; and ③ recovering the input sample by querying the inverse-network.first  the adversary generates a bag of samples x = (x1  x2  ...xm ) to query the target system  and observes the corre- sponding intermediate outputs v = ( fθ1(x1)  fθ1(x2)  ...fθ1(xm )) (lines 10-17 in algorithm 2).we consider three cases for se- lecting x: (1) the adversary has access to the original dataset used for training fθ   and adopts it as x; (2) the adversary does not have the original training set.instead  he has a dif- ferent set following the same distribution; (3) the adversary has neither the original dataset or its distribution.he has to randomly generate some samples.in our experiment  we generate pure noise sampled from the standard gaussian distribution (zeros mean  unit variance) to form x.next  the adversary can directly train an inverse-network −1 θ1 using v as the training input and x as the training target f (lines 19-29).we initialize the inverse-network with xavier initialization [13].we leverage l2 norm in the pixel space as the loss function (equation 4)  and stochastic gradient de- scent (sgd) to train the inverse-network.it is worth noting that the architecture of the inverse-network is not necessar- ily related to the target model fθ1.in our experiment  we use an entirely different architecture.−1 θ1 = arдminд  f  1 m  ||д( fθ1(xi )) − xi )||2  (4)  m(cid:88)  i =1  ˆx0 = inverse(д  fθ1(x0))  algorithm 2 black-box model inversion attack 1: function blackboxattack(fθ1  fθ1(x0)) 2: /* fθ1: the target model */ 3: /* x0: the target sensitive input to recover */ 4: /* fθ1(x0): the intermediate layer output */ 5: x=generatetrainingset() 6: д=traininversenet(x  fθ1) 7: 8: return ˆx0 9: 10: function generatetrainingset() 11: if known training set then 12: 13: else if known training distribution then 14: 15: else 16: 17: return x 18: 19: function traininversenet(x  fθ1) 20: /* k: batchsize */ 21: /* ϵ: stepsize */ 22: д(0)=init() 23: while (t < t ) do 24: 25:  x=data.trainingset x= newset ∼ data.trainingset x=gaussiannoise  (cid:80)k i =1 ||д( fθ1(xi )) − xi||2  randomly sample x1  x2...xk from x l(д(t )= 1 k д(t +1) = д(t ) − ϵ ∗ ∂l(д (t ) ) ∂д (t ) t+=1  2  26: 27: 28: end while 29: return д(t ) 30: 31: function inverse(д  fθ1(x0)) 32: 33: return ˆx0  ˆx0=д( fθ1(x0))  once the inverse-network f  −1 θ1 is obtained  the adversary can recover any inference sample from the intermediate- −1 level value: x = f θ1 (v).this approach is more efficient than rmle: (1) for each target sample  the adversary only needs to pass through the inverse-network once  while in rmle  an iterative process is required to solve the optimization problem; (2) calculating the inversed input is parameter-free  while rmle requires tuning the parameters (λ  β in eq.3).5.2 evaluation figures 4  5 and 6 show the recovered images of two datasets under three different circumstances.table 4 shows the psnr and ssim metrics of these attacks.from these recovery re- sults  we draw some conclusions.first  the adversary can recover the input with black-box access for most cases.the quality of the recovered images in mnist is very high when the split point is in conv1 or relu2.for cifar10  the recovered images still maintain high quality when the split point is in a shallow layer (conv11).they become relatively vague and lose certain details when the split point is in a deeper layer  e.g.layer relu32 for the cifar10 dataset.second  we observe that there is no significant difference between the cases where the adversary uses the same train- ing set  or a different set with the same distribution to train the inverse-network.for mnist  the attack with a different set are even slightly better than the ones with the same set.however  when the adversary does not know the training data distribution  and adopts randomly generated samples  the attack effects drop significantly.this is especially promi- nent in the case of the cifar10 split in the relu22 layer.we conclude that the knowledge of the training data distribution is important to recover samples from deep layers.(a) mnist.ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.figure 4: recovered inputs in black-box attacks (same training data)  6 query-free attacks the inverse-network approach requires the adversary to be able to query the target model  to generate the data set for training f .in this section  we consider the query-free setting  where the adversary cannot query the system  and does not know the client-side model information.the basic idea is that the adversary first reconstructs a shadow model  which can imitate the target model’s behavior  and then uses  −1 θ  ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.figure 5: recovered inputs in black-box attacks (dif- ferent training data  same distribution).(a) mnist.ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.figure 6: recovered inputs in black-box attacks (dif- ferent distributions).rmle over this shadow model to recover the sensitive input samples.6.1 shadow model reconstruction the problem in our consideration is: how can the adversary reconstruct a shadow model of the former model layers  fθ1   table 4: psnr (db) and ssim for black-box attacks  (a) mnist.dataset same set same dist rand set same set same dist rand set  mnist  conv1 relu2 39.64 20.35 20.81 40.72 7.72 14.76 0.7334 0.9887 0.8046 0.9950 0.7188 0.4310  psnr  ssim  cifar10  conv11 relu22 relu32 49.88 49.02 48.59 0.9993 0.9992 0.9996  19.81 19.36 12.79 0.6939 0.6802 0.2930  15.42 13.95 12.37 0.3124 0.2196 0.0440  only with the knowledge of the latter layers fθ2 and a dataset s drawn from the same distribution as the original training set? he cannot query the model with specified samples to get the intermediate values.the key insight of our approach is that  if the shadow ′ θ1  it should be able to classify model is reconstructed as f the input with high accuracy when combined with the later layers fθ2:  θ1(xi ))  for (xi   yi ) ∈ s ′  yi ∼ fθ2( fθ1(xi )) ∼ fθ2( f  (5) then the task of model reconstruction can be translated into minimizing the classification error of the composition ′ of the two models: fθ2( f θ1(xi )) versus yi.equation 6 shows the loss function for training the model  where m is the number of samples in s  crossentropy is the cross-entropy ′ θ1 is loss.equivalently  this means the training process of f ′ supervised at the output layer of fθ2.once the model f θ1 is reconstructed  the adversary can perform model inversion attacks using the rmle technique in section 4.′ θ1 = arдminд  f  1 m  m(cid:88)  i =1  crossentropy( fθ2(д(xi ))  yi )  (6)  algorithm 3 describes the query-free attacks.there are two phases: ① offline shadow model reconstruction (lines 10-21) and ② online model inversion (line 7).the shadow model reconstruction only needs to be performed once.then all the input samples can be recovered using the same shadow model  by inferencing only once for each input .in the shadow model reconstruction phase  a training set and an initial network are required (lines 13-14).we con- sider four cases with different adversary’s capabilities in two dimensions  i.e.training set and network structure: (1) the adversary’s dataset s is the same as the original set used for training fθ .he also knows the network structure of fθ1; (2) the adversary has a different dataset s from the original training set  but follows the same distribution.this assump- tion is reasonable  because there exist various public datasets for different tasks.he knows the network structure of fθ1; (3) the adversary has the same training set.but he does not know the structure of fθ1.he has to use an alternative one  algorithm 3 query-free model inversion attack 1: function queryfreeattack(fθ1  fθ2  fθ1(x0)) 2: /* fθ1: the target model */ 3: /* fθ2: the known model */ 4: /* x0: the target sensitive input to recover */ 5: /* fθ1(x0) the intermediate layer output */ ˆfθ1=modelreconstruction(s  fθ2) 6: ˆx0 = whiteboxattack( ˆfθ1  fθ1(x0)  t   λ  ϵ) 7: 8: return ˆx0 9: 10: function modelreconstruction(fθ2) 11: /* k: batchsize */ 12: /* ϵ: stepsize */ 13: s=generatetrainingset() 14: д0=initarchitecture() 15: while (t < t ) do 16:  (cid:80)k randomly sample x1  x2...xk and labels y1  y2...yk i =1 yi ( fθ2(дt (xi )) + (1−yi )(1− fθ2(дt (xi )) l(дt )= 1 k д(t +1) = дt − ϵ ∗ ∂l(дt ) ∂дt t+=1  from s  17: 18: 19: 20: end while 21: return д(t )  for the shadow model.we assume that both the target model and the shadow model are convolutional neural networks  but with different numbers of layers and filters  as well as filter sizes.table 5 shows the network structure configura- tions used in our experiments; (4) the adversary does not know the training set nor the network structure.after the training set and network structure are deter- mined  the adversary can adopt sgd to optimize the loss function of the composition of the two models.we choose the cross-entropy loss because it performs well on image classification tasks.other loss functions can be leveraged  if the adversary aims to find inverses of the dnn for different tasks.once the shadow model is obtained  the adversary can use rmle (algorithm 1) to recover the inputs.table 5: neural network configurations for query-free attacks.dataset mnist  cifar10  target model  layer one 5x5 conv layer conv1 two 5x5 conv layers relu2 conv11 one 3x3 64 filters layer relu22 two 3x3 64 filters layers + two 3x3 128 filters layers  shadow model  two 3x3 conv layers four 3x3 conv layers  one 3x3 16 filters layer + one 3x3 64 filters layer one 5x5 filters layer + one 5x5 128 filters layer  (a) mnist.ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.figure 7: recovered input in query-free attacks (same training data  same network structure).6.2 evaluation we illustrate the recovered images under the four adversary’s capability settings in figures 7  8  9 and 10 respectively.the corresponding quantitative results are listed in table 6.for mnist  the adversary can still recover the input im- ages from conv1 and relu2 layers.the quality of the images is relatively lower than the ones in the white-box or black- box setting.for cifar10  attacks are successful only from the shallow conv11 layer with the knowledge of training set or network structure.these results indicate that query-free attacks are harder to achieve than white-box or black-box attacks.this is straightforward  as the adversary now has smaller capabilities.besides  more layers on the trusted par- ticipant can also increase the difficulty of image recovery.we also observe that a different training set with the same distribution has similar effects on model inversion attacks.so the adversary does not need to know the exact training set for attacks.this is also observed in the black-box setting (section 5).however  if the adversary has no knowledge of the network structure  then an alternative network has worse performance.this emphasizes the importance of knowledge of network structure for a model inversion attack.7 discussions in this section  we review  summarize and compare the attack results under different settings.we explore the impacts of system features and adversary’s capabilities on the model inversion attacks.we also discuss possible defense solutions.(a) mnist.ref  conv1  relu2  ref  conv11  relu22  relu32  (a) mnist.ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.(b) cifar10.figure 8: recovered input in query-free attacks (differ- ent training data  same network structure).figure 10: recovered input in query-free attacks (dif- ferent training data  different structures.)  table 6: psnr (db) and ssim for query-free attacks  (a) mnist.psnr  ssim  dataset  net structure  same set  same net diff sets  same net same sets  diff nets diff sets  diff nets same set  same net diff sets  same net same sets  diff nets diff sets  diff nets  mnist  conv1 relu2 17.60 21.53 12.59 17.86 0.7423 0.9121 0.6430 0.6952  9.61 9.27 8.05 8.03 0.4981 0.4652 0.3790 0.3226  cifar10  conv11 relu22 relu32 21.16 21.45 17.55 13.06 0.9104 0.9145 0.6344 0.1553  12.74 11.51 12.46 11.30 0.1752 0.1723 0.2714 0.0467  11.09 11.98 10.68 11.47 0.0419 0.0102 0.0247 0.0793  ref  conv1  relu2  ref  conv11  relu22  relu32  (b) cifar10.figure 9: recovered input in query-free attacks (same training data  different structures.)  7.1 impact of system configurations from the results in previous sections  we observe that differ- ent split points can yield different attack effects.this raises an important question: how to split the neural network in the collaborative system  to make the inference data more secure?  we use the query-free attack over the lenet model (mnist dataset) as an example to explore this question.we select the split point at each layer  and perform model inversion attacks.figures 11 and 12 show the recovered images  and psnr/ssim metrics respectively.generally  we observe that the quality of recovered images decreases when the split layer goes deeper.this is straightfor- ward as the relationship between input and output becomes more complicated and harder to revert when there are more layers.besides  we also observe that the image quality drops significantly  both qualitatively (figure 11) and quantitatively (figure 12)  on the fully-connected layer (fc1)  indicating that model inversion with fully-connected layers is much harder than for convolutional layers.the reason is that a convo- lutional layer only operates on local elements (the locality depends on the kernel size)  while a fully-connected layer en- tirely mixes up the patterns from the previous layer.besides  the number of output neurons in a fully-connected layer is  typically much smaller than input neurons.so it is relatively harder to find the reversed relationship from the output of the fully-connected layer to the input.unfortunately  privacy is usually not considered when selecting the optimal split point in a collaborative system.in the case of an edge-cloud scenario  most layers (including all fully-connected layers) are commonly offloaded to the cloud  while the edge device only computes a small number of convolutional layers for feature extraction  due to power and resource constraints [25].this gives a chance for an untrusted cloud provider to steal sensitive inference input.takeaway: when selecting the split point in a collabora- tive inference system  privacy should also be considered  in addition to latency and power constraints.we recommend placing at least one fully-connected layer on the trusted par- ticipant to hide the information of sensitive input samples.ref  conv1  relu1  pool1  conv2  relu2  pool2  fc1  fc1act  fc2  fc2act  figure 11: recovered images in query-free attacks  7.2 impact of adversary’s capabilities in addition to the selection of split point  the adversary’s capability can also have an impact on the attack results.the question we consider is: which capabilities are critical for model inversion attacks? knowledge of target model.if the adversary can query the system  then it is not necessary for him to know the parameters or network structure on the trusted participant.comparing tables 3 and 4  we find that the effects of black- box attacks using our inverse-network technique are as good as the white-box attacks using rmle technique.however  if the adversary does not have access to the model query  figure 12: psnr and ssim in query-free attacks.apis and model parameters (the query-free setting)  then the knowledge of network structure plays a relatively important role in recovering inputs  as discussed in section 6.2 and table 6.knowledge of the training set.the adversary does not need to know the exact training set.using a different set following the same distribution  the adversary can recover the input images with similar quality in the black-box set- ting (table 4)  or the query-free setting (table 6).however  the knowledge of training data distribution is very critical: without such information  the adversary has to use randomly generated samples to reverse the network in the black-box attacks  whose performance drops significantly (table 4).in the query-free case  the adversary cannot reconstruct the model without knowing the training data distribution.capability of model query.this is also a critical require- ment for model inversion attacks.if the adversary is not able to query the model in a black-box setting  he has to reconstruct the model before recovering the input.it takes more effort to implement the attacks  and the performance is lower (comparing tables 4 and 6).takeaway: we recommend the model owner trains the tar- get model using a training set whose distribution is unknown to the adversary.restricting the query apis from untrusted participants can also make the attacks harder.7.3 comparisons of attack techniques we propose three different attack techniques under different threat models.we summarize and compare these techniques.table 7: applicability of techniques under different settings.rmle  inverse shadow network model  ✓ – –  ✓ ✓ –  ✓ ✓ ✓  white-box black-box query-free  00.10.20.30.40.50.60.70.80.910510152025psnrssimapplicability.table 7 lists the effectiveness of each tech- nique under different settings.the white-box scenario is the most basic setting: since the adversary knows all the details about the target model  other techniques without such an assumption can also be applied here  although some of them may not be efficient (e.g.reconstructing the shadow model).for the black-box setting  since the adversary does not know the model parameters  he cannot use the rmle technique.he can either adopt the inverse-network approach  or recon- struct the shadow model and then use rmle to recover the input.for the query-free scenario  since the adversary does not know the model parameters  and has no access to query the model  he can only use the shadow model reconstruction with rmle to recover the image.table 8: comparison of inverse-network and shadow model reconstruction in the black-box setting.technique  inverse-network  model reconstruction  inverse-network  model reconstruction  mnist  conv1 relu2 20.35 39.64 15.41 39.67 0.9887 0.7334 0.6103 0.9968  psnr  ssim  conv11 relu22 relu32 49.87 28.67 0.9993 0.9766  15.41 12.61 0.3124 0.1145  cifar10  19.81 18.02 0.6939 0.6893  performance.we first compare the attack performance of the rmle (table 3) and inverse-network 1 (table 4) ap- proaches in the white-box setting  respectively.we observe that when the adversary knows either the training data or its distribution  the recovered images from inverse-network maintain higher quality than the ones from rmle.otherwise  rmle performs better than inverse-network with randomly generated samples.we then compare the performance of inverse-network and shadow model reconstruction solutions in the black- box setting.as introduced in section 5  the adversary can query the model to get pairs of input and corresponding intermediate values  based on which he can reconstruct a shadow model.we implement this approach and show the quantitative comparisons with inverse-network in table 8.we find that inverse-network has better results than shadow model reconstruction for most datasets and split points.takeaway: for the white-box setting  if the adversary has no knowledge of the training set or distribution  he can use rmle for better performance.otherwise  he can select inverse-network  as it has better results  and takes less effort to implement and perform.for the black-box setting  inverse- network is recommended over shadow model reconstruction.for the query-free setting  shadow model reconstruction is the only applicable method.1inverse-network gives the same results for both white-box and black-  box settings.7.4 defenses since current privacy-preserving algorithms and systems all focus on training data  we provide some possible defense strategies to mitigate the inference privacy attacks discussed in this paper.running fully-connected layers before sending out re- sults.as shown in section 7.1  a fully-connected layer can mix up inputs  and hide information about the inference sam- ples.so a model owner can deploy at least one such layer on the first trusted participant.this makes it very difficult for the adversary to recover the input.typically  the fully- connected layers follow convolutional layers in a dnn.this requires computing all convolutional layers on the client- side  which can be heavy for an edge device.make the client-side network deeper.as illustrated in figures 11 and 12  both qualitative and quantitative measure- ments of the inversed images become worse as the network becomes deeper.therefore  making the client-side network deeper can help mitigate the attacks.on the other hand  deeper networks increase the compution on the client-side.the client device or iot device may not have sufficient com- putation  storage or battery resources for this  nor for the above mitigation strategy.trusted execution on untrusted participants.the hard- ware support for a trusted execution environment (tee)  e.g.intel sgx  arm trustzone  is effective at secure remote computation and data confidentiality protection against priv- ileged adversaries.in the case of collaborative dnn compu- tation  the inference application can be deployed inside a tee (or secure enclave) on the untrusted participants  and the intermediate values are encrypted against the adversary during transmission between participants.this can provide privacy protection for the inference data.however  this re- quires special architecture support on the cloud side  and careful crypto key management.differential privacy.we can use differential privacy to add noise and obfuscate sensitive information.specifically  we can add noise to the inference input  and the intermediate value becomes v = fθ1(x + ϵ ).we can also add noise directly to the intermediate value before sending it to the untrusted participant: v = fθ1(x ) +ϵ.in these two cases ϵ is the random noise that satisfies differential privacy.it is obvious that there exists a trade-off between usability and privacy: as a higher level of noise is added  the model accuracy may drop.homomorphic encryption.this allows the inference ap- plication on the untrusted participant to directly perform dnn computations on encrypted input  so the sensitive in- formation will not be leaked.a drawback of homomorphic encryption is that it suffers from huge inefficiency and is not applicable for all dnn operations.8 related work 8.1 machine learning privacy attacks training data privacy attacks.there are different types of privacy attacks against the training data.the first type is property inference attacks  which tries to infer some proper- ties of the training data from the model parameters.attacks were demonstrated in traditional machine learning classifiers [3] and fully-connected neural networks [12].a special case of property inference attacks is membership inference attacks  which infers whether one individual sample is included in the training set.this attack was first presented in [41].the following work explored the feasibility of attacks with different adversary’s capabilities [39]  model features [30  49]  in generative adversarial networks [18  29]  and collaborative training systems [32].the second type are model inversion attacks [10]: given a machine learning model  and part of the training samples’ fea- tures  the adversary can recover the rest of the features of the samples.advanced model inversion attacks were designed to recover images from deep neural networks in single-party systems [9]  and collaborative learning systems [22].the third type are model encoding attacks [42]: the adver- sary with direct access to the training data can encode the sensitive data into the model for a receiver entity to retrieve.model privacy attacks.the adversary attempts to steal the model parameters [44]  hyperparameters [46] or structures [23  33]  via prediction apis  memory access side channels  etc.inference data privacy attacks.closer to our study is the work [48]  which adopted a power side channel to recover inference data.however  this attack required the adversary to compromise the victim device for side-channel information collection  and it could only recover simple images (single pixel).our attack is applied to the collaborative systems  and can recover any arbitrary complex data without access or knowledge of the victim’s computation and data.8.2 machine learning privacy solutions current solutions only focus on training data protection: enhancing the algorithms.distributed training was in- troduced to protect the training data [15  40]  as different participants can use their own data for model training.the sgx security enclaves in intel processors were used to pro- tect the training tasks against privileged adversaries [24  34].cao et al.[5] proposed a methodology to remove the effects of sensitive training samples on the models.abadi et al.[2] applied differential privacy to add noise in the stochastic gradient descent process to eliminate the parameters’ depen- dency on the training data.enhancing the dataset.bost et al.[4] proposed to encrypt the data before feeding them into the training algorithm.they designed machine learning operators which can op- erate on the encrypted data.zhang et al.[51] showed that adding noise to the training dataset is effective in protect- ing training data privacy.generative adversarial network with differential-privacy is adopted [45  52] to generate arti- ficial data for training dnn models while removing sensitive information from the original data.9 conclusions while the privacy threat of training data in deep learning is well studied  and defenses have been investigated  the pri- vacy of inference data is less studied.in this paper  we explore the feasibility of recovering sensitive data in the deep learn- ing inference process.we discover that in a collaborative inference system  an adversary who controls one participant can easily recover the inference samples from intermedi- ate values.we propose a new set of attack techniques to compromise the inference data privacy in collaborative deep learning systems  under different attack settings.we system- atically compare these different techniques  demonstrating that the adversary can successfully and reliably recover the inputs with very few prerequisites.we hope that the importance of inference data privacy protection can be addressed through this study.for instance  when selecting the split point for edge-cloud offloading  pre- vious work only considered the power and performance requirements.with the feasibility of stealing the inference data  privacy should also be an important factor for partition- ing the neural network.future work could include the study of the trade-off among power  performance and security for edge-cloud offloading  exploration of more powerful attacks  and realization of possible defense mechanisms
32,2019-Opening the Blackbox of VirusTotal- Analyzing Online Phishing Scan Engines.pdf, online scan engines such as virustotal are heavily used by re- searchers to label malicious urls and files.unfortunately  it is not well understood how the labels are generated and how reliable the scanning results are.in this paper  we focus on virustotal and its 68 third-party vendors to examine their labeling process on phishing urls.we perform a series of measurements by setting up our own phishing websites (mimicking paypal and irs) and submitting the urls for scanning.by analyzing the incoming network traffic and the dynamic label changes at virustotal  we reveal new insights into how virustotal works and the quality of their labels.among other things  we show that vendors have trouble flagging all phish- ing sites  and even the best vendors missed 30% of our phishing sites.in addition  the scanning results are not immediately updated to virustotal after the scanning  and there are inconsistent results between virustotal scan and some vendors’ own scanners.our results reveal the need for developing more rigorous methodologies to assess and make use of the labels obtained from virustotal.ccs concepts • security and privacy → web application security.acm reference format: peng peng*  limin yang‡  linhai song†  gang wang‡.2019.opening the blackbox of virustotal: analyzing online phishing scan engines.in in- ternet measurement conference (imc ’19)  october 21–23  2019  amsterdam  netherlands.acm  new york  ny  usa  8 pages.https://doi.org/10.1145/ 3355369.3355585  1, online scan engines  designed to scan malware files and malicious websites  are critical tools for detecting new threats [3  4  7  8].virustotal is one of the most popular scanning services that are widely used by researchers and industry practitioners [8].virus- total provides both file scan (for malware analysis) and url scan services (for detecting phishing and malware hosts).it works with more than 60 security vendors to aggregate their scanning results.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.imc ’19  october ,
33,2019-Phishfarm.pdf,—phishing attacks have reached record volumes in recent years.simultaneously  modern phishing websites are grow- ing in sophistication by employing diverse cloaking techniques to avoid detection by security infrastructure.in this paper  we present phishfarm: a scalable framework for methodically testing the resilience of anti-phishing entities and browser blacklists to attackers’ evasion efforts.we use phishfarm to deploy 2 380 live phishing sites (on new  unique  and previously-unseen .com domains) each using one of six different http request ﬁlters based on real phishing kits.we reported subsets of these sites to 10 distinct anti-phishing entities and measured both the occurrence and timeliness of native blacklisting in major web browsers to gauge the effectiveness of protection ultimately extended to victim users and organizations.our experiments revealed shortcomings in current infrastructure  which allows some phishing sites to go unnoticed by the security community while remaining accessible to victims.we found that simple cloaking techniques representative of real-world attacks— in- cluding those based on geolocation  device type  or javascript— were effective in reducing the likelihood of blacklisting by over 55% on average.we also discovered that blacklisting did not function as intended in popular mobile browsers (chrome  safari  and firefox)  which left users of these browsers particularly vulnerable to phishing attacks.following disclosure of our ﬁndings  anti-phishing entities are now better able to detect and mitigate several cloaking techniques (including those that target mobile users)  and blacklisting has also become more consistent between desktop and mobile platforms— but work remains to be done by anti-phishing entities to ensure users are adequately protected.our phishfarm framework is designed for continuous monitoring of the ecosystem and can be extended to test future state-of-the-art evasion techniques used by malicious websites.i,  phishing has maintained record-shattering levels of volume in recent years [1] and continues to be a major threat to today’s internet users.in , about the conditions under which cloaking is effective  as we found that each anti-phishing entity exhib- ited distinctive blacklisting of different cloaking techniques alongside varying overall speed.for example  the mobile-only filter b showed 100% effectiveness against blacklisting across all entities.on the other hand  the javascript-based filter f was 100% effective for some entities  delayed blacklisting for others  but was in fact more likely to be blacklisted than filter a by others still.in many cases  there was also a lack of blacklisting of filter a sites (i.e.those without cloaking).entities— in particular  the clearinghouses— at times failed to ultimately blacklist a reported site despite extensive crawling activity.although it is possible that our direct reporting methodology led to some sites being ignored altogether  the exceptional performance of gsb with respect to filter a shows that a very high standard is realistic.we detail each entity’s behavior in section v-f.to provide meaningful measurements of entity performance with respect to different browsers and cloaking ﬁlters  we propose a scoring system in section v-b which seeks to capture both the response time and number of sites blacklisted by each anti-phishing entity for each browser and/or ﬁlter.in addition  we summarize our ﬁndings visually in figures 2 and 3 by plotting the cumulative percentage of sites blacklisted over time  segmented by each browser or by each cloaking technique  respectively.lastly  we analyze web trafﬁc logs to make observations about the distinctive behavior of each anti- phishing entity.although much of our analysis focuses on the results of the large-scale full tests  we also make comparisons to the preliminary test data when appropriate.a.crawling behavior  of all sites that we launched during the preliminary and full tests  most (81.9%) saw requests from a web crawler  and a majority of sites therein (66.0%) was successfully retrieved at least once (i.e.bypassing cloaking if in use).in a handful of cases  our reports were ignored by the entities and thus resulted in no crawling activity  possibly due to volume or similarity  table vi: aggregate entity blacklisting performance scores in the full tests (colors denote subjective assessment: green— good  yellow— lacking  red— negligible blacklisting)  gsb  sbf  gsb ie edge opera  p bf  t bf (min.)  smartscreen gsb ie edge opera  sbf  p bf t bf  apwg  sbf  gsb ie edge opera  p bf t bf  phishtank gsb ie edge opera  sbf  p bf t bf  paypal  sbf  gsb ie edge opera  p bf t bf  filter a 0.942 0 0 0 0.970 112  filter a 0.005 0.176 0.183 0 0.212 548  filter a 0.563 0.113 0.129 0.242 0.576 194  filter a 0.077 0.096 0.085 0.074 0.106 386  filter a 0.133 0.102 0.123 0.119 0.167 675  filter b 0 0 0 0 0 n/a  filter b 0 0 0 0 0 n/a  filter b 0 0 0 0 0 n/a  filter b 0 0 0 0 0 n/a  filter b 0 0 0 0 0 n/a  filter c 0.030 0 0 0 0.031 50  filter c 0.005 0.003 0.003 0.005 0.016 2889  filter c 0.356 0 0 0.185 0.344 243  filter c 0 0 0 0 0 n/a  filter c 0.149 0.040 0.056 0.029 0.172 440  filter d 0.899 0 0 0 0.953 100  filter d 0 0 0 0 0 n/a  filter d 0 0 0 0 0 n/a  filter d 0 0 0 0 0 n/a  filter d 0.052 0 0.046 0 0.078 1331  filter e 0.104 0 0 0 0.106 81  filter e 0 0.301 0.329 0 0.364 391  filter e 0.777 0.626 0.761 0.545 0.803 125  filter e 0 0 0 0 0 n/a  filter e 0.198 0.074 0.193 0.191 0.288 1077  filter f 0.692 0 0 0 0.712 107  filter f 0.009 0.411 0.421 0 0.455 298  filter f 0 0 0 0 0 n/a  filter f 0.026 0 0 0.024 0.136 2827  filter f 0.167 0.137 0.163 0.120 0.182 338  sb 0.533 0 0 0 0.457 0.947  sb 0.004 0.296 0.311 0 0.038 0.649  sb 0.339 0.246 0.297 0.262 0.328 1  sb 0.024 0.026 0.028 0.032 0.025 0.467  sb 0.104 0.140 0.160 0.143 0.138 0.995  s c  s c  s c  s c  s c  to previous reports; we mitigated this risk through the large sample size and discuss it in more detail in section v-g.the distribution of crawler hits was skewed left  characterized by a small number of early requests from the entity to which we reported  followed by a large stream of trafﬁc from it as well as other entities.importantly  different cloaking techniques showed no signiﬁcant effect on the time of the ﬁrst crawling attempt; the median response time ranged from 50 to 53 minutes  from the time of reporting  for all ﬁlter types.during our full experiments  only sites which were crawled were ultimately blacklisted.generally  the crawling also had to result in successful retrieval of the phishing content for blacklisting to occur  though in 10 cases (all in the gsb experiment with filter d)  a site would be blacklisted despite a failed retrieval attempt; possible factors for this positive phishing classiﬁcation are described in prior work [20].b.entity scoring  for each entity tested  let u be the set of phishing urls reported to the entity  let b be the set of browsers monitored  let t be the set of observed blacklisting times  let f be the set of cloaking ﬁlters used  and let m s denote the worldwide browser market share.additionally  we deﬁne the value of the function accessible(b  f) to be true if and only if a phishing site with ﬁlter f is designed to be accessible in browser b.for each url-browser combination in b  u  per formula 1  we deﬁne a normalized performance score su rlb on the range r0  1s  where 0 represents no blacklisting and 1 repre- sents immediate blacklisting relative to the time reported.the score decays linearly over our 72-hour observation window  ś  table vii: formulas for aggregate scores (per test).ą  u  su rlb “  ´ tu rl reported  blacklisted in b  otherwise su rlb  ÿ  ÿ  #  @ b  u rl p b 1 ´ tu rl blacklistedb ą 0  twindow  @ b  f p b  f  sbf “ @ b p b  sb “ ÿ  u rl  fu rl“f  n  sbf n  f  accessiblepb  fq  s “  m sb m sb  ¨ sb  b  (1)  (2)  (3)  (4)  (e.g.a site blacklisted after 36 hours would have su rlb “ 0.5).we take the average of all these url-browser scores for each browser-ﬁlter combination  as sbf   per formula 2.we further aggregate the sbf scores to meaningfully sum- marize the protection of each browser by each entity: the browser score sb  as per formula 3  is the average of all sbf for a given browser b  but limited to ﬁlters f accessible in b.to gauge the blacklisting of each cloaking ﬁlter  we report p bf as the raw proportion of sites blacklisted in at least one browser for each respective ﬁlter.note that p bf will always be greater than or equal to any sbf because the former is not reduced by poor timeliness; we thus additionally report t bf   the mean time to blacklisting for each ﬁlter f (in minutes).to capture the overall real-world performance of each entity  we average all sb  weighted by the market share of each browser  to produce s  as per formula 4.the scores s allow us to efﬁciently compare the performance between entities and would be useful in modeling long-term trends in future deployments of phishfarm.we also include and the proportion of all sites crawled  c  to illustrate the entity’s response effort.we present the aforementioned aggregate scores in table vi for all entities in the full tests.indeed  the large number of 0 or near-0 scores was disappointing and representative of multiple ecosystem weaknesses which we discuss in the following sections.scores for the preliminary tests are found in table viii in appendix ii.note that because chrome  firefox  and safari showed nearly identical scores across all experiments  we simplify the table to report the highest respective score under the gsb heading.we make a similar simpliﬁcation for ie 11 on windows 10 and 8.1.we do not separately report  the performance of mo- bile browsers because we observed the behavior of mobile browsers to be directly related to their desktop counterparts.during the preliminary tests  mobile firefox and opera mir- rored the blacklisting— and thus also the scores— of their desktop versions.mobile chrome and mobile safari showed no blacklist warnings whatsoever for any of our phishing sites and thus receive sb scores of 0.during the full tests  the behavior of mobile chrome  safari  and opera remained unchanged.firefox stopped showing blacklist warnings  and its scores thus dropped to 0 (the 0 scores of mobile browsers represented a serious issue; this was corrected after we con- tacted google and mozilla after the full tests  as discussed in section vi-a1).we did not test mobile versions of microsoft  fig.2: blacklisting over time in each browser (full tests).fig.3: effect of cloaking on blacklisting over time (full tests).browsers because mobile ie is no longer supported; edge for android and ios was released after we began testing.c.preliminary vs.full tests  we observed many core similarities when comparing the performance same entity between the preliminary and full tests.we also saw improvements related to the recommenda- tions we shared during our disclosure meetings  in particular with respect to the apwg’s treatment of filters c and e.notably  during the full tests  crawler trafﬁc to sites with cloaking increased by 44.7% relative to sites without cloaking  while the overall trafﬁc volume also increased by 89.7%.we discuss all entity-speciﬁc improvements in section v-f.the comparison also revealed some surprises  however.the main experimental difference between the two sets of tests  apart from sample size  was our exclusive use of random urls in the full tests.on the other hand  the preliminary tests in- cluded a sampling of deceptive urls.in the preliminary tests  we observed that edge and ie were quick to display blacklist warnings for sites with certain deceptive urls.in fact  many type iv urls (with domain names containing either the paypal brand or deceptive keywords) saw proactive zero-hour warnings in these browsers without any prior crawler activity.figure 5b in appendix ii shows the effect of url type on blacklisting in the preliminary tests.during the full tests  no phishing site was blacklisted unless it had previously been visited by a crawler.in the absence of deceptive urls  we thus observed all the blacklists to be purely reactive; this  in turn  led to lower sb scores of edge  ie  and sites with filter b  and a lower overall score s for smartscreen.d.browser performance  figure 2 shows the percentage of our phishing sites black- listed over the course of all the full experiments  grouped by browser  but limited to sites intended to be accessible in each respective desktop browser (i.e.filter b was excluded for all and filters c and d were excluded for ie  edge  and opera).chrome  safari  and firefox consistently exhibited the high- est overall blacklisting speed and coverage  but were still far from covering all cloaked sites.opera generally outperformed the microsoft browsers during the ﬁrst four hours  but was later overtaken for the remainder of our experimental period.in the  absence of deceptive phishing urls  edge and ie simply lost their edge in the full tests; figure 5c in appendix ii shows their superior performance in the preliminary tests.1) consistency between browsers: chrome  firefox  and safari are all protected by the gsb blacklist; our tests conﬁrmed these browsers do consistently display blacklist warnings for the same set of websites.however  during our full tests  chrome displayed warnings up to 30 minutes earlier than safari  and up to one hour earlier than firefox; the warnings became consistent after the ﬁve-hour mark.upon further investigation  we believe that this disparity was caused by different caching implementations of the gsb update api (v4) in each browser (this api ensures privacy and quick lookups but requires periodic refreshes of the entire blacklist [53]).latency is a notable shortcoming of blacklists [6] and it appears that browser blacklist caching can still be improved from a security perspective.edge and ie 11 (both protected by smartscreen) proved far less consistent.in the full tests  edge displayed warnings up to two hours earlier and covered more sites than ie until the 24-hour mark.however  in the preliminary tests— which involved deceptive urls detectable by heuristics— ie would often display preemptive warnings for sites which would not be blocked in edge for several hours  if at all.these deviations were evident in the preliminary apwg  smartscreen  and paypal tests as per table viii in appendix ii.e.filter performance  figure 3 shows the percentage of our phishing sites black- listed over the course of all the full experiments  grouped by cloaking ﬁlter type.because this summary view masks some of the distinctive per-entity behavior we observed in table vi  figure 4 in appendix i should be consulted as a supplement.note that because these ﬁgures consider all browser-ﬁlter combinations  their y-axes differ from figure 2 and the sbf scores in table vi.nevertheless  they all convey the ﬁnding that there exist considerable gaps in today’s blacklists.the overall earliest blacklisting we observed occurred after approximately 40 minutes  in chrome.signiﬁcant growth took place between 1.5 and 6.5 hours and continued at a slower rate thereafter for the full 72-hour period.compared to sites without cloaking  our cloaking techniques showed moderate  to high effectiveness throughout the life of each phishing site.filter b saw no blacklisting whatsoever across any desktop or mobile browser we tested.filters e and f proved most effective in the early hours of deployment  while the geo- speciﬁc filters c and d saw the lowest amount of blacklisting in the long term.between 48 and 72 hours after deployment  sites with filter e overtook filter a sites by a small margin; upon further analysis  we found that this was due to a high level of interest in such sites following reporting to the apwg.all other types of sites with cloaking were on average less likely to be blacklisted than sites without.f.entity performance  although no single entity was able to overcome all of the cloaking techniques on its own  collectively the entities would be successful in doing so  with the exception of filter b (this has since been corrected as per the discussion in section vi-a1).1) google safe browsing: due to the high market share of the browsers it protects  gsb is the most impactful anti- phishing blacklist today.it commanded the highest score s in both the preliminary and full tests.gsb’s key strength lies in its speed and coverage: we observed that a crawler would normally visit one of our sites just seconds after we reported it.94.7% of all the sites we reported in the full tests were in fact crawled  and 97% of sites without cloaking (filter a) ended up being blacklisted.blacklisting of filter d was comparable  and filter f improved over the preliminary tests.however  gsb struggled with filter e  which blocked hostnames speciﬁc to google crawlers.it also struggled with filter c  which denied non-us trafﬁc.the reason the re- spective p bf scores are low is that if the initial crawler hit on the phishing site failed  gsb would abandon further crawling attempts; the initial hit almost always originated from a predictable non-us ip address.another weakness appears to be data sharing  as none of the sites we reported to gsb ended up being blacklisted in edge  ie  or opera.2) microsoft smartscreen: smartscreen proved to be the only native anti-phishing blacklist to leverage proactive url heuristics to blacklist phishing sites  which allowed microsoft browsers to achieve high scores during the preliminary tests.these heuristics were mainly triggered by urls with a deceptive domain name.in the preliminary tests  edge proved to be exceptionally well-protected  achieving a top sb score of 0.87— the highest of any browser.in the full tests  the performance of ie improved over the preliminary tests and became more consistent with that of edge.surprisingly  smartscreen was more likely to blacklist sites with cloaking than those without  possibly due to use of classiﬁcation of cloaking (which would be commendable) alongside low trust of our phishing reports (see limitations).reporting to smartscreen did not seem to signiﬁcantly affect any non-microsoft browsers; the entity thus shares a similar shortcoming with gsb.smartscreen was also among the slowest entities to reactively respond to phishing reports  and its overall coverage was poor  which is its key weaknesses.3) apwg: the apwg was the second-highest scoring entity in our full tests and showed consistent protection of all browsers.its score s increased substantially compared to the preliminary tests due to improvements to bypass filters c and e  which allowed apwg reports to result in blacklisting of such sites in gsb browsers— something not achieved when we reported directly to gsb.the apwg also generated the highest level of crawler trafﬁc of any entity we tested.unfortunately  the apwg failed to blacklist any sites with filter d or f in the full tests; its preliminary success proved to have been related solely to the detection of deceptive urls by ie and edge.interestingly  we saw a large increase in the blacklisting of sites with filter e after the 24-hour mark; after analyzing the trafﬁc logs we believe that this is due to data sharing with paypal (this trend is also reﬂected in figure 4e).4) phishtank: phishtank is a community-driven clearing- house that allows human volunteers to identify phishing con- tent [54]; it also leverages a network of crawlers and partners to aid in this effort.it was the second-highest performer in our preliminary tests thanks to its expeditious blacklisting in gsb browsers.in the full tests  we were surprised to see that only 46.7% of sites reported were crawled  and very few sites were ultimately blacklisted.despite this  phishtank generated the second-highest volume of total crawler trafﬁc.we do not know the reasons for its shortcomings and phishtank did not reply to our disclosure; we suspect that the manual nature of classiﬁcation by phishtank may be a limiting factor.5) paypal: during the preliminary tests  paypal’s own abuse reporting service struggled to bypass filters d and f  but overcame the latter in the full experiments while maintaining a moderate degree of blacklisting.its protection of opera also improved between the two tests.despite crawling all but two of the sites we reported in the full tests  the average response time and browser protection ended up being poor overall.we cannot disclose the reasons for this but expect to see a future improvement as a result of our ﬁndings.6) remaining entities: performance scores for entities only included in the preliminary tests are found in table viii within appendix ii.high-level descriptions of each follow.all sites we reported to eset ended up being crawled  but only a fraction of those– all with filter a– were actually black- listed.overall timeliness was poor  though the blacklisting did span multiple browsers.netcraft yielded the best protection of opera in the preliminary tests  but overall it struggled with most of our cloaking techniques and did not deliver timely blacklisting.in retrospect  given the unexpectedly poor performance of phishtank in the full tests  we would have been interested in re-testing netcraft.reports to the us cert led to minimal crawler activity and blacklisting; disregarding heuristics from the microsoft browsers  only a single site was blacklisted.phishing reports we sent to mcafee effectively bypassed filter a and filter e but only appeared to lead to timely blacklisting in microsoft edge.reports to websense had no effect beyond crawler trafﬁc related to the url heuristics used by microsoft; while we were hopeful the e- mail reporting channel we used would prove fruitful  this is  understandable given that the company focuses on protection of its enterprise customers.g.limitations  our ﬁndings are based on a controlled (to the extent possible without sending out real spam e-mails) empirical experiment and observations from a large set of supporting metadata and a high volume of anti-abuse crawler trafﬁc.our study focuses exclusively on native phishing blacklist protection that is available by default in the browsers and platforms tested.systems with third-party security software may see enhanced protection [6]  though cloaking can also have an impact on systems powering such software.within its scope  our analysis should still be considered with certain limitations in mind.we suspect that real-world blacklisting of phishing attacks may be more timely than what our results otherwise suggest  as our efforts to isolate cloaking as a variable in our experimental design (i.e.by using randomized domain names  never rendering the actual phishing content in browsers being monitored  and using .com domains months after registration) also eliminated many of the methods that the ecosystem can use to detect or classify phishing (e.g.url-  network-  or dns-based analysis).how- ever  this reduced detectability is offset  to an extent  by the possibility of malicious actors to likewise evade such forms of detection.we observed in the preliminary tests that only urls containing brand names were quicker to be blacklisted than others; in the wild  there is also a shifting tendency to abuse compromised infrastructure and distribute random phishing urls in lieu of more deceptive alternatives [8]  [49].in terms of factors under our control  it was not ﬁnancially feasible to achieve a one-to-one mapping between ip addresses and all of our domains; this is a skewing factor which may have acted in favor of blacklists in the full tests  such as with the 10 filter d sites which were blacklisted despite not being successfully retrieved during the gsb experiment.finally  we only submitted a single and direct report for each phishing site deployed.although real-world phishing sites might see a much higher volume of automated reports (e.g.from sources such as spam ﬁlters)  the volume of per-url phishing reports in the wild can in fact be reduced by attackers (e.g.through the use of redirection links).more importantly  direct reports such as ours (in particular to the blacklist operators) might be subject to more suspicion because anti- phishing entities must account for adversaries who willingly submit false reports or seek to proﬁle crawling infrastructure.although the blacklist operators to which we disclosed did not express concern with our reporting methodology  we learned that crawling infrastructure used to respond to direct reports is indeed designed to be unpredictable to mitigate adversarial submissions.smartscreen’s lower crawl rate may be explained by this; gsb  on the other hand  consistently responded quickly and seemed to give our direct reports a high level of priority.it is therefore possible that either classiﬁcation of reporting channel abuse works very accurately  or that report- ing channels are more vulnerable to adversarial submissions  than what the entities otherwise believe; regardless  to improve the future effectiveness of reporting  we propose an alternative approach in section vi-b2.ultimately  if each report represents a chance that a phishing site will be blacklisted  we believe that our experimental design still captures trends therein; moreover  our ﬁndings with respect to cloaking effectiveness are consistent with internal paypal e-mail and web trafﬁc data pertaining to actual victims of phishing.to address its current limitations  our framework could be adapted to follow a different (possibly collaboratively arranged) reporting methodology  consider a broader range of cloaking techniques  or even be applied to proactively-detected live phishing urls for which cloaking can be proﬁled.1) statistical tests: in the full tests  we were surprised to ﬁnd the blacklisting performance of each entity with respect to the different ﬁlters to have far more clear-cut gaps than in the preliminary tests; 11 of the 30 the per-entity ﬁlter groups saw no blacklisting whatsoever (per table vi).although anova as originally planned could still be meaningfully applied to the subset of entities which had three or more ﬁlter groups that satisﬁed homogeneity of variance [50] (i.e.had some blacklisting activity)  we chose not to perform such tests as the resulting power would be below our target  and instead relied on direct observations supported by crawler metadata.if our experiment were to be repeated to validate improvements in blacklisting or continuously test the ecosystem  we believe that statistical tests could be highly useful in assessing whether per-entity blacklisting performance improved signiﬁcantly for each respective cloaking ﬁlter.vi.security recommendations  based on analysis of our experimental ﬁndings  we propose several possible improvements to the anti-phishing ecosystem.a.cloaking  our ﬁrst set of recommendations focuses speciﬁcally on the  cloaking techniques we tested.1) mobile users: we believe that  the highest priority within the current ecosystem should be effective phishing protection for mobile users.such users are not only inher- ently more vulnerable to phishing attacks [41]  but now also comprise the majority of web trafﬁc [4].our work has been impactful in better securing mobile users by enhancing existing anti-phishing systems.for over a year between mid-2017 and late 2018  gsb blacklists (with nearly 76% global market share) simply did not function properly on mobile devices: none of our phishing sites with filter a  e or f (targeting both desktop and mobile devices) showed any warnings in mobile chrome  safari  or firefox despite being blacklisted on desktop.we conﬁrmed the disparity between desktop and mobile protection through periodic small-scale testing and by analyzing an undisclosed dataset of trafﬁc to real phishing sites.following our disclosure  we learned that the inconsistency in mobile gsb blacklisting was due to the transition to a new mobile api designed to optimize data usage  which ultimately did not function as intended.because  blacklisting was not rectiﬁed after our full tests  we contacted the entities anew.as a result  in mid-september 2018 mozilla patched firefox (from version 63) such that all desktop warnings were also shown on mobile.google followed suit days thereafter with a gsb api ﬁx that covered mobile gsb browsers retroactively; mobile chrome and safari now mirror desktop listings  albeit with a shorter-lived duration to lower bandwidth usage.chrome  safari  and firefox thus again join opera as mobile browsers with effective blacklists  though some popular mobile browsers still lack such protection [41].upon close inspection of the preliminary test results  we found that filter b sites were solely blacklisted due to their suspicious urls rather than our reports.during our full tests  not a single site with filter b was blacklisted in any browser  interestingly despite the fact that crawlers did successfully retrieve many of these sites.gsb addressed this vulnerability in mid-september 2018  together with the aforementioned api ﬁx.through a subsequent ﬁnal redeployment of phishfarm  we veriﬁed that sites with filter b were being blacklisted following reports to gsb  the apwg  and paypal.other entities— including ones we did not test— should ensure that sites targeted at mobile users are being effectively detected.2) geolocation: although our experiments only considered two simple geolocation ﬁlters (us and non-us)  our ﬁndings are indicative of exploitable weaknesses in this area.given the overwhelming amount of crawler trafﬁc from the us (79%  per table ix in appendix iii)  filter c should not have been as effective as it proved to be.we hypothesize that other geo-speciﬁc ﬁlters would have similarly low blacklisting rates  in part due to the crawler characteristics discussed in section v-f1.country- or region-speciﬁc ﬁltering paired with localized page content is not an unusual sight in real-world paypal phishing kits that we have analyzed.3) javascript: it is trivial to implement javascript-based cloaking such as filter f.this technique proved to be effective in slowing blacklisting by three of the ﬁve entities in our full tests.fortunately  smartscreen bypasses this technique well  and paypal started doing so following our disclosure.the broader ecosystem should better adapt to client-side cloaking  in particular if its sophistication increases over time.b.anti-phishing entities  we also offer a number of more general recommendations  for anti-phishing systems of tomorrow.1) continuous testing: the mere failure of mobile black- listing that we observed during our experiments is sufﬁcient to warrant the need for continuous testing and validation of black- list performance.periodic deployment of phishfarm could be used for such validation.in addition  continuous testing could help ensure that future cloaking techniques— which may grow in sophistication— can effectively be mitigated without compromising existing defenses.as an added beneﬁt  trends in the relative performance of different entities and the overall timeliness and coverage of blacklisting could be modeled.2) trusted reporting channels: the phishing reporting channels we tested merely capture a suspected url or a  malicious e-mail.while the latter is useful in identifying spam origin  we believe a better solution would be the implementa- tion of standardized trusted phishing reporting systems that allow the submission of speciﬁc metadata (such as victim geolocation or device).trusted channels could allow detection efforts to more precisely target high-probability threats while minimizing abuse from deliberate false-negative submissions; they could also simplify and expedite collaboration efforts between anti-phishing entities and abused brands  which may hold valuable intelligence about imminent threats.3) blacklist timeliness: the gap between the detection of a phishing website and its blacklisting across browsers represents the prime window for phishers to successfully carry out their attacks.at the level of an individual entity  cloaking has a stronger effect on the occurrence rather than the time- liness of blacklisting.however  if we look at the ecosystem as a whole in figure 3  cloaking clearly delays blacklisting overall.our test results show that blacklisting now typically occurs in a matter of hours— a stark improvement over the day- or week-long response time observed years ago [6]  [55].however  given the tremendous increase in total phishing attacks since then (on the order of well over 100 attacks per hour in 2018 [2])  we believe that even today’s 40-minute best-case blacklist response time is too slow to deter phishers and effectively protect users.the gap needs to be narrowed  especially by slower entities (i.e.those not directly in control of blacklists).future work should investigate the real-world impact of delays in blacklisting on users and organizations victimized by phishing attacks in order to accurately establish an appropriate response threshold.4) volume: gsb  the apwg  and paypal crawled nearly all of the phishing sites we reported.in particular  gsb proved to deliver a consistently agile response time despite the high number of reports we submitted.other entities fell short of this level of performance.with increasing volumes of phishing attacks  it is essential that all players in the ecosystem remain robust and capable of delivering a consistent response.5) data sharing: data sharing has long been a concern within the ecosystem [56].we found that the two main blacklist operators (gsb and smartscreen) did not appear to effectively share data with each other  as per table vi.however  clearinghouse entities (apwg and phishtank) and paypal itself showed consistent protection across all browsers.unfortunately  the timeliness and overall coverage of clearing- houses appear to be inferior to those of the blacklist operators in their respective browsers.closer cooperation could thus not only speed up blacklisting  but also ensure that malicious sites are blocked universally.strengthening this argument  perhaps a breakdown in communication between infrastructure used by different entities accounted for those of our sites which were successfully crawled but not ultimately blacklisted.vii.related work  to the best of our knowledge  our work is the ﬁrst controlled effort to measure the effects of cloaking in the context of  phishing.several prior studies measured the general effective- ness of anti-phishing blacklists and the behavior of phishing kits; none of the prior work we identiﬁed considered cloaking  which may have had a skewing effect on the datasets and ecosystem ﬁndings previously reported.cloaking itself has previously been studied with respect to malicious search engine results; invernizzi et al.[7] proposed a system to detect such cloaking with high accuracy.oest et al.[8] later studied the nature of server-side cloaking techniques within phishing kits and proposed approaches for defeating each.virvilis et al.[41] carried out an evaluation of mobile web browser phishing protection in 2014 and found that major mobile web browsers at the time included no phishing protection.like that of sheng et al.this evaluation was empirical and based on a set of known phishing urls.in our work  we found that mobile chrome  safari  and firefox now natively offer blacklist protection  but that this protection was not functioning as advertised during our tests.the work most similar to ours is nss labs’ [39] recent use of a proprietary distributed testbed [57] to study the timeliness of native phishing blacklisting in chrome  firefox  and edge.the main limitation of nss labs’ approach is the reliance on feeds of known phishing attacks; any delay in the appearance of a site in each source feed can affect the accuracy of blacklisting time measurements.furthermore  phishing sites could be overlooked in the case of successful cloaking against the feeds.we address these limitations by having full control over the deployment and reporting time of phishing sites.sheng et al.[6] took an empirical approach to measure the effectiveness of eight anti-phishing toolbars and browsers powered by ﬁve anti-phishing blacklists.the authors found that heuristics by microsoft and symantec proved effective in offering zero-hour protection against a small fraction of phishing attacks  and that full propagation across phishing blacklists spanned several hours.this work also found that false positive rates in blacklists are near-zero; we thus did not pursue such tests in our experiments.while sheng et al.’s work was based on phishing sites only 30 minutes old  and was thus better controlled than earlier blacklist tests [58]  the datasets studied were of limited size and heterogeneous in terms of victim brands; the anti-phishing tools evaluated are now dated.in addition  sheng et al.checked blacklist status with a granularity of one hour— longer than our 10 minutes.han et al.[25] analyzed the lifecycle of phishing sites by monitoring cybercriminals’ behavior on a honeypot web server.the authors timed the blacklisting of the uploaded sites across google safe browsing and phishtank.uniquely  this work sheds light on the time between the creation and deployment of real phishing sites.a key difference in our work is our ability to customize and test different conﬁgurations of phishing kits instead of waiting for one to be uploaded.for instance  we could target speciﬁc brands or conﬁgure our own cloaking techniques to directly observe the ecosystem’s re- sponse.our experiments suggest a signiﬁcantly faster blacklist response time by the ecosystem than what han et al.found; our test sample size was also nearly ﬁve times larger.the differences between today’s phishing trends and those seen in prior work show that the ecosystem is evolving quickly.this warrants regular testing of defenses and re- evaluation of criminals’ circumvention techniques and attack vectors; it also underscores the importance of scalable and automatable solutions.our testbed shares some similarities with previous work [39]  [6] but it is the only such testbed to offer full automation without the need for intervention during the execution of experiments  and the only one to actively deploy sites and directly send reports to entities.viii.conclusion  by launching and monitoring a large set of phishing sites  we carried out the ﬁrst controlled evaluation of how cloaking techniques can hamper the timeliness and occurrence of phish- ing blacklist warnings in modern web browsers.as a result of our disclosure to anti-phishing entities  mobile blacklisting is now more consistent  and some of the cloaking techniques we tested are no longer as effective; others represent ongoing vulnerabilities which could be addressed through tweaks to existing detection systems.such tweaks should also seek to improve overall timeliness of blacklists to better counter the modern onslaught of phishing attacks.blacklist protection should not be taken for granted; continuous testing— such as that supported by our framework— is key to ensuring that browsers are secured sufﬁciently  and as intended.cloaking carries a detrimental effect on the occurrence of blacklisting due to the fundamentally-reactive nature of the main detection approach currently used by blacklists; it thus has the potential to cause continuous damage to the organiza- tions and users targeted by phishers.although no single entity we tested was able to individually defeat all of our cloaking techniques  collectively the requisite infrastructure is already in place.in the short term  collaboration among existing entities could help address their individual shortcomings.we observed that proactive defenses (such as the url heuristics of smartscreen) proved to deliver superior protection— but only under the right circumstances.in the long term  the ecosystem should move to more broadly implement general- purpose proactive countermeasures to more reliably negate cloaking.it is important for the ecosystem to be able to effectively bypass cloaking because it is merely one way in which phishing sites can be evasive.for instance  with cloaking alongside redirection chains or bulletproof hosting  phishing sites might otherwise avoid existing mitigations far more successfully than what we have observed.phishing has proven to be a difﬁcult problem to solve due to attackers’ unyielding persistence  the cross-organizational nature of infrastructure abused to facilitate phishing  and the reality that technical controls cannot always compensate for the human weakness exploited by social engineers.we believe that continuous and close collaboration between all anti-abuse entities  which can lead to a deep understanding of current threats and development of intelligent defenses  is the crux of optimizing controls and delivering the best possible long-term protection for phishing victims.acknowledgments  the authors would like to thank the reviewers for their insightful feedback and suggestions.this work was partially supported by paypal  inc.and a grant from the center for cy- bersecurity and digital forensics at arizona state university
34,"2019-Poster- Data Quality for Security Challenges- Case Studies of Phishing, Malware and Intrusion Detection Datasets.pdf", techniques from data science are increasingly being applied by researchers to security challenges.however  challenges unique to the security domain necessitate painstaking care for the models to be valid and robust.in this paper  we explain key dimensions of data quality relevant for security  illustrate them with several popular datasets for phishing  intrusion detection and malware  indicate operational methods for assuring data quality and seek to inspire the audience to generate high quality datasets for security challenges.keywords semiotics; data quality; data difficulty; data poisoning acm reference format: rakesh m.verma  victor zeng and houtan faridi.2019.poster: data quality for security challenges: case studies of phishing  malware and intrusion de- tection datasets.in 2019 acm sigsac conference on computer and commu- nications security (ccs ’19)  november 11–15  2019  london  united kingdom.acm  new york  ny  usa  3 pages.https://doi.org/10.1145/3319535.3363267 1, researchers have been applying data mining and machine learning techniques to security challenges for the past , through three different case studies we have illustrated the issues with datasets for security challenges.we have shown how to find them and how to address them.we defined dataset difficulty as a measure of dataset quality.much remains to be done  for example  when to stop cleaning.one possibility is to see if dataset sources or other key metadata can be identified accurately based on the data.acknowledgments research supported in part by nsf grants cns 1319212  dge 1433817  due 1356705 and iis 1659755
35,2019-Poster- Understanding User_s Decision to Interact with Potential Phishing Posts on Facebook using a Vignette Study.pdf,—facebook  the largest social networking site (sns) with over one billion active monthly users  has been woven into the everyday life of many people.while this platform has drastically improved how we interact with one another  it has also opened up a multitude of security and privacy issues.for example  online attackers are increasingly employing phishing attacks on facebook  seeking to fool their victims by posing as friends using fake or compromised accounts.these attacks are hard to recognize by the facebook defense system and users alike  and few studies give any insight into how users interact with such attacks.in this study  we take the ﬁrst step to understand how users react and decide whether to click when they encounter sns posts with links  including possibly suspicious links.we found that users decide to interact with shared contents based on their relationship with the post author (from whose account the post is shared; perhaps compromised).at the same time  they mostly ignore the location of the shared post (e.g.post author’s wall or target user’s wall)  and any context pointing to a post possibly being suspicious.we also explored the potential of showing a visual warning for suspicious posts.although our simple warning system failed to prevent users from clicking on suspicious posts altogether  it did reduce the likelihood of users clicking on such posts.based on our ﬁndings  we identiﬁed the scope of future work to protect users against phishing attacks in snses.i,  phishing is a digital security attack that exploits human errors in online navigation [1].according to the report from anti-phishing working group [,
36,2019-Where does it go_ refining indirect-call targets with multi-layer type analysis.pdf, system software commonly uses indirect calls to realize dynamic program behaviors.however  indirect-calls also bring challenges to constructing a precise control-flow graph that is a standard pre- requisite for many static program-analysis and system-hardening techniques.unfortunately  identifying indirect-call targets is a hard problem.in particular  modern compilers do not recognize indirect- call targets by default.existing approaches identify indirect-call targets based on type analysis that matches the types of function pointers and the ones of address-taken functions.such approaches  however  suffer from a high false-positive rate as many irrelevant functions may share the same types.in this paper  we propose a new approach  namely multi-layer type analysis (mlta)  to effectively refine indirect-call targets for c/c++ programs.mlta relies on an observation that function pointers are commonly stored into objects whose types have a multi- layer type hierarchy; before indirect calls  function pointers will be loaded from objects with the same type hierarchy “layer by layer”.by matching the multi-layer types of function pointers and func- tions  mlta can dramatically refine indirect-call targets.mlta is effective because multi-layer types are more restrictive than single- layer types.it does not introduce false negatives by conservatively tracking targets propagation between multi-layer types  and the layered design allows mlta to safely fall back whenever the anal- ysis for a layer becomes infeasible.we have implemented mlta in a system  namely typedive  based on llvm and extensively evaluated it with the linux kernel  the freebsd kernel  and the firefox browser.evaluation results show that typedive can elimi- nate 86% to 98% more indirect-call targets than existing approaches do  without introducing new false negatives.we also demonstrate that typedive not only improves the scalability of static analysis but also benefits semantic-bug detection.with typedive  we have found 35 new deep semantic bugs in the linux kernel.ccs concepts • security and privacy → systems security; software and ap- plication security.keywords layered type analysis; indirect-call targets; function pointers; cfi  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.ccs ’19  november 11–15  2019  london  united kingdom © 2019 association for computing machinery.acm isbn 978-1-4503-6747-9/19/11...$15.00 https://doi.org/10.1145/3319535.3354244  acm reference format: kangjie lu and hong hu.2019.where does it go? refining indirect-call targets with multi-layer type analysis.in 2019 acm sigsac conference on computer and communications security (ccs ’19)  november 11–15  2019  london  united kingdom.acm  new york  ny  usa  16 pages.https://doi.org/10.1145/3319535.3354244  1, function pointers are commonly used in c/c++ programs to sup- port dynamic program behaviors.for example  the linux kernel provides unified apis for common file operations such as open().internally  different file systems have their own implementations of these apis  and the kernel uses function pointers to decide which concrete implementation to invoke at runtime.such an invocation is known as an indirect call (icall for short).common icall targets include callback functions  jump-table entries  and virtual functions.while icalls are common and useful  by their dynamic nature  icall targets cannot be precisely decided through static analysis.this leads to inherent challenges in constructing precise global control-flow graph (cfg) that connects icalls to their targets.in particular  compilers such as gcc and llvm do not recognize icall targets by default.users of cfg have two options: stopping an analysis when encountering icalls or continuing an analysis by taking all address-taken functions as potential targets.both options have apparent drawbacks.the former limits the coverage of the analysis  while the latter limits the scalability and precision of the analysis  and hurts the strength of system hardening techniques.more specifically  many bug detection tools using inter-procedural analysis choose to skip icalls [, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of nsf or onr
37,2020-An Analysis of Phishing Blacklists Google Safe Browsing OpenPhish and PhishTank.pdf, blacklists play a vital role in protecting internet users against phish- ing attacks.the effectiveness of blacklists depends on their size  scope  update speed and frequency  and accuracy - among other characteristics.in this paper we present a measurement study that analyses 3 key phishing blacklists: google safe browsing (gsb)  openphish (op)  and phishtank (pt).we investigate the uptake  dropout  typical lifetimes  and overlap of urls in these blacklists.during our 75-day measurement period we observe that gsb contains  on average  1.6 million urls  compared to 12 433 in pt and 3 861 in op.we see that op removes a significant proportion of its urls after 5 and 7 days  with none remaining after 21 days - potentially limiting the blacklist’s effectiveness.we observe fewer urls residing in all 3 blacklists as time-since-blacklisted increases – suggesting that phishing urls are often short-lived.none of the 3 blacklists enforce a one-time-only url policy - therefore protecting users against reoffending phishing websites.across all 3 blacklists  we detect a significant number of urls that reappear within 1 day of removal – perhaps suggesting premature removal or re-emerging threats.finally  we discover 11 603 unique urls residing in both pt and op – a 12% overlap.despite its smaller average size  op detected over 90% of these overlapping urls before pt did.ccs concepts • security and privacy → malware and its mitigation; phish- ing; • general and reference → empirical studies; measurement;  keywords security  phishing  blacklists  measurement study  acm reference format: simon bell and peter komisarczuk.2020.an analysis of phishing black- lists: google safe browsing  openphish  and phishtank.in proceedings of the australasian computer science week multiconference  melbourne  vic  australia  february 4–6  2020 (acsw 2020)  11 pages.https://doi.org/10.1145/3373017.3373020  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than the author(s) must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.acsw 2020  february 4–6  2020  melbourne  vic  australia © 2020 copyright held by the owner/author(s).publication rights licensed to associa- tion for computing machinery.acm isbn 978-1-4503-7697-6/20/02...$15.00 https://doi.org/10.1145/3373017.3373020  1  1, originating in the 1990s on aol before expanding to other plat- forms such as email  sms  and social media: phishing attacks lure their victims into revealing sensitive information such as pass- words and credit card numbers by spoofing legitimate organisations.phishing campaigns are a dangerous threat in the cyber world and the number of these attacks continues to grow: over 180 000 unique phishing websites were detected by the anti-phishing working group (apwg) in q1 ,
38,2020-An investigation of phishing awareness and education over time- When and how to best remind users.pdf,  security awareness and education programmes are rolled out in more and more organisations.however  their eﬀective- ness over time and  correspondingly  appropriate intervals to remind users’ awareness and knowledge are an open ques- tion.in an attempt to address this open question  we present a ﬁeld investigation in a german organisation from the public administration sector.with overall 409 employees  we eval- uated (a) the eﬀectiveness of their newly deployed security awareness and education programme in the phishing context over time and (b) the eﬀectiveness of four diﬀerent reminder measures – administered after the initial eﬀect had worn oﬀ to a degree that no signiﬁcant improvement to before its deploy- ment was detected anymore.we ﬁnd a signiﬁcantly improved performance of correctly identifying phishing and legitimate emails directly after and four months after the programme’s deployment.this was not the case anymore after six months  indicating that reminding users after half a year is recom- mended.the investigation of the reminder measures indicates that measures based on videos and interactive examples per- form best  lasting for at least another six months.1,  maintaining information security is an important challenge for organisations  and also for governmental and public admin- istration sectors.the so-called german national it planning council [71] requires german organisations in the public administration sector to implement information security man- agement systems (isms).one of the goals of such ismss  copyright is held by the author/owner.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee.usenix symposium on usable privacy and security (soups) ,
39,2020-Demo- PhishBench 2.0- A Versatile and Extendable Benchmarking Framework for Phishing.pdf, we describe version 2.0 of our benchmarking framework  phish- bench.with the addition of the ability to dynamically load fea- tures  metrics  and classifiers  our new and improved framework allows researchers to rapidly evaluate new features and methods for machine-learning based phishing detection.researchers can compare under identical circumstances their contributions with numerous built-in features  ranking methods  and classifiers used in the literature with the right evaluation metrics.we will demon- strate phishbench 2.0 and compare it against at least two other automated ml systems.keywords automatic framework  phishing  machine learning  benchmarking  acm reference format: victor zeng  xin zhou  shahryar baki  and rakesh m.verma.2020.demo: phishbench 2.0: a versatile and extendable benchmarking framework for phishing.in proceedings of the 2020 acm sigsac conference on computer and communications security (ccs ’20)  november 9–13  2020  virtual event  usa.acm  new york  ny  usa  3 pages.https://doi.org/10.1145/3372297.3420017  1, phishing is a serious and challenging problem with a large amount of research [11].while previous literature has used a variety of machine learning algorithms and evaluation metrics  few studies compare the performance of these algorithms using common met- rics and datasets  or evaluate the metrics’ suitability.moreover  phishing researchers usually test their proposed methods against limited baselines and with few metrics when presenting new fea- tures or approach(es) [4].hence  there is a need for a benchmarking framework to evaluate such systems as comprehensively as possi- ble.phishbench 1.0 [5  14] is a framework for testing features and classifiers on identical pipelines.it supports both email and url datasets and contains 1,
40,2020-Dns cache poisoning attack reloaded- Revolutions with side channels.pdf, in this paper  we report a series of flaws in the software stack that leads to a strong revival of dns cache poisoning — a clas- sic attack which is mitigated in practice with simple and effective randomization-based defenses such as randomized source port.to successfully poison a dns cache on a typical server  an off-path adversary would need to send an impractical number of 232 spoofed responses simultaneously guessing the correct source port (16-bit) and transaction id (16-bit).surprisingly  we discover weaknesses that allow an adversary to “divide and conquer” the space by guess- ing the source port first and then the transaction id (leading to only 216 + 216 spoofed responses).even worse  we demonstrate a number of ways an adversary can extend the attack window which drastically improves the odds of success.the attack affects all layers of caches in the dns infrastructure  such as dns forwarder and resolver caches  and a wide range of dns software stacks  including the most popular bind  unbound  and dnsmasq  running on top of linux and potentially other oper- ating systems.the major condition for a victim being vulnerable is that an os and its network is configured to allow icmp error replies.from our measurement  we find over 34% of the open resolver pop- ulation on the internet are vulnerable (and in particular 85% of the popular dns services including google’s 8.8.8.8).furthermore  we comprehensively validate the proposed attack with positive results against a variety of server configurations and network con- ditions that can affect the success of the attack  in both controlled experiments and a production dns resolver (with authorization).ccs concepts • networks → cross-layer protocols; • security and privacy → network security.keywords dns cache poisoning  side channel  off path attack  icmp rate limit  this work is licensed under a creative commons attribution international 4.0 license.ccs ’20  november 9–13  2020  virtual event  usa © 2020 copyright held by the owner/author(s).acm isbn 978-1-4503-7089-9/20/11.https://doi.org/10.1145/3372297.3417280  acm reference format: keyu man  zhiyun qian  zhongjie wang  xiaofeng zheng  youjun huang  and haixin duan.2020.dns cache poisoning attack reloaded: revolutions with side channels.in proceedings of the 2020 acm sigsac conference on computer and communications security (ccs ’20)  november 9–13  2020  virtual event  usa.acm  new york  ny  usa  14 pages.https://doi.org/10.1145/3372297.3417280  1, domain name system (dns) is an essential part of the internet  orig- inally designed to translate human-readable names to ip addresses.nowadays  dns has also been overloaded with many other secu- rity critical applications such as anti-spam defenses [38]  routing security (e.g.rpki) [10].in addition  dns also plays a crucial role in bootstrapping trust for tls.tls certificates are now commonly acquired by proving the ownership of a domain [,
41,2020-PhishTime- Continuous Longitudinal Measurement of the Effectiveness of Anti-phishing Blacklists.pdf,  due to their ubiquity in modern web browsers  anti- phishing blacklists are a key defense against large-scale phishing attacks.however  sophistication in phishing websites—such as evasion techniques that seek to defeat these blacklists—continues to grow.yet  the e(cid:29)ectiveness of blacklists against evasive websites is di(cid:28)cult to measure  and there have been no methodical e(cid:29)orts to make and track such measurements  at the ecosystem level  over time.we propose a framework for continuously identifying un- mitigated phishing websites in the wild  replicating key as- pectsoftheircon(cid:27)gurationinacontrolledsetting andgenerat- ing longitudinal experiments to measure the ecosystem’s pro- tection.in six experiment deployments over nine months  we systematicallylaunchandreport2 862new(innocuous)phish- ing websites to evaluate the performance (speed and coverage) and consistency of blacklists with the goal of improving them.we show that methodical long-term empirical measure- ments are an e(cid:29)ective strategy for proactively detecting weak- nesses in the anti-phishing ecosystem.through our exper- iments  we identify and disclose several such weaknesses  including a class of behavior-based javascript evasion that blacklists were unable to detect.we (cid:27)nd that enhanced protec- tions on mobile devices and the expansion of evidence-based reporting protocols are critical ecosystem improvements that could better protect users against modern phishing attacks  which routinely seek to evade detection infrastructure.1, phishing attacks represent a signi(cid:,
42,2021-Assessing Browser-level Defense against IDN-based Phishing.pdf,  internationalized domain names (idn) allow people around the world to use their native languages for domain names.unfortunately  because characters from different languages can look like each other  idns have been used to imperson- ate popular domains for phishing  i.e.idn homograph.to mitigate this risk  browsers have recently introduced defense policies.however  it is not yet well understood regarding how these policies are constructed and how effective they are.in this paper  we present an empirical analysis of browser idn policies  and a user study to understand user perception of homograph idns.we focus on 5 major web browsers (chrome  firefox  safari  microsoft edge  and ie)  and 2 mo- bile browsers (android chrome and ios safari) and analyze their current and historical versions released from january 2015 to april 2020.by treating each browser instance as a black box  we develop an automated tool to test the browser policies with over 9 000 testing cases.we ﬁnd that all the tested browsers have weaknesses in their rules  leaving oppor- tunities for attackers to craft homograph idns to impersonate target websites while bypassing browsers’ defense.in addi- tion  a browser’s defense is not always getting stricter over time.for example  we observe chrome has reversed its rules to re-allow certain homograph idns.finally  our user study shows that the homograph idns that can bypass browsers’ defense are still highly deceptive to users.overall  our results suggest the need to improve the current defense against idn homograph.1,  the internet is progressively globalizing  and yet for a long time  most internet domain names were restricted to english characters (in combination with hyphen and digits) [43].to allow people around the world to use their native languages for domain names  internationalized domain names (idn)  *co-ﬁrst authors with equal contribution.were introduced and standardized in ,
43,2021-Catching Phishers By Their Bait- Investigating the Dutch Phishing Landscape through Phishing Kit Detection.pdf,  off-the-shelf  easy-to-deploy phishing kits are believed to lower the threshold for criminal entrepreneurs going phishing.that is  the practice of harvesting user credentials by tricking victims into disclosing these on fraudulent websites.but  how do these kits impact the phishing landscape? and  how often are they used? we leverage the use of tls certiﬁcates by phishers to uncover possible dutch phishing domains aimed at the ﬁnancial sector between september 2020 and january 2021.we collect 70 different dutch phishing kits in the un- derground economy  and identify 10 distinct kit families.we create unique ﬁngerprints of these kits to measure their preva- lence in the wild.with this novel method  we identify 1 363 dutch phishing domains that deploy these phishing kits  and capture their end-to-end life cycle – from domain registration  kit deployment  to take-down.we ﬁnd the median uptime of phishing domains to be just 24 hours  indicating that phishers do act fast.our analysis of the deployed phishing kits reveals that only a small number of different kits are in use.we dis- cover that phishers increase their luring capabilities by using decoy pages to trick victims into disclosing their credentials.in this paper  we paint a comprehensive picture of the tac- tics  techniques and procedures (ttp) prevalent in the dutch phishing landscape and present public policy takeaways for anti-phishing initiatives.1,  phishing is a pervasive type of social engineering that harvests user credentials by tricking targets into disclosing personal or ﬁnancial information – e.g.credit card details – on a fraudu- lent website.deploying a phishing website has become trivial with so-called ‘phishing kits’  which can be bought  leased or even downloaded for free in the underground economy – like dark net markets [34]  social media platforms or secure messaging services like telegram [, of han et al.[17]  as this would indicate that both victim and attacker originate from the same country.during installation and testing of the kit  visitors are occa- sionally redirected to popular benign domains like google or bing  or to the website of the target organization.during our crawls  we observed 49 different phishing domains doing this before their phishing kit was fully deployed and operational.end-to-end life cycle steps we can determine timestamps of all steps within a typical phishing campaign – shown in fig- ure 2 and explained in section 2 – by combining the retrieved whois records and crawling timestamps of all identiﬁed phish- ing domains.unfortunately  460 fqdns in our dataset lack whois information due to inconsistent information formatting or server errors beyond our control.therefore  these domains are excluded from the analysis in this paragraph.additionally  we focus this analysis on type iv phishing domains only.as type iii domains include hijacked domains which have not been registered purposefully for phishing.the end-to-end life cycle analysis on the remaining 818 domains is summarized in figure 12 as a horizontal box plot   with the hours since domain registration on a logarithmic x-axis.as indicated by the red bars inside the boxes  a phish- ing domain is online – i.e.returns a successful http 200 response – three hours after registration on average.often- times  quickly followed by the installation of a phishing kit  on average only one hour later.after a successful installation  the phisher sends out the bait to its potential victims and waits for credentials to be ﬁlled in.the domain goes ofﬂine after 40 hours on average.the majority of domains complete this full life cycle within a couple of days.note however  that there are also outliers.in these cases  the domain was registered many days in advance  waiting to be used by the attacker.in our dataset  only 114 of the 818 domains (14%) were registered more than 24 hours before coming online.6.3 external resources & evasion techniques during our analysis of phishing domains in the wild  we noticed that some websites make external connections.as explained in the section 4.1  phishing websites could either include all impersonated resources – e.g.javascript  css and images – on the domain  or refer to resources hosted externally.analyzing the resources loaded by all identiﬁed phishing domains tells us that only 104 domains (7.6% of the total dataset) load their resources directly from their benign counterparts.this ﬁnding contradicts the assumption under- lying the work of oest et al.[39] and makes their method of analyzing web server logs for malicious external requests less robust  as only a very small portion of websites in our dataset is pursuing this method.however  it does conﬁrm the ﬁndings of han et al.[17] and cova et al.[9]  who also observed a neg- ligible portion of phishing kits with resources loaded from the target organization.these authors studied attacker behavior on honeypot domains  which is based on the assumption that attackers hijack domains to use for phishing.although our measurement methodology is not perfectly suited to ﬁnd such hijacked domains – as these domains often already have tls certiﬁcates – we did ﬁnd 18 of them.all of these domains in- clude the full fqdn of the target organization as subdomains and have a slightly longer uptime of 72 hours on average.evasion techniques as explained in section 4  some phish- ing kits deploy evasion techniques to prevent detection by anti-  figure 10: cdf of a phishing domain uptime (n = 1 288)  domains with multiple certiﬁcates issued excluded  figure 11: histogram of kit installation hours (n = 1 363)  3768    30th usenix security symposium  usenix association  0255075100125150175time online (h)0.000.250.500.751.00fraction01234567891011121314151617181920212223time of the day (utc+1)050100frequencyfigure 12: boxplot of timestamps in the end-to-end life cycle of the identiﬁed phishing domains (n = 818)  phishing services such as apwg [14] and google safebrows- ing [20].these techniques  often referred to as cloaking  allow phishers to show a different page to a potential victim than to a crawler [21  53].although our methodology is focused on detecting the use of speciﬁc phishing kits in the wild and not to identify cloaking  we did observe such evasion tech- niques many times.in fact  946 (69%) of the detected phishing domains returned a blank screen – and no favicon – to our crawler when we visited the domain  meaning that the phish- ing website detected us and deployed cloaking techniques.however  our phishing kit detection was still possible because these websites returned a successful response for the ﬁles included in the ﬁngerprint.the phishing kit responsible for most of these cloaking activities was again the uadmin kit  which combined some server-side and client-side cloaking.on the server-side  it checked the ip address with a block list and created a random path for every visitor  as explained in section 4.2.on the client-side  it deployed a simple javascript timeout to evade non-javascript crawlers.the combination of both techniques is shown in listing 1.7 external validation  to benchmark and validate our methodology  we compare our results with data from the apwg ecrime exchange (ecx) [15].this repository contains phishing activity from all over the world  including many dutch phishing domains.a comparison shows that our methodology covers a much broader spectrum of phishing domains  capturing known dif- ferentiations in the phishing landscape.in total  only 77 phish- ing domains detected using our methodology  overlap with the apwg database  meaning that 1 286 domains are not listed in their repository.by comparing the date on which a phishing domain was initially detected by our crawler with the data it was submitted to the ecx  we ﬁnd that our method was able to identify phishing domains much faster.in 76 out of the 77 cases (99%)  our crawler detected the phishing domains faster than apwg  with a median time difference of 11.3 hours (almost half a day) earlier.interestingly  the domains that overlap with the ecx repository had clearly more bank names included in their domain name.61 of the domains (79%) overlapping with ecx contained a reference to a bank  whereas only 44% had this in the complete dataset.this external validation shows that our methodology has the potential to detect phishing websites very swiftly which could save unsuspecting people from this kind of fraud.8 throwing out the bait  in the previous sections of this paper  we have unraveled the characteristics of every step in the end-to-end life cycle of a phishing campaign  except for the last step: sending out the text messages  e-mails or social media posts  the so-called bait.although our measurement system does not contain the input data necessary to thoroughly analyze this step of the life cycle  the authors are among the target population of phishers and thus regularly receive the thrown out bait themselves.during our data collection period  we collected these messages and looked into the ones that contained links to domains in our dataset.this allows us to show the complete timeline of events in a phishing campaign life cycle.we discuss an example in the following section.verify your identity within the ﬁrst two weeks of our re- search  we received a text message seemingly originating from digid  the ofﬁcial dutch digital identity service.the mes- sage shown in figure 14  stated that a suspicious login was detected and that immediate action was necessary to prevent cancellation of the account.this is a prime example of the scarcity and consistency luring techniques as described in sec- tion 2.1.the link included in the message directed victims to https://deblokkeren-digid.xyz  a type iv domain made with a phishing kit belonging to the uadmin family.this website was registered only six hours before the mes- sage was received and fully operational just three hours later.on the website  potential victims were asked to verify their identity by logging into their online bank account.multiple options are displayed on the decoy page as shown in fig- ure 13a  allowing the victim to choose their preferred bank.upon clicking on one of the buttons  the victim is redirected to yet another phishing page as shown in figure 13b  which mimics the chosen bank’s login screen.that page eventu- ally captures the login credentials of the victim.the use of the digid decoy page is a prime example of the technique depicted in figure 1.within a day  only 12 hours later  the domain was taken ofﬂine.usenix association  30th usenix security symposium    3769  1101001000time since domain registration (h)domain offlinekit installeddomain online(a)  (b)  figure 13: landing (decoy) page in 13a: indicating that veriﬁcation through a bank account is necessary to prevent account deactivation and the actual phishing page in 13b after clicking on a bank of choice on which user credentials are harvested  sms message – 17-09-2020 21:32 (translated)  [my digid] there has been a suspicious login in your my digid account.verify this directly to pre- vent cancellation of your my digid account through: https://deblokkeren-digid.xyz/inloggen  figure 14: text message demanding digid veriﬁcation and corresponding timeline of deblokkeren-digid.xyz  9 related work  earlier work on phishing involves many different points of view and subjects.ranging from creating robust domain detection methods [5  12  13  27  32  45]  phishing kit analy- sis [9  17  38]  evasion techniques [21  53] to research focused on victim behavior [49].much effort has been devoted to the creation of robust detection techniques  but less is known about the life cycle  ecosystem and actors behind such at- tacks.only a limited number of researchers have investigated this part of phishing [35  39]  which we deem essential to fully understand the ecosystem and to be able to create robust countermeasures.analysis on phishing kits early work on phishing kits in 2008 by cova et al.[9] focused on the analysis of ‘free’ phish- ing kits.they noticed that packages containing easy-to-deploy phishing websites often contained backdoors which exﬁltrated  the gathered information also to third parties and that 100% of the investigated kits were written in the php language.in their phisheye study  han et al.[17] share insights into live phishing websites created by deploying phishing kits on honeypot domains.using their sandboxed approach  they were able to lure phishers into installing phishing kits on their honeypot servers of which the behavior was closely monitored.the authors analyzed both phisher and victim actions on the phishing website  showed that phishing kits are only active for less than 10 days since their installation and that most of the victims share the same country of origin.during their 5 months analysis period (sep 2015 – jan 2016)  they collected 643 unique phishing kits of which 74% were correctly installed by 471 distinct attackers.additionally  they discovered that only 10 phishing kits loaded the resources directly from the website of the targeted organization.measurements on live phishing domains in recent work  oest et al.[38] analyzed .htaccess ﬁles – commonly used on apache web servers – to capture the evasive behavior of phishers.these ﬁles allow phishers to protect themselves against anti-phishing or search engine crawlers.their paper states that deny ip and user-agent ﬁlters are the most prevail- ing blacklisting technologies  whilst the allow ip ﬁlter type is often used to target speciﬁc countries.additionally  they proposed a new high-level classiﬁcation scheme for phishing urls that builds upon the work of garera et al.[12].this tax- onomy categorizes phishing urls into ﬁve categories with different hiding and lure strategies.we also used that taxon- omy to classify the urls detected by our measurements in section 5.1.the work closest to ours is from the same authors  who continued their research by investigating the end-to-end life cycle of phishing attacks in 2020.this work relied on the observation that a substantial proportion of phishing pages make requests for web resources to the websites that the at- tackers impersonate [39].a unique collaboration with a large payment provider enabled them to link such web requests to  3770    30th usenix security symposium  usenix association  17-sep 15:0917-sep 20:0918-sep 01:0918-sep 06:09domain name registredtls certificate issueddomain crawled and onlinedomain offlinesms receivedthe phishing websites they are originating from.this gave the authors an in-depth look into phishing campaigns from the moment the attacker installs the phishing page to the moment victims disclose their credentials.they found that the average phishing attack spans 21 hours and that modern web browsers display a warning for a detected domain after 16 hours.oest et al.[39] called the gap between the launch of the attack and detection by anti-phishing bodies the ‘golden hours’ of phishing  in which the attackers gather 38% of their phished credentials.as our work shared a similar goal – analyzing the end-to-end life cycle of a phishing campaign – we share a number of ﬁndings.namely  the use of extensive use of server-side cloaking  victim-speciﬁc paths and the presence of mitm-proxies in phishing kits.additionally  our conclu- sions regarding the duration of an average phishing attack are comparable.however  there are also notable differences.their work is focused on one single organization and includes both http and https trafﬁc whereas our work focuses on the entire dutch ﬁnancial sector  but was limited to do- mains served over https only.furthermore  they relied on the assumption that phishing domains load resources directly from the target website  whereas we discovered that only a negligible portion of domains in our analysis did so.10 discussion  limitations analyzing a phenomenon like phishing always brings its inherent limitations and so does this study.as all other work on this topic  our methodology is only able to cap- ture part of the phishing landscape.we identify the following limitations:  we are aware of the fact that by our choice of methodology  we are limited to phishing domains secured by https con- nections only.yet we believe  as 78% of all phishing in 2020 is delivered through https according to the apwg [16] and the fact that oest et al.[39] concluded that phishing served over https was three times more effective  the effects of that concise decision to be limited.also note that our approach re- sults in our ability to identify type iii and iv phishing domains only  and thus miss the three other types.another limitation of this work is that we are limited to identifying known phishing kits.phishing domains that do not match any of our prede- ﬁned ﬁngerprints are simply not marked as phishing.besides these missing kits  phishers could also change the ﬁle names or structure of their phishing kits  which would also render our detection methodology less effective.however  the main advantage of phishing kits is that they are easy to deploy for any criminal that wants to go phishing.therefor  we do not expect that phishers that deploy these kits are either capable or willing to make numerous changes each time they deploy a new phishing website.on the other hand  our ﬁngerprinting methodology also has a detection advantage for websites that deploy certain cloaking strategies.as explained in section 4.1  some phishing websites ban ip ranges or user-agents known  to be used by anti-phishing services through php scripts on the homepage  or show a different landing page depending on the country of origin.these methods make detection based on the characteristics of the page – e.g.login forms  bank icons  etc.– rather difﬁcult.however  searching for known ﬁles – our ﬁngerprints – on such domains bypasses these evasion methods and results in a robust detection of a phishing kit.we started our crawling infrastructure three months be- fore data collection started  which allowed us to carefully examine the domains missed by our crawler.as explained in section 3.4  we created ﬁngerprints based on source code of live phishing websites missed by our crawling during testing.so even without obtaining the actual phishing kit  we were able to create robust ﬁngerprints.unfortunately  the largest limitation is in missing data we do not see.as explained in section 5.1  many domain names do not contain
44,2021-Catching Transparent Phish- Analyzing and Detecting MITM Phishing Toolkits.pdf, for over a decade  phishing toolkits have been helping attackers automate and streamline their phishing campaigns.man-in-the- middle (mitm) phishing toolkits are the latest evolution in this space  where toolkits act as malicious reverse proxy servers of online services  mirroring live content to users while extracting cre- dentials and session cookies in transit.these tools further reduce the work required by attackers  automate the harvesting of 2fa- authenticated sessions  and substantially increase the believability of phishing web pages.in this paper  we present the first analysis of mitm phishing toolkits used in the wild.by analyzing and experimenting with these toolkits  we identify intrinsic network-level properties that can be used to identify them.based on these properties  we develop a machine learning classifier that identifies the presence of such toolkits in online communications with 99.9% accuracy.we conduct a large-scale longitudinal study of mitm phishing toolkits by creating a data-collection framework that monitors and crawls suspicious urls from public sources.using this infrastruc- ture  we capture data on 1 220 mitm phishing websites over the course of a year.we discover that mitm phishing toolkits occupy a blind spot in phishing blocklists  with only 43.7% of domains and 18.9% of ip addresses associated with mitm phishing toolkits present on blocklists  leaving unsuspecting users vulnerable to these attacks.our results show that our detection scheme is resilient to the cloaking mechanisms incorporated by these tools  and is able to detect previously hidden phishing content.finally  we propose methods that online services can utilize to fingerprint requests origi- nating from these toolkits and stop phishing attempts as they occur.ccs concepts • security and privacy → phishing.keywords phishing; web security; social engineering  permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.copyrights for components of this work owned by others than acm must be honored.abstracting with credit is permitted.to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.request permissions from permissions@acm.org.ccs ’21  november 15–19  2021  virtual event  republic of korea.© 2021 association for computing machinery.acm isbn 978-1-4503-8454-4/21/11...$15.00 https://doi.org/10.1145/3460120.3484765  acm reference format: brian kondracki  babak amin azad  oleksii starov  and nick nikiforakis.2021.catching transparent phish: analyzing and detecting mitm phish- ing toolkits.in proceedings of the 2021 acm sigsac conference on com- puter and communications security (ccs ’21)  november 15–19  2021  vir- tual event  republic of korea.acm  new york  ny  usa  17 pages.https: //doi.org/10.1145/3460120.3484765 1, the combination of a username and password is the default gate- keeper to nearly all online services that users interact with  on a daily basis.it is therefore no surprise that this sensitive informa- tion is in high demand among malicious actors who go to great lengths to obtain it  in order to access sensitive information and act on behalf of victims.one of the most commonly used methods of acquiring this information is through social engineering  in the form of phishing.phishers impersonate trustworthy entities in an attempt to lure victims into disclosing private information  such as account credentials and banking information.traditionally  phishing websites were hosted entirely on attacker- owned and compromised web servers where attackers would host realistic-looking copies of their target websites in hope of convinc- ing users to disclose their credentials.these credentials were stored either on the original server or communicated to the attacker (e.g.via an email) for later abuse [57].these rudimentary phishing se- tups required substantial effort on behalf of attackers to clone target websites  make the necessary content-modification to make these sites operational  and repeat this entire process to match updates to the ui of the target website.to reduce the effort required by attackers to create and serve phishing content  all-in-one phishing toolkits began to overtake traditional setups.these toolkits revolutionized how phishing web- sites are created by automatically fetching static copies of web pages from targeted websites  serving them to victims  and prevent- ing detection through cloaking mechanisms—all while requiring minimal effort by attackers [50].however  the increasing adoption of two-factor-authentication (,
45,2021-Compromised or Attacker-Owned- A Large Scale Classification and Study of Hosting Domains of Malicious URLs.pdf,  1 ,  the mitigation action against a malicious website may differ greatly depending on how that site is hosted.if it is hosted under a private apex domain  where all its subdomains and pages are under the apex domain owner’s direct control  we could block at the apex domain level.if it is hosted under a public apex domain though (e.g.a web hosting service provider)  it would be more appropriate to block at the subdo- main level.further  for the former case  the private apex do- main may be legitimate but compromised  or may be attacker- generated  which  again  would warrant different mitigation actions: attacker-owned apex domains could be blocked per- manently  while only temporarily for compromised ones.in this paper  we study over eight hundred million virus- total (vt) url scans from aug.1  ,  we design machine learning models to distinguish two kinds of malicious url hosting apex domains  public and private.this classiﬁcation helps security professionals specify which domain levels to block  the whole apex domain in the case of private apexes or speciﬁc subdomains/path sufﬁxes in the case of public ones.our results show that we can classify apex domains as public or private with 97.2% accuracy  97.7% pre- cision and 95.6% recall.from the private malicious domains  we also design another machine learning model to differenti- ate attacker-owned from compromised hosting apexes.this distinction is crucial to help security operators take the ap- propriate mitigation actions.for example  attacker-owned do- mains could be blocked permanently whereas compromised ones temporarily.the result shows that this classiﬁer achieves 96.8% accuracy with 99.1% precision and 93.4% recall.we also design a classiﬁer with high accuracy to classify public websites as attacker-owned or compromised.in terms of statis- tics  our results reveal a concerning trend of the malicious domains observed from vt url feed: most of the attacks are launched from websites whose apexes are not owned by attackers.even though public apex domains are less than 1% of the apexes hosting malicious websites  they amount to a whopping 46.5% malicious web pages seen in vt url feed during our study period.out of the remaining websites (53.5%)  we observe that attackers mostly compromise benign websites (65.6%) to launch their attacks  whereas only 34.4% of malicious websites are hosted on domains created by at- tackers.understandably  public malicious websites exhibit the opposite trend where most (79.5%) are attacker owned.the key insight here is that more has to be done by legitimate domain owners to prevent miscreants from misusing their domains to launch stealthy attacks
46,2021-Data Poisoning Attacks to Deep Learning Based Recommender Systems.pdf,—recommender systems play a crucial role in helping users to ﬁnd their interested information in various web services such as amazon  youtube  and google news.various recom- mender systems  ranging from neighborhood-based  association- rule-based  matrix-factorization-based  to deep learning based  have been developed and deployed in industry.among them  deep learning based recommender systems become increasingly popular due to their superior performance.in this work  we conduct the ﬁrst systematic study on data poisoning attacks to deep learning based recommender systems.an attacker’s goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users.to achieve this goal  our attack injects fake users with carefully crafted ratings to a recommender system.speciﬁcally  we formulate our attack as an optimization problem  such that the injected ratings would maximize the number of normal users to whom the target items are recommended.however  it is challenging to solve the optimization problem because it is a non-convex integer programming problem.to address the challenge  we develop multiple techniques to approximately solve the optimization problem.our experimental results on three real- world datasets  including small and large datasets  show that our attack is effective and outperforms existing attacks.moreover  we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users.our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.i,  in the era of data explosion  people often encounter infor- mation overload problems in their daily lives.for example  when they are shopping online  reading news  listening to music or watching videos  they often face challenges of choosing their interested items from a huge number of candi- dates.recommender systems help people ﬁnd their interested items easily by mining historical user-item interaction data.therefore  recommender systems have been widely used in the real world  which brings huge economic beneﬁts.unlike the non-personalized recommender system that rec- ommends the same items to all users  personalized recom- mender system that we focus in this work uses users’ historical  network and distributed systems security (ndss) symposium ,
47,2021-Discovering and Measuring Malicious URL Redirection Campaigns from Fake News Domains.pdf,—malicious urls are used to distribute malware and launch social engineering attacks.they often hide behind redirection networks to evade detection.due to the difﬁculty in discovering redirection trafﬁc in real-time  previous approaches to understanding redirection networks were reactive and passive.we propose a proactive algorithm that is able to uncover redirec- tion networks in real-time given a small set of seed domains.our method works in three steps: (1) collecting redirection paths  (2) clustering domains that share common nodes along redirection paths  and (3) searching for other domains co-hosted on similar ip addresses.we evaluate our method using real websites that we discovered while auditing 2 300 popular fake news sites.we seeded our algorithm with a subset of 276 fake news domains that redirect  and uncovered three large-scale redirection campaigns.we further veriﬁed that 91% of entry point domains were not new  but recently expired  re-registered  and parked on dedicated hosts.to mitigate this threat vector  we deployed our system to automatically collect newly re-registered domains and publish new redirection networks.during a ﬁve-month period  our threat intelligence reports have received over 50 000 google search impressions  and have been recommended by commercial vendor tools.we also reported ﬁndings to google and amazon web services  both of which have acted promptly to remove malicious artifacts.our work offers a viable approach to continuously discover evasive redirection trafﬁc from re-registered domains.fake news  expired domain  redirection campaign  proactive discovery  index terms—url redirection  domain registration   i,  malicious urls are evasive  short-lived  and usually hide behind redirection networks [1]  [,
48,2021-FARE- Enabling Fine-grained Attack Categorization under Low-quality Labeled Data.pdf,—supervised machine learning classiﬁers have been widely used for attack detection  but their training requires abun- dant high-quality labels.unfortunately  high-quality labels are difﬁcult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers.without such labels  it is challenging to train and deploy targeted countermeasures.in this paper  we propose fare  a clustering method to enable ﬁne-grained attack categorization under low-quality labels.we focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarse- grained labels available for different attack types.the core idea of fare is to take full advantage of the limited labels while using the underlying data distribution to consolidate the low- quality labels.we design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarse- grained labels.we then train an input transformation network to map the input data into a low-dimensional latent space for ﬁne-grained clustering.using two security datasets (android malware and network intrusion traces)  we show that fare signiﬁcantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness.further  we perform an initial deployment of fare by working with a large e-commerce service to detect fraudulent accounts.with real- world a/b tests and manual investigation  we demonstrate the effectiveness of fare to catch previously-unseen frauds.i,  machine learning is widely used to build security appli- cations.many security tasks such as malware detection and abuse/fraud identiﬁcation can be formulated as a supervised classiﬁcation problem [31]  [45]  [64]  [10]  [36]  [19]  [17].by collecting and labeling benign and malicious samples  defenders can train supervised classiﬁers to distinguish attacks from benign data (or distinguish different attack types).a key challenge faced by these supervised classiﬁers is that their training requires abundant high-quality labels.many supervised models  especially deep-learning models  are data- hungry  requiring a large quantity of labeled data to achieve a decent training outcome.in addition  the labels need to have good coverage of all interest.a classiﬁer cannot reliably detect a certain type of attack unless  the attack types of  §equal contribution.network and distributed systems security (ndss) symposium ,
49,"2021-Interactions with Potential Mis_Disinformation URLs Among US Users on Facebook, 2017-2019.pdf", misinformation and disinformation online — and on social media in particular — have become a topic of widespread concern.re- cently  facebook and social science one released a large  unique  privacy-preserving dataset to researchers that contains data on urls shared on facebook in 2017-2019  including how users in- teracted with posts and demographic data from those users.we conduct an exploratory analysis of this data through the lens of mis/disinformation  finding that posts containing potential and known mis/disinformation urls drew substantial user engagement.we also find that older and more politically conservative u.s.users were more likely to be exposed to (and ultimately re-share) poten- tial mis/disinformation  but that those users who were exposed were roughly equally likely to click regardless of demographics.we discuss the implications of our findings for platform interventions and further study towards understanding and reducing the spread of mis/disinformation on social media.ccs concepts • human-centered computing → empirical studies in collab- orative and social computing; • information systems → so- cial networking sites; • security and privacy → social aspects of security and privacy;  keywords misinformation  disinformation  social media acm reference format: aydan bailey  theo gregersen  and franziska roesner.2021.interactions with potential mis/disinformation urls among u.s.users on facebook  2017-2019.in acm sigcomm 2021 workshop on free and open communi- cations on the internet (foci’21)  august 27  2021  virtual event  usa.acm  new york  ny  usa  11 pages.https://doi.org/10.1145/3473604.3474561  1, the last several years have brought significant concerns about the spread and influence of (unintentionally false) misinformation and (intentionally false) disinformation online [,  we collapse gender for readability of the heatmap.) the percentages represent the fraction of views that an average url-containing post received from each demographic group (i.e.for each heatmap  the percentages sum to 100%).figure 3 thus compares how frequently  relative to each other  different overlapping demographic groups were shown posts con- taining urls.for both of our mis/disinformation url subsets  we find that u.s.users who are older and more politically right-leaning were more likely to see misinformation urls in their feeds.the top baseline heatmap  by contrast  shows a larger percentage of views from younger and farther left-leaning users — in other words  the trends for low-quality news and fact-checked false urls were not merely reflections of overall trends of everyone’s facebook usage and/or experience.numbers of views).we also note that the total tpfc false url views for the domains in table 3 were substantial considering the smaller number of tpfc false urls: just dozens of tpfc false urls produced several million views (perhaps by definition  given that viral misinformation is prioritized for facebook’s fact checking).spread over time.we also ask: how quickly did urls in the different subsets spread? in figure 1  we investigate the cumulative views over time  averaged across urls  for each of our subsets.since the dataset provides timestamp information only at month-level granularity  we can track only cumulative monthly views.we find that the average url (in all subsets) spreads most quickly within the first month after the initial post  suggesting that mis/disinformation interventions must be deployed quickly to be effective.from figure 1 we also see that urls in the tpfc false subset received significantly more views  on average  than those in other subsets (hence likely making them targets for fact-checking).in addition  tpfc false urls were longer-lived  receiving a greater fraction of views after the first month compared to the others (nearly doubling in cumulative views).unfortunately  the granularity of the timestamps in the dataset prevent us from investigating whether spread decreased substantially after the time of fact checking.demographic differences in exposure.we now turn to our sec- ond research question  considering different demographic groups (gender  age  and political leaning).while we do not know the over- all demographic breakdown of facebook users in general  we can compare demographics in interactions with our u.s.urls baseline to interactions with our low-quality news and tpfc false url subsets.figure 2 shows the percentage of views from users in different demographic categories for the average url in each url subset.for example  considering the tpfc false url subset  we find that  20  foci’21  august 27  2021  virtual event  usa  aydan bailey  theo gregersen  and franziska roesner  begin by considering clicking behaviors.unlike views  which can result from facebook pushing content to users  clicks are actions taken directly by users (though still predicated on having seen the post in the first place).overall clicking behaviors.figure 4’s set of “clicks” bars shows average click-through rates  i.e.average clicks-per-view.we find that compared to the baseline of all u.s.urls  which users clicked on roughly 6% of the time they see them  urls in our mis/disinfor- mation sets were clicked on more often  roughly 8-9% of the time.in other words  the average u.s.user who saw a potential mis/dis- information url was more likely to click on it than the average user who encountered any random link on facebook.we note that this comparison depends on our choice of baseline  comparing to the “average” u.s.-shared url; the reasons for increased engage- ment with potential mis/disinformation here may result not from mis/disinformation tactics but (for example) because news-style content receives more engagement in general.demographic differences in click-through rate.the heat- maps in figure 5 break down the click-through rates for different demographic groups and url subsets.here we show a baseline click-through rate heatmap on top: the percentages (with confi- dence intervals of 95%) represent the average click-through rate per demographic intersection for all u.s.-shared urls.the next two heatmaps show differences in click-through rate from the baseline: for each demographic intersection  we calculated the ratio of the click-through rate to the baseline click-through rate.(for example  a value of 1 would indicate no change  while a value of 2 would indicate that the click-through rate doubled.)  to our surprise  the demographic trends observed when consid- ering views (above) do not fully hold here.in addition to baseline click-through rate behavior being roughly comparable on both sides of the political spectrum  we see that for the tpfc false and low- quality news urls  differences in click-through rates compared to the baseline were not skewed by political page affinity.that is  despite the demographic differences in who saw these posts  political leaning did not seem to impact on click-through rates for known or potential mis/disinformation urls.we do continue to see differences by age  though not unique to mis/disinformation: older adults were more likely to click on urls in general (top heatmap)  without additional age-related differences for potential mis/disinformation urls.we cannot distinguish why people clicked on links or the impacts of reading the underlying content (e.g.perhaps politically left- leaning users click on right-leaning links to debunk them).still  this finding suggests that political differences in engagement with misinformation reported in prior research may be partially due to who is exposed by the platform in the first place rather than more fundamental differences between groups.moreover  we emphasize that this exposure is not random: facebook’s newsfeed algorithm may optimize showing these posts to precisely those users more likely to click on them  and our findings may reflect the success (or at least uniformity) of facebook’s algorithm in doing just that.still  this result underscores the importance of limiting initial exposure to mis/disinformation  as we discuss further in section 5.figure 3: percentage of url views by demographic groups (age and political page affinity)  for each url subset.per- centages are presented in ranges with 95% confidence intervals due to differential privacy noise.4.3 clicking: who clicked what? when a facebook user saw a post containing a url  what did they do? figure 4 shows a variety of behaviors aggregated across all users in our dataset  for each url subset.in this subsection  we  21  interactions with potential mis/disinformation urls among u.s.users on facebook  foci’21  august 27  2021  virtual event  usa  figure 4: average actions per view for urls in each subset.95% confidence intervals are shown.4.4 sharing: who shared what? we now consider url sharing behaviors.our dataset includes two types of interactions: shares  which involve users re-sharing a facebook post containing a url after clicking on it  and shares without clicks  where users re-share a post containing a url without first clicking on the url.these two metrics are mutually exclusive  meaning that “shares” refers only to shares with clicks.we also emphasize that our interaction data (with associated demographic information) includes only re-shares  not original posts of a url.overall sharing behaviors.figure 4 shows that mis/disinforma- tion urls garnered more shares — and more shares without clicks — per view than average u.s.-shared urls.share rates are particularly high for tpfc false urls.since facebook prioritizes fact-checking viral mis/disinformation  this finding may reflect in part how the tpfc false subset was selected in the first place.however  note that the sharing rates for the low-quality news urls approached the rates for these noteworthy tpfc false urls  i.e.they also drew substantial engagement (per view).we emphasize again that the comparison to baseline depends on our choice of a broad baseline: we see that (these) potential mis/disinformation urls spread more efficiently than other u.s.urls  on average  but we leave to fu- ture work more detailed comparisons against other potential url subsets of interest (e.g.mainstream news).across all url subsets  we were surprised at the high rate of shares without clicks.considering share counts (rather than shares- per-view)  nearly half of all shares  for all url subsets  was done by facebook users who had not clicked on the url they were re-sharing.specifically  the percentage of shares without clicks for the average url was (with 95% confidence intervals): 42.29- 42.35% in the overall u.s.-shared dataset  39.81-40.18% for tpfc false  and 41.66-42.13% for low-quality news.the fact that these ratios are similar across all url subsets suggests that there were not necessarily url or content based differences driving this behavior.demographic differences in sharing urls.we consider the rates at which different groups shared the urls they see.figure 6 is constructed like the click-through rate heatmaps (with a top baseline  and differences from that baseline below)  now considering the average shares-per-view ratio per demographic intersection.here we consider both shares with and without clicks  i.e.all re- shares.note that as with clicks  we cannot distinguish the goals  of a share (e.g.sharing because one agrees with the article or to debunk it).unlike for click-throughs  we again see a trend towards higher sharing rates (given that a user is exposed) of mis/disinformation urls by politically right-leaning users  particularly of tpfc false urls.unlike views  here the impact of political leaning seems to dominate compared to age — younger right-leaning users were also more likely to share these urls compared to baseline.at the same time  we find that older facebook users were slightly more likely to share urls in general  regardless of whether they are mis/disinformation.finally  we investigate specifically what was shared by facebook users in different demographic buckets.figure 7 considers the top 50 most viewed domains from our baseline and low-quality news url subsets  plotting the average age and political leaning of users who shared urls from those domains.we have labeled some example domains  including a cluster of potential mis/disinformation urls shared mostly by older  politically conservative users  as well as examples on the political left.(the tpfc false url subset is omitted from this comparison because there are not enough flagged urls per most-viewed tpfc false domain to reduce noise via averaging sufficiently to allow for meaningful comparison.)  5 discussion reflecting on demographic differences.our results show de- mographic differences in who interacted with potential mis/dis- information on facebook in 2017-2019.most strikingly  we find that older adults with right-leaning u.s.political affinities were more exposed to (i.e.viewed more posts containing) potential mis/disinformation urls.as one caveat  recall that identifying mis/disinformation is challenging — while we relied on reputable sources for our url lists  we cannot exclude the possibility that these lists (and facebook’s own fact-checking) may disproportion- ately include politically conservative content.future work should continue to consider how to identify mis/disinformation and in- vestigate our research questions with those perspectives.future work should also consider additional baseline url subsets (e.g.trustworthy news sources by some definition).also noteworthy is that political demographic differences flat- tened out for click-through rates (though they reappeared for shar- ing rates).in other words  once a user saw a post containing a mis/disinformation url  political leaning did not seem to play a  22  foci’21  august 27  2021  virtual event  usa  aydan bailey  theo gregersen  and franziska roesner  (a) the top heatmap shows baseline click-through rates (clicks per view) by demographic group  for all u.s.-shared urls.(a) the top heatmap shows baseline sharing rates (shares per view) by demographic group  for all u.s.-shared urls.(b) differences in click-through rates from baseline (e.g.tpfc false urls were clicked through by adults age 65+ and ppa=2 1.37-1.45x more often than average u.s.-shared urls).(b) differences in sharing rates from baseline (e.g.average tpfc false urls were shared by adults age 65+ and ppa=2 2.48-2.61x more often than average u.s.-shared urls).figure 5: baseline and difference in click-through rates by demographic groups and url subsets.ranges representing 95% confidence intervals are shown; overlapping ranges do not allow us to distinguish the underlying non-private values.figure 6: baseline and difference in shares/view by demo- graphic groups and url subsets.ranges representing 95% con- fidence intervals are shown; overlapping ranges do not allow us to distinguish the underlying non-private values.23  interactions with potential mis/disinformation urls among u.s.users on facebook  foci’21  august 27  2021  virtual event  usa  figure 7: average demographics for users who shared urls from the top 50 most viewed domains.for both the u.s.-shared urls baseline and the low-quality news url subset  we consider the top 50 most-views domains  and calculate the average demographics (age and political page affinity) for users who re-shared posts containing urls from those domains.several example domains are labeled and 95% confidence intervals are shown.role in whether a user clicked on it.this finding may reflect in part facebook’s success targeting users likely to click  and recall that we do not know why people clicked or whether they took the false or low-quality content at face value.still  this result should caution us not to overestimate the impact of political leaning on how and how often people engage with mis/disinformation without considering the likelihood of being exposed in the first.the fact that we again see demographic differences for shares may result from many reasons that future work might study: one observation we make is that unlike clicks  shares are publicly visible  which may impact behaviors among different demographic groups.at the same time  we see that older adults were more likely to be exposed to (i.e.view)  click on  and re-share potential mis/dis- information than younger adults.this is the case both because older adults were disproportionately likely to be shown posts con- taining these urls  but also because they then exhibited higher click-through and sharing rates on facebook in general.future defenses — either on social media platforms or outside of them (e.g.educational efforts) — should thus look towards reducing suscepti- bility of older adults in particular.comparisons to prior findings  and future research ques- tions.our findings contribute to a broader understanding of how people interact with potential mis/disinformation on social media  based on a unique  large-scale dataset.while smaller-scale and con- trolled studies help explain how or why people interact with “fake news”  larger-scale measurements can shed light on broader trends.most related to our work  prior results from larger-scale studies of facebook and twitter suggested that false news (under various definitions) is shared more by u.s.politically right-leaning users and older adults  though sharing is generally rare [18  20]; that face- book users are more likely to see  click  and share content aligned with their political ideology [2]; that users tend to focus on a small number of news sources [37]; and that low-quality news sources  24  may receive more user engagement than mainstream news [8].our findings confirm and update these prior (often pre-2017) results with data from 2017-2019  and emphasize an important nuance in how we should interpret findings about demographic differences: overall trends can be heavily influenced by what content users are exposed to by the platform in the first place.our findings confirm other recently published results based on this dataset [19]  which used a different list of low-credibility news domains (based on newsguard [31]) but also found that older  more conservative adults were more likely to see and share those urls.since the underlying dataset is not publicly available  we believe that multiple  corroborating peer-reviewed sets of findings are sci- entifically valuable.in addition to a complementary perspective considering different mis/disinformation url subsets  our work also adds an analysis of clicks (in addition to views and shares) and highlights that views should be interpreted as reflecting users’ exposure rather than their direct behaviors.our quantitative results also raise questions for follow-on  more qualitative investigations.most importantly  our results can reveal trends in what facebook users do  but not why or under what conditions they do it.for instance: why (and which types of content) do people reshare so frequently without clicking on the associated urls? why do people click on urls in posts and what is the result of that action — e.g.how often do they click in order to fact-check? how and why do older adults use facebook differently? while our data does not allow us to answer these questions  we emphasize that large-scale dataset releases like this one  despite their limitations  are crucial to allowing researchers to see these trends and formulate such follow-on questions.implications for defenses.platform defenses can aim to prevent mis/disinformation from reaching users (preventing exposure)  or warn them when they encounter it (reducing susceptibility) or when they attempt to share it (reducing spread).our finding that  foci’21  august 27  2021  virtual event  usa  aydan bailey  theo gregersen  and franziska roesner  people across the political spectrum were roughly equally likely to click on (but not necessarily re-share) potential mis/disinformation once exposed suggests that preventing exposure may be crucial.put another way  one might question facebook’s role and responsibility in surfacing these posts to susceptible users in the first place.the fact that people frequently re-share posts without clicking on urls suggests that interventions at sharing time may also be valuable (e.g.a recent twitter change prompting users to click on urls before retweeting them [22]).regarding interventions at the time of a user’s exposure — such as “false news” labels on posts — our dataset unfortunately pro- vides limited insight.because most shares of urls happen in the first month or two of their lifetime on facebook  the month-level granularity of our data prevents us from investigating whether the spread of tpfc-false urls decreases significantly once they have been fact-checked (and thus labeled in users’ feeds).we can confirm from our data that the scope of facebook’s fact-checking efforts is limited (to already viral and clear-cut false cases)  mean- ing that facebook’s fact-checking alone can address only the tip of the iceberg of potential mis/disinformation on the platform.that said  facebook’s methods indeed succeed at fact-checking highly popular mis/disinformation urls with potentially great impact.6 conclusion we conducted an exploratory analysis of a large-scale dataset of urls shared on facebook in 2017-2019  investigating how and how much facebook users were exposed to and interacted with posts containing potential mis/disinformation urls  and how these inter- actions differed across demographic groups.we find that potential mis/disinformation urls received substantial user engagement  particularly from older adults and from u.s.politically-right lean- ing users (though not uniformly)  and add to a rich and growing literature on mis/disinformation on social media.there are many additional questions we could have investigated in this dataset (e.g.considering different url subsets)  and many more questions that this particular dataset cannot answer.we hope that our exploratory analysis provides a foundation for continued investigations.acknowledgements we are very grateful to social science one and to facebook for their partnership in making the facebook url shares dataset available to researchers including ourselves  and to the facebook team for their logistical and technical support working with the data.we also wish to thank sewoong oh for his guidance on working with differentially private data.we thank our anonymous reviewers and our shepherd  drew springall  for their feedback.we are also grateful to scott bailey for reviewing an earlier draft of this paper.we thank eric zeng for providing us with the updated low-quality news url list.other members of the uw security and privacy research lab also helped contribute feedback and analysis ideas.this work was supported in part by a social media and democracy research grant from the social science research council  by the national science foundation under awards cns-1513584 and cns- 1651230  and by the uw center for an informed public and the john s.and james l.knight foundation.25 
50,2021-Knowledge and Capabilities that Non-Expert Users Bring to Phishing Detection.pdf,  phishing emails are scam communications that pretend to be something they are not in order to get people to take actions they otherwise would not.we surveyed a demographically matched sample of 297 people from across the united states and asked them to share their descriptions of a speciﬁc expe- rience with a phishing email.analyzing these experiences  we found that email users’ experiences detecting phishing messages have many properties in common with how it ex- perts identify phishing.we also found that email users bring unique knowledge and valuable capabilities to this identiﬁ- cation process that neither technical controls nor it experts have.we suggest that targeting training toward how to use this uniqueness is likely to improve phishing prevention.1,  email is one of the most commonly used methods of commu- nication  especially in large organizations and for e-commerce.over 3.9 billion people have email accounts  and collectively they send and receive over ,
51,2021-Manipulating the Byzantine- Optimizing Model Poisoning Attacks and Defenses for Federated Learning.pdf,—federated learning (fl) enables many data owners (e.g.mobile devices) to train a joint ml model (e.g.a next- word prediction classiﬁer) without the need of sharing their private training data.however  fl is known to be susceptible to poisoning attacks by malicious participants (e.g.adversary- owned mobile devices) who aim at hampering the accuracy of the jointly trained model through sending malicious inputs during the federated training process.in this paper  we present a generic framework for model poisoning attacks on fl.we show that our framework leads to poisoning attacks that substantially outperform state-of-the-art model poisoning attacks by large margins.for instance  our attacks result in 1.5× to 60× higher reductions in the accuracy of fl models compared to previously discovered poisoning attacks.our work demonstrates that existing byzantine-robust fl algorithms are signiﬁcantly more susceptible to model poisoning than previously thought.motivated by this  we design a de- fense against fl poisoning  called divide-and-conquer (dnc).we demonstrate that dnc outperforms all existing byzantine-robust fl algorithms in defeating model poisoning attacks  speciﬁcally  it is 2.5× to 12× more resilient in our experiments with different datasets and models.i,  federated learning (fl) is an emerging learning paradigm in which many data owners (called clients) collaborate in training a common machine learning model  without sharing their private training data.in this setting  a central server (e.g.a service provider) repeatedly collects some updates that the clients compute using their local private data  aggregates the clients’ updates using an aggregation algorithm (agr)  and ﬁnally uses the aggregated client updates to tune the jointly trained model (called the global model)  which is broadcasted to a subset of the clients at the end of each fl training epoch.while fl has emerged as a promising solution for many in-the-wild learning settings that involve mutually untrusting clients  fl mechanisms are prone to poisoning attacks [31]: malicious clients can attempt to degrade the utility (e.g.model accuracy) of the resulting global model by contributing malicious model updates during the fl training process.a  network and distributed systems security (ndss) symposium ,  we presented a general framework to mount systematic model poisoning attacks on fl.we demonstrated that our framework results in attacks that outperform state-of-the-art poisoning attacks against all byzantine-robust fl algorithms by large margins.we gave concrete reasons for the and strength of our attacks  which future byzantine-robust fl algorithms should address.we also presented a robust aggrega- tion algorithm  called divide-and-conquer  that outperforms all existing robust aggregation algorithms in defeating poisoning attacks on fl.acknowledgements  this work was supported in part by the nsf grant cps-  1739462
52,2021-Phishpedia- A Hybrid Deep Learning Based Approach to Visually Identify Phishing Webpages.pdf,  recent years have seen the development of phishing detec- tion and identiﬁcation approaches to defend against phishing attacks.phishing detection solutions often report binary re- sults  i.e.phishing or not  without any explanation.in con- trast  phishing identiﬁcation approaches identify phishing webpages by visually comparing webpages with predeﬁned legitimate references and report phishing along with its target brand  thereby having explainable results.however  there are technical challenges in visual analyses that limit existing solu- tions from being effective (with high accuracy) and efﬁcient (with low runtime overhead)  to be put to practical use.in this work  we design a hybrid deep learning system  phishpedia  to address two prominent technical challenges in phishing identiﬁcation  i.e.(i) accurate recognition of identity logos on webpage screenshots  and (ii) matching logo variants of the same brand.phishpedia achieves both high accuracy and low runtime overhead.and very importantly  different from common approaches  phishpedia does not require train- ing on any phishing samples.we carry out extensive experi- ments using real phishing data; the results demonstrate that phishpedia signiﬁcantly outperforms baseline identiﬁcation approaches (emd  phishzoo  and logosense) in accurately and efﬁciently identifying phishing pages.we also deployed phishpedia with certstream service and discovered 1 704 new real phishing websites within 30 days  signiﬁcantly more than other solutions; moreover  1 133 of them are not reported by any engines in virustotal.1 ,  phishing  an important step in an attack chain  has evolved over the past years to such an extent that it is now available and delivered as a service [19  49  71].as per recent reports [,
53,2021-PhishPrint- Evading Phishing Detection Crawlers by Prior Profiling.pdf,  security companies often use web crawlers to detect phishing and other social engineering attack websites.we built a novel  scalable  low-cost framework named phishprint to enable the evaluation of such web security crawlers against multiple cloaking attacks.phishprint is unique in that it completely avoids the use of any simulated phishing sites and blocklisting measurements.instead  it uses web pages with benign content to proﬁle security crawlers.we used phishprint to evaluate 23 security crawlers including highly ubiquitous services such as google safe browsing and microsoft outlook e-mail scanners.our 70-day evaluation found several previously unknown cloaking weaknesses across the crawler ecosystem.in particular  we show that all the crawlers’ browsers are either not supporting advanced ﬁngerprinting related web apis (such as canvas api) or are severely lacking in ﬁngerprint diversity thus exposing them to new ﬁngerprinting-based cloaking attacks.we conﬁrmed the practical impact of our ﬁndings by de- ploying 20 evasive phishing web pages that exploit the found weaknesses.18 of the pages managed to survive indeﬁnitely despite aggressive self-reporting of the pages to all crawlers.we conﬁrmed the speciﬁcity of these attack vectors with 1150 volunteers as well as 467k web users.we also proposed countermeasures that all crawlers should take up in terms of both their crawling and reporting infrastructure.we have relayed the found weaknesses to all entities through an elab- orate vulnerability disclosure process that resulted in some remedial actions as well as multiple vulnerability rewards.1,  the web has been seeing an increasing amount of social engineering attacks of late.web-based social engineering attacks such as phishing and malvertisements [45] have been on the rise [14  15  33].url blocklisting services such as google’s safe browsing (gsb) and microsoft’s smartscreen have been working as front-line defenses in protecting the users against these kinds of attacks.most web browsers lookup domain names in these blocklists before proceeding to display the web pages to the users.for example  chrome   firefox  safari  and samsung internet web browsers which together account for about 90% of the market share use the gsb blocklist [3].gsb is deployed in about four billion devices worldwide and shows millions of browser warnings every day protecting users from web attacks.such blocklists are populated with the help of web security crawlers that regularly scout web-pages to evaluate them.however  in order to evade these crawlers  miscreants employ many cloaking techniques [,
54,2021-Practical Blind Membership Inference Attack via Differential Comparisons.pdf,—membership inference (mi) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model  e.g.a deep neural network.there are two types of mi attacks in the literature  i.e.these with and without shadow models.the success of the former heavily depends on the quality of the shadow model  i.e.the transferability between the shadow and the target; the latter  given only blackbox probing access to the target model  cannot make an effective inference of unknowns  compared with mi attacks using shadow models  due to the insufﬁcient number of qualiﬁed samples labeled with ground truth membership information.in this paper  we propose an mi attack  called blindmi  which probes the target model and extracts membership seman- tics via a novel approach  called differential comparison.the high-level idea is that blindmi ﬁrst generates a dataset with nonmembers via transforming existing samples into new samples  and then differentially moves samples from a target dataset to the generated  non-member set in an iterative manner.if the differential move of a sample increases the set distance  blindmi considers the sample as non-member and vice versa.blindmi was evaluated by comparing it with state-of-the- art mi attack algorithms.our evaluation shows that blindmi improves f1-score by nearly 20% when compared to state-of- the-art on some datasets  such as purchase-50 and birds-200  in the blind setting where the adversary does not know the target model’s architecture and the target dataset’s ground truth labels.we also show that blindmi can defeat state-of-the-art defenses.i,  machine learning (ml)  especially deep learning (dl)  has achieved  or even surpassed  human-level performance on many critical areas  such as medical diagnosis [5]  [6]  image and speech recognition [17]  [, contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements  either expressed or implied  of nsf
55,2021-SoK- Still Plenty of Phish in the Sea - A Taxonomy of User-Oriented Phishing Interventions and Avenues for Future Research.pdf,  phishing is a prevalent cyber threat  targeting individuals and organizations alike.previous approaches on anti-phishing measures have started to recognize the role of the user  who  at the center of the target  builds the last line of defense.however  user-oriented phishing interventions are fragmented across a diverse research landscape  which has not been systematized to date.this makes it challenging to gain an overview of the various approaches taken by prior works.in this paper  we present a taxonomy of phishing interven- tions based on a systematic literature analysis.we shed light on the diversity of existing approaches by analyzing them with respect to the intervention type  the addressed phishing attack vector  the time at which the intervention takes place  and the required user interaction.furthermore  we highlight shortcomings and challenges emerging from both our liter- ature sample and prior meta-analyses  and discuss them in the light of current movements in the field of usable security.with this article  we hope to provide useful directions for future works on phishing interventions.1,  phishing is a frequently employed cyber attack to get hold of users’ sensitive information  such as login details or bank- ing account numbers.furthermore  criminals increasingly use phishing attacks to distribute malware [90].the conse- quences of a successful attack can reach from individual per- sonal losses or compromised accounts to complete organiza- tions or networks being infected by malware  often combined  copyright is held by the author/owner.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee.usenix symposium on usable privacy and security (soups) ,
56,2021-SquirRL- Automating Attack Analysis on Blockchain Incentive Mechanisms with Deep Reinforcement Learning.pdf,—incentive mechanisms are central to the function- ality of permissionless blockchains: they incentivize participants to run and secure the underlying consensus protocol.designing incentive-compatible incentive mechanisms is notoriously chal- lenging  however.as a result  most public blockchains today use incentive mechanisms whose security properties are poorly understood and largely untested.in this work  we propose squirrl  a framework for using deep reinforcement learning to analyze attacks on blockchain incentive mechanisms.we demonstrate squirrl’s power by ﬁrst recovering known attacks: (1) the optimal selﬁsh mining attack in bitcoin [52]  and (2) the nash equilibrium in block withholding attacks [16].we also use squirrl to obtain several novel empirical results.first  we discover a counterintuitive ﬂaw in the widely used rushing adversary model when applied to multi-agent markov games with incomplete information.second  we demonstrate that the optimal selﬁsh mining strategy identiﬁed in [52] is actually not a nash equilibrium in the multi-agent selﬁsh mining setting.in fact  our results suggest (but do not prove) that when more than two competing agents engage in selﬁsh mining  there is no proﬁtable nash equilibrium.this is consistent with the lack of observed selﬁsh mining in the wild.third  we ﬁnd a novel attack on a simpliﬁed version of ethereum’s ﬁnalization mechanism  casper the friendly finality gadget (ffg) that allows a strategic agent to amplify her rewards by up to 30%.notably  [10] shows that honest voting is a nash equilibrium in casper ffg; our attack shows that when casper ffg is composed with selﬁsh mining  this is no longer the case.altogether  our experiments demonstrate squirrl’s ﬂexibility and promise as a framework for studying attack settings that have thus far eluded theoretical and empirical understanding.i,  blockchains today require participants to expend substantial resources (storage  computation  electricity) to ensure the correctness and liveness of other users’ transactions.most public blockchains therefore rely critically on incentive mechanisms to motivate users to participate in blockchain consensus protocols.that is  users are typically paid (in native cryptocurrency) to sustain the system.incentive mechanisms are therefore central to the functionality of most permissionless blockchains.for example  bitcoin’s consensus protocol requires partici- pants (also known as miners) to build a sequential data structure  of blocks  where each block is generated in a computationally intensive process (called mining).to incentivize participation  bitcoin miners receive a block reward (in bitcoins) for every block they mine that is accepted by the rest of the network.miners also receive smaller transaction fees for transactions they include in a block; this is done to prevent miners from simply mining empty blocks without doing useful work.the incentive mechanisms of block rewards and fees have driven the remarkable growth of the bitcoin ecosystem.in practice  poorly-designed incentive mechanisms can be exploited by rational users.by deviating from the protocol- speciﬁed behavior  users may be able to accumulate more rewards than what they are nominally entitled to.for example  selﬁsh mining is a well-known attack on bitcoin’s incentive mechanism that allows a strategic miner to reap more than her fair share of block rewards by waiting to publish her blocks until she would cause the most damage to the honest majority [17].many subsequent papers have explored both attacks on bitcoin’s incentive mechanism [11]  [16]  [,.first  notice the non-smoothness in figure 5.this is an artifact of the osm strategies solved in [52].for hash powers between 0.15 and 0.3  osm learns one of multiple strategies that are functionally equivalent in the single-agent setting.due to randomness in policy iteration  the solver may choose different strategies for different hash powers  but they all have the same relative reward [52].in our setting  these choices are not equivalent  and can lead to different rewards for the osm agent.this effect causes the non-smoothness in figure 5 (also see appendix d)  but does not indicate incorrect results.α  -  d r a w e r  e v i t a l e r  α  -  d r a w e r  e v i t a l e r  α  -  d r a w e r  e v i t a l e r  attacker hash power α  attacker hash power α  per-attacker hash power α  fig.5: squirrl gives equal or higher rela- tive rewards compared to osm; however  excess relative rewards can be negative.fig.6: we can alleviate the problems with the rushing adversary (figure 5) by using our more realistic model.fig.7: in the multi-agent setting  we ob- serve no gains from selﬁsh mining in the presence of three parties.second  figure 5 shows a counterintuitive effect: the excess relative rewards of squirrl can be negative; e.g.at hash power 0.2  our agent performs slightly worse than the honest agent.although we cannot guarantee squirrl’s optimality  we observe that even if our agent uses the honest strategy  its rewards are still lower than those of the honest agent.out of the environment poses new challenges  by preventing agents from being able to react to honest actions.in reality  block propagation times are generally much faster than block mining speeds [14]  so strategic miners should have time to react to published blocks before the next block is released.not allowing this makes it difﬁcult to extract excess rewards.this is happening because to model a worst-case rushing adversary  we implemented the honest agent as part of the environment  as is standard in prior work [17]  [41]  [52].even if the strategic agent uses the honest strategy  it is constrained to choose its actions after the honest agent  which leads to diminished rewards.therefore  unlike in the single-agent setting  a rushing adversary in a multi-agent setting can actually perform worse than the honest party!  although this phenomenon may seem like an artifact of our timing model  we ﬁnd that it applies more generally to multi- agent games with incomplete information.consider a game between a  b  and c  where each player must vote for an option in set {0 1}  and agent a (our “honest  agent) always votes randomly.suppose b and c know a’s vote va (i.e.a rushing model)  and suppose c’s ﬁnal reward is equal to the number of votes for the winning option  i.e.maxi∈{0 1} ∑x∈{a b c} 1{vx = i}.now if b employs a strategy where it always votes for 1− va  c’s reward is always 2.on the other hand  if a’s vote is not visible to the other agents  then c’s expected reward under an optimal strategy is strictly larger; with probability 1/4  all agents will choose the same option  giving a reward of 3.hence  in general  a rushing adversary can lead to a strictly lower expected reward for one of the agents.coming back to the selﬁsh mining setting  this suggests that the rushing adversary model we (and others) posed may not be appropriate for multi-strategic-agent settings.an attacker should be able to mimic honest behavior perfectly  but the rushing adversary model does not allow attackers in the multi-agent setting to do that.this observation could be of broader interest to the security community as the empirical and theoretical analysis of multi-agent systems becomes more widespread [24]  [31].a) solutions: our ﬁndings suggest that honest agents should not necessarily be implemented as part of the environ- ment.moving them outside the environment at least allows strategic parties to mimic honest strategies as they are deﬁned within the model.however  simply moving the honest agent  to incorporate both constraints  we make two modeling choices that are a notable departure from prior literature (1) we model the honest party as an agent with a ﬁxed strategy outside the environment (2) instead of assuming a block is mined at every time slot  we have a block mining event every m turns (in our experiments  we let m = 4).the honest party will always act in the turn following a block mining event  giving attackers time to react before the next block is mined.we refer to this as the  time-segmentation model .this changes the pomg: the state space s now includes  time   and so does the observation space ω.furthermore  oi(s)  the observation agent i sees at state s also includes the time.training methodology.the more realistic model introduces a substantial new difﬁculty to the training process: a longer time horizon over which to optimize.with sparser block mining events  the agent must learn to plan.this is a widely- acknowledged difﬁculty in drl [48]  and simply running the default ppo conﬁguration from rllib produces poor results.to combat this difﬁculty  we leverage the existing structure in the problem to modify the training methodology.our modiﬁcations are as follows:  • train for longer: approximately 2m episodes • anneal m from 0 to the desired value of 4  increasing m by 1 every 500k steps.• we want to detect vulnerabilities  so we bias the agents towards selﬁsh mining by adding a bonus of 0.1 · max(2m − total episodes 0)/2m if the agent waits between episode 500k and 1.6m.in general  biasing agents towards dishonest behavior is a good choice when analyzing the security of a system.• at environment initialization  we run osm agents in place of squirrl agents in the game for a random number of block creation events.then we use this leftover state as the initial episode state.it is not necessary to use the osm strategy for initialization; any initialization of states that gives sufﬁcient coverage over all states sufﬁces [1].0.050.100.150.200.250.300.350.400.450.030.020.010.000.010.02squirrl (vs osm) osm (vs osm) 0.10.20.30.40.50.0000.0050.0100.0150.0200.0250.0300.0350.040squirrl (vs osm)osm (vs osm)0.00.10.20.30.40.50.000.010.020.030.042 squirrl agents3 squirrl agentsα  -  d r a w e r  e v i t a l e r  s n o i t c a  ’ h c t a m  ‘  f o  n o i t c a r f  iteration number  fig.8: deviations from the equilibrium strategy are short- lived: squirrl agent 2 chooses to increase its matching proportion at around iteration 100  loses reward immediately  and then returns to the equilibrium strategy.these curves are at 0.1733 hash power for each strategic agent  with the rest given to the honest agent.γi = 1/3 for all strategic agents.• set the discount factor η = 0.997 rather than η = 0.99 to increase the incentive for the agent to plan ahead.• batch size of 1048576 steps.in sections vi-c and vi-d  we demonstrate that we obtain physically realistic results under our new modeling choices  in addition to obtaining other novel results.c.osm is not a nash equilibrium  we apply this new time-segmented  non-rushing model to obtain figure 6.we now observe the more physically realistic result that when agent c is instantiated with squirrl or osm  it always outperforms honest agents  unlike in the previous model.notice that as α → 0.5  the excess relative reward tends to zero because the honest party’s hash power tends to 0  so there is less excess reward to claim.furthermore  in a competition with osm  agent c does better using drl (blue line) than osm (orange line).this implies that osm is not actually a ne.in other words  the approach of [41] to analyze restricted strategy sets is not sufﬁcient.if we had restricted agents to either honest mine or to follow osm  then we might have (incorrectly) concluded from figure 6’s orange line that osm is a ne  similarly to how [41] concludes that semi-selﬁsh mining is a ne.d.selﬁsh mining may be unproﬁtable with k ≥ 3 agents  our next experiments involve training multiple strategic agents against one another in a selﬁsh mining game under the time-segmented  non-rushing model.notice that these experiments can only be run with drl  as the environment is both unknown and dynamic.we highlight one observation from figure 7: with three adaptive strategic agents  the agents could not achieve reward better than honest mining.notice that the training modiﬁcations we detailed in sub- section vi-a all bias our agents to behave more selﬁshly.in  addition  we let γi = 1/3 for all i  the maximum possible follower fraction: however  the equilibrium the agents settle on is honest mining.figure 8 illustrates in solid lines the relative reward of each agent minus its hash power (α = 0.1733) and in dotted lines the fraction of match actions  which is a proxy for the agent’s strategy.matching more often is a more aggressive strategy; the honest strategy never matches.when agents deviate from the honest strategy by matching more (e.g.agent 2 around iteration 100)  they lose reward and quickly revert to an honest strategy.these experiments suggest (but do not prove) that honest mining is a nash equilibrium for k ≥ 3 strategic agents.vii.beyond selfish mining  we have thus far focused on selﬁsh mining attacks.to show the general applicability of squirrl  we apply it to two problems that are not selﬁsh mining: voting-based ﬁnality protocols [9] and block withholding [16].a.casper the friendly finality gadget (ffg)  in this section  we demonstrate a novel attack on the ethereum blockchain’s planned ﬁnalization protocol  called casper the friendly finality gadget (ffg) [9].casper ffg is a proof-of-stake (pos)  voting-based protocol for ﬁnalizing blocks in proof-of-work (pow) blockchains.casper ffg includes an incentive mechanism to ensure that participating nodes  or validators do not deviate from the desired behavior.our goal is to use squirrl to exploit these incentive mechanisms to amplify an agent’s reward and/or subvert the integrity of the voting process.our attack illustrates how an agent can combine pow selﬁsh mining with pos strategic voting to amplify her own rewards.to the best of our knowledge  this is the ﬁrst attack to combine selﬁsh mining with strategic voting in byzantine fault-tolerant (bft)-style protocols.these experiments cannot be solved with value/policy iteration because the state space is continuous.casper ffg validators are meant to ﬁnalize the ﬁrst block (or checkpoint) of every epoch  deﬁned as a chunk of consecutive (cid:96) blocks on the same chain.finalization occurs via voting.when a checkpoint receives more than 2/3 of the votes  it is justiﬁed.here  the votes are weighted by the voters’ deposits.if multiple checkpoints exist at the same height  a validator should vote for the checkpoint on the longer chain.if two consecutive checkpoints on the same chain are justiﬁed  the ﬁrst checkpoint is ﬁnalized and it will remain in the canonical chain forever.a chain that does not include every ﬁnalized checkpoint in the system is considered invalid  even if it is the longest (greatest-work) chain in the system.the casper ffg incentive mechanism is designed to ensure that validators (a) participate in every epoch’s voting protocol  and (b) vote for the same checkpoint if multiple options exist.to achieve this  each validator v makes a deposit dv into a smart contract on the ethereum chain to join the validator pool.roughly  if a checkpoint c at height h(c) is ﬁnalized  then all the voters who voted for c see their deposit grow  whereas any voters who voted for a different checkpoint c(cid:48) with h(c(cid:48)) = h(c) will see their deposit shrink.we implement a simpliﬁed version of the casper ffg incentive mechanism and voting process that captures the essence of the protocol.at any given time step  we model  0204060801001200.040.020.00agent 0 rel.reward - 0204060801001200.020.00agent 1 rel.reward - 0204060801001200.100.050.00agent 2 rel.reward - 0.000.010.020.03agent 0 match proportion0.000.010.02agent 1 match proportion0.0000.0050.0100.015agent 2 match proportionfig.9: total relative reward  including both voting and mining rewards.fig.10: relative reward from casper ffg (not including mining rewards).fig.11: policies of ne vs rl.p1’s hash rate m1 = 0.1  and pi uses hash power xi to inﬁltrate the other pool.the voting process as either active or inactive.if the voting process is inactive  then the actions and state transitions are the same as in selﬁsh mining.if the voting process is active  then the attacker can choose: (1) to vote  in which case it allocates all of its votes to its own fork  (2) to publish blocks  in which case the transitions mimic those in the selﬁsh mining setting  or (3) to wait and continue mining without taking any publicly-facing actions.while the attacker is waiting  honest validators can vote for a checkpoint.in practice  validators will not vote simultaneously; we model this heterogeneity by staggering the honest votes according to a random process.in a given time slot  with probability pvote the honest parties randomly choose a longest chain to vote for.we allocate to this longest chain a proportion of honest votes equal to min(max(x 0) 1− β− vh)  where x ∼ n (0.1 0.05)  vh is the total proportion of votes already allocated by the honest parties  and β is the proportion of total votes available that are under the control of the attacker.in other words  we choose a random fraction of the remaining  uncast honest votes and allocate them to the checkpoint on the current longest chain.the random vote allocation distribution reﬂects the non-uniformity of block propagation in the blockchain.with probability 1− pvote  a block is generated by a miner and the block structure changes according to the selﬁsh mining setting.the voting process is active at the beginning of any epoch.it becomes inactive if (1) one checkpoint receives more than 2/3 of the votes (2) one chain containing a checkpoint becomes the canonical chain through selﬁsh mining transitions—for instance  if the attacker chooses to adopt the honest chain.block rewards are calculated as follows: miners get one unit of reward for every block that ends up on the canonical chain.voting rewards are calculated (roughly) as in casper ffg  with the validator deposit dv scaled appropriately to reﬂect the ratio of real-world deposit magnitudes to block rewards.the differences between our modeling assumptions and the full scheme are detailed in appendix e.results.as the agent’s voting and mining power increases  squirrl learns to exploit the penalty for incorrect voting to penalize the honest party.a common attack strategy discovered by squirrl is as follows.first  the squirrl agent accumulates and hides two checkpoints  c(cid:48) → c(cid:48)(cid:48)  through selﬁsh mining.when the honest party releases a checkpoint c  the agent immediately releases c(cid:48) and triggers a competing voting process.the agent then waits until the honest checkpoint c accumulates close to (but not above) 1/3 votes.the agent then releases c(cid:48)(cid:48)  causing checkpoint c(cid:48) to be included in a longer chain than c.the remaining voters will vote for c(cid:48) according to the voting rule and c(cid:48) will be justiﬁed.the honest voters for c are penalized  amplifying the agent’s relative reward.figure 9 shows the total relative rewards accumulated by an agent with the same fraction in mining hash power and voting pool deposit.we vary this fraction up to 1/3 because casper ffg is not secure above 1/3 adversarial voting power  but honest voting is shown to be a nash equilibrium for agents with < 1/3 voting power in [10].1 we observe that this attack allows the agent to amplify its rewards more than selﬁsh mining alone.figure 10 illustrates dramatic gains in relative voting reward  up to 30% over honest rewards.this attack raises an important practical concern.the interest rate associated with voting rewards in casper is close to extrinsic interest rates (e.g.the stock market).hence  if an adversary is able to drive down an honest participant’s rewards  honest voters may leave the voting pool  making it easier for an adversary to control more than 1/3 of the voting power.hence  this attack can actually affect the integrity of the ﬁnalization mechanism itself.an important implication of this case study is that system designers should consider how incentive mechanisms compose with other incentive mechanisms.that is  casper ffg in isolation is not vulnerable to these attacks.it is only by combining mining incentives with the casper ffg voting protocol that we observe this vulnerability.b.block withholding attacks  we explore a second case study in which agents perform block withholding attacks [16]  [23]  [33]  [38]  [51]  an attack observed in practice (see  e.g.[64]).in block withholding attacks  a mining pool inﬁltrates miners into opponent pools to diminish their revenue and gain a competitive advantage.the attacking pool deploys mining resources in a target pool and submits partial solutions  i.e.proofs of work  to earn rewards.if the attacking pool mines a block in the target pool  it withholds it.the target pool thus loses block rewards and revenue relative to its hash power declines.1this result considers casper ffg voters in isolation  without accounting  for the possibility of an agent’s concurrent mining activity.0.0000.0330.0660.0990.1320.1650.1980.2310.2640.2970.330attacker s mining power  / voting power 0.000.050.100.150.200.250.300.350.40relative rewardhonestsm1+honest votingosm+honest votingsquirrl0.260.280.300.320.340.260.280.300.320.340.360.380.0000.0330.0660.0990.1320.1650.1980.2310.2640.2970.330attacker s mining power  / voting power 0.00.10.20.30.4voting reward fractionhonestsm1+honest votingosm+honest votingsquirrl0.10.20.30.40.50.60.70.8p2 s hash power m20.0000.0250.0500.0750.1000.1250.1500.1750.200infiltrating hashraterl-x1ne-x1rl-x2ne-x2in prior work  eyal [16] showed that for two competing mining pools  there is a (unique) nash equilibrium where each pool assigns a fraction of its resources to inﬁltrate and sabotage the other.squirrl automatically learns pool strategies that converge to the same revenues as predicted by that equilibrium.we adopt the same model as in eyal [16].in the two- party version of this model  strategic mining pools p1 and p2 each possess “loyal” miners with hash rates of m1 and m2  respectively  0 ≤ m1 + m2 ≤ 1.the remaining miners mine on their own  not forming or joining a pool.a miner loyal to pool pi may either mine honestly in pi or inﬁltrate p3−i  as dictated by pi.when an inﬁltrating miner loyal to pi generates a partial block reward  the reward is relayed to pi and split among all registered miners in pi  as well as the miners who are loyal to pi but currently inﬁltrating p3−i.the goal is to maximize the revenue of each miner  normalized by the revenue when there is no block withholding attack.denote the hash power of miners loyal to pi and inﬁltrating p3−i by xi.thus 0 ≤ xi ≤ mi.we set up the two-agent rl experiment using the reward functions deﬁned in [16].each agent is assigned a mining hash power mi and aims to maximize its reward by choosing xi  the hash power inﬁltrated into the other pool  from a continuous action space [0 mi].the reward to be optimized is the immediate normalized revenue (again  as deﬁned in [16]).there is no state transition in this environment: the game has episode length 1.two agents take turns to adapt their strategies given the best strategy the other agent learned in the last episode.we trained the model using ppo because it is suitable for the multi-agent setting  as mentioned before  and supports continuous action spaces.for reproducibility  our setting of hyperparameters in ppo and training results are speciﬁed as follows.we set the clipping parameter ε to 0.1  with a linear learning rate schedule decaying from 10−5 to 10−7.the entropy coefﬁcient β is set to 0.01 initially and decays to β ← β(1 − timestep/schedule) every training step  with schedule set to 109.after 106 episodes  both the strategies and rewards converge to those in the nash equilibrium speciﬁed in [16]  to within 0.01.the detailed policies and revenues are plotted in ﬁgs.11 and 12 respectively.related work in [23] uses rl  speciﬁcally a policy gradient based learning method [7]  to study block withholding among multiple agents in a setting with dynamic hash rates.a limitation of that work is that it uses a discrete action space  not a continuous one.one interesting feature is its inclusion of a probabilistic model of migration of unafﬁliated (free agent) miners to the most successful pools  an extension of the model in [33].unfortunately  this model is rather artiﬁcial  with no grounding in empirical study  so we chose not to duplicate it.viii.related work  a number of recent works have analyzed direct attacks on  and economic ﬂaws in cryptocurrency protocols.(1) selﬁsh mining.the concept of selﬁsh mining was introduced by eyal and sirer in [17]  and a large body of resulting work has sought to reﬁne related mining models and compute protocol security thresholds in a selﬁsh mining context [44]  [47]  [50]  [52].much of this work (including [52]) uses mdp solving to compute optimal selﬁsh mining strategies.these exact solutions are less robust to unexpected  real-time changes in honest hashpower than our rl-based approach.an enhanced model with two selﬁsh agents and one honest agent is considered in [3]  but this work does not consider the presence of multiple rational actors in the network  which is more realistic for a cryptocurrency mining setting.our techniques allow for reasoning about more realistic models at the expense of theoretical guarantees.(2) general mining attacks.a wide range of work has also focused on potential mining attacks besides selﬁsh mining.one example is difﬁculty attacks [21]  [43]  in which miners can proﬁtably manipulate a chain’s difﬁculty by secretly raising difﬁculty on their own private chain [2]  switching between com- peting currencies secured by the same mining hardware [43]  or pausing mining activities around difﬁculty adjustment time [21].in some situations  this can discourage miners from mining at all [28].the study of inducing discouragement in our framework is left to future work.attacks involving miner manipulation of user transactions  via censorship or reordering  are surveyed in [27].many such attacks have been shown in theory  allowing miners to double-spend user funds or proﬁt from incorrect assumptions in second-layer applications [6]  [36]  [42].such attacks have been observed in practice [13]  [15] and can be performed without hashpower by bribing existing miners [42]  [65].because these attacks yield direct revenue for miners  they can almost certainly be used to subsidize a successful mining attack as described in our work  lowering the required hashpower threshold.lastly  attacks against miners are possible at the network layer.one example is dos attacks on mining pools  which have been observed in practice [67] and which more often affect larger pools [26].another is eclipse attacks [12]  [58]  [59]  which can ensure a node is connected to only attackers and is applied to blockchain systems in [25]  [40].it has shown that such attacks can interact with selﬁsh mining to increase efﬁcacy [44]  and routing-based eclipse attacks have been observed in blockchains [61].(3) rl  mdps  and computer security.our work focuses in part on analyzing multi-agent games with drl.a connection between mdps and mult-agent rl was made in a seminal paper by littman [37]  which proposes the use of rl to extend mdp analysis to multi-agent games.recent work (e.g.[55]  [69]  [76]) has applied this technique to cybersecurity actors  analyzing meta-games between attackers and defenders.our work extends this style of analysis into a setting where multiple “attackers  compete to maximize their own proﬁt share.unlike in traditional security  our setting is not mutually exclusive (multiple attackers can proﬁt)  and requires attackers to continually respond to each others’ actions.this greatly increases strategy space complexity in a way likely inherent to the open participation of most cryptocurrency protocols.ix.conclusion  in this work  we propose squirrl  a deep rl-based frame- work to automate vulnerability detection in blockchain incentive mechanisms.we have shown that squirrl can approximate known theoretical results regarding attacks on blockchain incentive mechanisms.it can also handle settings that are  intractable using classical techniques like policy iteration  such as multiple competing agents or continuous state spaces.[20]  squirrl cannot prove the security of a mechanism.we have shown  however  how squirrl can serve as a powerful “quick- and-dirty  tool  allowing protocol designers to gain intuition about their protocols in cases where theoretical analysis is infeasible.we also believe that future work will illuminate new uses  including other classes of incentive-based attacks  e.g.time-bandit attacks [13]
57,2021-To Err.Is Human- Characterizing the Threat of Unintended URLs in Social Media.pdf,—to make their services more user friendly  online so- cial media platforms automatically identify text that corresponds to urls and render it as clickable links.in this paper  we show that the techniques used by such services to recognize urls are often too permissive and can result in unintended urls being displayed in social network messages.among others  we show that popular platforms (such as twitter) will render text as a clickable url if a user forgets a space after a full stop at the end of a sentence  and the ﬁrst word of the next sentence happens to be a valid top level domain.attackers can take advantage of these unintended urls by registering the corresponding domains and exposing millions of twitter users to arbitrary malicious content.to characterize the threat that unintended urls pose to social media users  we perform a large-scale study of unintended urls in tweets over a period of 7 months.by designing a classiﬁer capable of differentiating between intended and unintended urls posted in tweets  we ﬁnd more than 26k unintended urls posted by accounts with tens of millions of followers.as part of our study  we also register 45 unintended domains and quantify the trafﬁc that attackers can get by merely registering the right domains at the right time.finally  due to the severity of our ﬁndings  we propose a lightweight browser extension which can  on the ﬂy  analyze the tweets that users compose and alert them of potentially unintended urls and raise a warning  allowing users to ﬁx their mistake before the tweet is posted.i,  social media platforms  like twitter  facebook  and linkedin are increasingly becoming the main way in which people obtain news and communicate with the rest of the world.twitter  as one of the most popular of these platforms  was shown to be able to shape political campaigns [,
58,2021-Trojaning Attack on Neural Networks.pdf, with the fast spread of machine learning techniques  sharing and adopting public machine learning models become very popular.this gives attackers many new opportunities.in this paper  we pro- pose a trojaning attack on neuron networks.as the models are not intuitive for human to understand  the attack features stealthiness.deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driv- ing).we first inverse the neuron network to generate a general trojan trigger  and then retrain the model with external datasets to inject malicious behaviors to the model.the malicious behaviors are only activated by inputs stamped with the trojan trigger.in our attack  we do not need to tamper with the original training process  which usually takes weeks to months.instead  it takes minutes to hours to apply our attack.also  we do not require the datasets that are used to train the model.in practice  the datasets are usually not shared due to privacy or copyright concerns.we use five different applications to demonstrate the power of our attack  and perform a deep analysis on the possible factors that affect the attack.the results show that our attack is highly effective and efficient.the trojaned behaviors can be successfully triggered (with nearly 100% possibility) without affecting its test accuracy for normal input data.also  it only takes a small amount of time to attack a complex neuron network model.in the end  we also discuss possible defense against such attacks.1, we are entering the era of artificial intelligence (ai).neural net- works (nn) are one of the most widely used ai approaches.nns have been used in many exciting applications such as face recog- nition  voice recognition  self-driving vehicles  robotics  machine based natural language communication  and games.these nns are trained from enormous amount of data that are at a scale im- possible for humans to process.as a result  they have superseded humans in many areas.for example  alphago had defeated human world champions in go games.in the foreseeable future  ais (i.e.well-trained models) will become consumer products just like our everyday commodities.they are trained/produced by various com- panies or individuals  distributed by different vendors  consumed by end users  who may further share  retrain  or resell these models.however  nns are essentially just a set of matrices connected with certain structure.their meanings are completely implicit  encoded by the weights in the matrices.it is highly difficult  if not impossible  to reason about or explain decisions made by a nn [,
59,2021-Why They Ignore English Emails- The Challenges of Non-Native Speakers in Identifying Phishing Emails.pdf,  prior work in cybersecurity and risk management has shown that non-native speakers of the language used in phishing emails are more susceptible to such attacks.despite much research on behaviors english speakers use to avoid phishing attacks  little is known about behaviors of non-native speak- ers.therefore  we conducted an online survey with 862 non- native english speakers (284 germans  276 south koreans  and 302 japanese).our ﬁndings show that participants  espe- cially those who lacked conﬁdence in english  had a higher tendency to ignore english emails without careful inspection than emails in their native languages.furthermore  both the german and south korean participants generally followed the instructions in the email in their native languages without careful inspection.finally  our qualitative analysis revealed ﬁve main factors that formed the participants’ concerns in identifying english phishing emails.these ﬁndings highlight the importance of providing non-native speakers with speciﬁc anti-phishing interventions that differ from those for native speakers.1,  phishing is a form of online fraud that acquires such sensitive information as account credentials and credit card information by masquerading as a legitimate business or reputable person.since the mid-90s  an increasing body of research in the ﬁelds of cybersecurity and risk management has led to the develop- ment of techniques to combat phishing [15  56].however  it remains a huge cybersecurity threat [64].it is noteworthy that  copyright is held by the author/owner.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee.usenix symposium on usable privacy and security (soups) ,
60,2022-Inferring Phishing Intention via Webpage Appearance and Dynamics- A Deep Vision Based Approach.pdf,  1 ,  explainable phishing detection approaches are usually based on references  i.e.they compare a suspicious web- page against a reference list of commonly targeted legitimate brands’ webpages.if a webpage is detected as similar to any referenced website but their domains are not aligned  a phish- ing alert is raised with an explanation comprising its targeted brand.in comparison to other techniques  such explainable reference-based solutions are more robust to ever-changing phishing webpages.however  the webpage similarity is still measured by representations conveying only partial intentions (e.g.screenshot and logo)  which (i) incurs considerable false positives and (ii) gives an adversary opportunities to compro- mise user conﬁdence in the approaches.in this work  we propose  phishintention  to extract precise phishing intention of a webpage by visually (i) extracting its brand intention and credential-taking intention  and (ii) inter- acting with the webpage to conﬁrm the credential-taking in- tention.we design phishintention as a heterogeneous system of deep learning vision models  overcoming various technical challenges.the models “look at” and “interact with” the web- page for its intention  which are robust to potential html obfuscation.we compare phishintention with four state-of- the-art reference-based approaches on the largest phishing identiﬁcation dataset consisting of 50k phishing and benign webpages.for similar level of recall  phishintention achieves signiﬁcantly higher precision than the baselines.moreover  we conduct a continuous ﬁeld study on the internet for two months to discover emerging phishing webpages.phishin- tention detects 1 94,  we proposed phishintention to identify and explain phishing webpages through static and dynamic analysis.the heteroge- neous solution extracts brand and credential-taking intentions of phishing webpages.our experiments on a large dataset and ﬁeld study demonstrated the effectiveness of phishintention over existing solutions.in the future  we will apply phishin- tention into an online phishing monitoring system to collect active phishing kits  and study their runtime behaviors with various program analysis techniques [42–44  65].11 discussions  acknowledgement  diversiﬁed intention.the intention of a webpage to require credential can be more diversiﬁed than those considered in this work.for example  some phishing webpages can use a new form of ui  (e.g.qr code)  to retrieve credentials.thus  a more enriched intention detector is required in the future.diversiﬁed interaction model.our approach requires con-  we thank the anonymous reviewers who help us to improve this work.this research is supported by the national re- search foundation  prime minister’s ofﬁce  singapore un- der its corporate laboratory@university scheme  national university of singapore  and singapore telecommunications ltd  the minister of education  singapore (t2ep20120-0019   1646    31st usenix security symposium  usenix association  475677111667614245310420810200000400000600000emdphishzoovisualphishnetphishpediaphishintention# reported phishing3013156869383101147363105001000emdphishzoovisualphishnetphishpediaphishintention# real phishing in top1k# phishing not reported by virustotal in top1k1813129258211012118t1-251res1901  moet32020-0004)  the national research foundation singapore through its national satellite of ex- cellence in trustworthy software systems ofﬁce (nsoe- tss2019-05).[16] ahmet selman bozkir and murat aydos.logosense: a companion hog based logo detection scheme for phish- ing web page and e-mail brand recognition.computers & security  95:101855  2020
61,2022-Phishing in Organizations- Findings from a Large-Scale and Long-Term Study.pdf,—in this paper  we present ﬁndings from a large- scale and long-term phishing experiment that we conducted in collaboration with a partner company.our experiment ran for 15 months during which time more than 14 000 study participants (employees of the company) received different simulated phishing emails in their normal working context.we also deployed a reporting button to the company’s email client which allowed the participants to report suspicious emails they received.we measured click rates for phishing emails  dangerous actions such as submitting credentials  and reported suspicious emails.the results of our experiment provide three types of contri- butions.first  some of our ﬁndings support previous literature with improved ecological validity.one example of such results is good effectiveness of warnings on emails.second  some of our re- sults contradict prior literature and common industry practices.surprisingly  we ﬁnd that embedded training during simulated phishing exercises  as commonly deployed in the industry today  does not make employees more resilient to phishing  but instead it can have unexpected side effects that can make employees even more susceptible to phishing.and third  we report new ﬁndings.in particular  we are the ﬁrst to demonstrate that using the employees as a collective phishing detection mechanism is practical in large organizations.our results show that such crowd-sourcing allows fast detection of new phishing campaigns  the operational load for the organization is acceptable  and the employees remain active over long periods of time.i,  phishing remains a major problems on the internet [1].deceptive emails that trick users to perform unsafe actions are getting increasingly sophisticated [, of previous academic studies and com- mon industry practices.in particular  we ﬁnd that embedded phishing training  as commonly used in the industry today  can lead to unexpected side effects and even be detrimental to phishing prevention.this is a signiﬁcant ﬁnding  due to wide use of this practice in the industry.and third  our results provide new insights to phishing in organizations.in particular  as one of the main contributions of this paper  our experiment is the ﬁrst to demonstrate that crowd-sourced phishing detection can be effective  fast  and sustainable over long periods of time.during our experiment  the employees reported thousands of suspicious emails which represented hundreds of real and previously unseen phishing campaigns.the reporting speed of our simulated phishing emails indicates that new campaigns can be detected within few minutes from their launch.we designed a simple process- ing pipeline that combined automated and manual analysis for the reported emails.our experiment shows that through such techniques  the reported emails can be made low even in large organizations.our experiment also demonstrates that large employee bases can collectively retain sufﬁciently high reporting rates over long periods of time.in summary  this paper is the ﬁrst to demonstrate that crowd-sourced phishing detection is a practical and effective option for many organizations.load of handling all  the operational  to summarize  this paper makes the following contributions: 1) extensive measurement study on human factors of phish-  ing and phishing prevention in large organizations.2) supportive results for several previous research ﬁndings  with improved ecological validity.3) contradicting ﬁndings that challenge the conclusions of previous research studies and popular industry practices.4) large-scale evaluation of crowd-sourced phishing re- porting that shows fast detection  small operational overhead  and sustained employee reporting activity.paper outline.this paper is organized as follows.in sec- tion ii  we deﬁne our research questions and provide an overview of our ﬁndings.we describe our experimental setup in section iii.we report results related to employee demo- graphics in section iv.section v shows how phishing vulner- ability evolved over time in our study.section vi explains our results related to warnings and embedded training.section vii analyzes crowd-sourced phishing detection.in section viii we discuss validity of our study.section ix reviews related work and section x concludes the paper.ii.research questions and main findings  in this section  we ﬁrst deﬁne the research questions that our study was designed to answer and then provide a summary of our main ﬁndings  both summarized in table i.a.research questions  rq1: which employees fall for phishing? the ﬁrst goal of our experiment was to understand which employees in a large organization are the most likely to fall for phishing.in particu- lar  we wanted to understand how employee characteristics that are easily available to organizations  such as age  gender  and the assumed level of computer use in one’s job type  correlate with phishing susceptibility.rq2: how does organization’s vulnerability to phishing evolve over time? the second goal of our experiment was to understand how the continued presence of phishing affects organizations over time.we examine topics like how large is the fraction of the employee base that will eventually fall for phishing  and how many individuals repeatedly fall for phishing [42].rq3: how effective are phishing warnings and training? our third goal was to understand how large organizations can help their employees to recognize phishing emails and thus defend themselves against phishing.today  organizations can choose from a range of tools and educational measures de- signed for this purpose.in our study  we focused on evaluating tools that can be deployed to a large employee base with a moderate cost  as such tools are commonly used in practice.the ﬁrst tool whose effectiveness we decided to examine was warnings on top of suspicious emails.warnings are used in many popular email clients and services such as gmail [43]: they are shown on top of the emails where automated phishing detection mechanism has identiﬁed some risky or suspicious features in the email  but it could not label the email as phishing with sufﬁciently high conﬁdence (often email ﬁlters are tuned to be permissive to avoid too many false positives).the second tool we wanted to test was simulated phishing exercises [32]  [11] in combination with embedded train- ing [33].during the last decade  simulated phishing exercises have become a common industry practice [18]  [19]  [17]  [20].in a simulated phishing exercise  the organization sends emails that mimic real phishing emails to their employees and then track which employees perform unsafe actions such as clicking links or disclosing credentials to a web page.often such exercises are combined with embedded training (sometimes also called contextual training)  where employees that fail the exercise (e.g.by clicking on a link or disclosing their credentials) are forwarded to an information resource like a web page that provides educational material about phishing.rq4: can employees help the organization in phishing de- tection? the fourth goal of our experiment was to understand if a large employee base can collectively help the organization in phishing prevention.more precisely  we wanted to under- stand whether using employees as a crowd-sourced phishing  table i: summary of our research questions and main results that include ﬁndings that support prior literature  ﬁndings that contradict previous studies  and new insights.the two most signiﬁcant contributions of this paper are marked in bold.findings studies in literature  that  support previous  findings that contradict previous studies in literature  new ﬁndings on phishing in organi- zations  rq1: which employees fall for phishing? (section iv)  age and computer skills correlate with phishing susceptibility [40]  [41]  [34]  [36]  [35]  [29]  [11]  [26]  gender does not correlate with phishing susceptibility (contradicts [36]  [30]  [40]  [41])  type of computer use is more predic- tive for phishing vulnerability than amount of computer use  rq2: how does organization’s vulnerability to phishing evolve over time? (section v)  rq3: how effective are phishing warnings and training? (section vi)  rq4: can employees help the organization in phishing detection? (section vii)  there are several “repeated clickers” in a large organization [42]  many employees will eventually fall for phishing if continuously exposed  warnings on top of suspicious emails are effective [38]  [37]  [39]  voluntary embedded training in simulated phishing exercises is not effective (contradicts [33]  [32]  [34])  more detailed warnings are not more effective than simple ones  crowd-sourcing phishing email de- tection is both effective and feasi- ble  detection mechanism is efﬁcient (can phishing campaigns be detected fast enough?)  practical (does the administrative load of processing the reported emails remain acceptable?)  and sustainable (will employees continue to report emails over time?) in a large organization.additionally  our aim was to understand if the presence of a feedback mechanism encourages employees to report suspicious emails more.b.summary of main findings  next  we provide an overview of the main ﬁndings of our experiment  and brieﬂy discuss how these results relate to prior research literature (a more detailed survey of related work is given in section ix).our ﬁndings support claims of previous studies with improved ecological validity; contradict prior conclusions and common industry practices; and provide new insights related to phishing in large organizations.findings related to rq1.the results of our experiment support previous work which has showed that age [40]  [41]  [34]  [36]  [35]  [29] and computer skills [11]  [26] both correlate with phishing vulnerability.similar to previous studies  we also ﬁnd that older and younger employees are more at risk  as well as people with lower computer skills.our experiment improves the ecological validity of these studies which were either smaller [36]  [35]  [30]  [34]  [29]  [26]  shorter in duration [36]  [35]  [26]  featured populations with less diversity (e.g.mostly university students and employ- ees [41]  [34]  [29]  skewed in age [35])  or featured role- play or quiz-style studies only [40]  [41]  [29].opposed to previous literature [36]  [30]  [40]  [41]  we do not ﬁnd gender to correlate with phishing susceptibility.the correlation that we observe is explained much better by skewed distribution of the different types of jobs among genders.we improve on these studies by reporting on a larger and more diverse population in their day-to-day job environment.as a new ﬁnding  our study shows that the most vulnerable employees are those who use computers daily for repetitive  task with a specialized software only  rather than those em- ployees who do not need computers in their day-to-day job.that is  in our experiment  the type of computer use is more predictive for phishing vulnerability than the amount.we discuss these topics more in section iv.findings related to rq2.similar to previous studies  we ﬁnd several “repeated clickers” who fail simulated phishing exercises multiple times [42].we also ﬁnd that if exposure to phishing continues in an organization  eventually a signiﬁcant fraction of employees will fall for phishing.we elaborate on these results in section v.findings related to rq3.our results support the previous studies that ﬁnd contextual warnings effective [38]  [37]  [39] and the common industry practice of using such warnings [43].we improve these studies thanks to a larger  more general population with a rigorous control group.interestingly  contradicting prior research results [33]  [32]  [34] and a common industry practice [19]  [17]  [20]  [18]  we found that the combination of simulated phishing exercises and voluntary embedded training (i.e.employees were not required to complete the training) not only failed to improve employee’s phishing resilience  but it actually even the made employees more susceptible to phishing.compared to our experiment  previous studies featured less participants [33]  [31]  [28]  [16]  [32]  [27]  were shorter in time [16]  [33]  [27]  had populations with little diversity [31]  [28]  [27] or tested a role-playing setting only [32]  [31].our results suggest caution in the design of embedded training: we discuss the possible reasons (such as false sense of corporate it security) and practical implications of this somewhat surprising and non-intuitive ﬁnding at length in section vi.another novel ﬁnding of our study is that adding more details to contextual warnings (e.g.explaining the reasons the email was ﬂagged as suspicious) does not reduce phishing effectiveness signiﬁcantly.fig.1: overview of the measurement infrastructure that we deployed in the partner company.findings related to rq4.one of the main contributions of this paper is that we demonstrate experimentally that crowd- sourced phishing detection can be efﬁcient and sustainable in large organizations.the idea of crowd-sourcing phishing detection to employees has been suggested in previous pa- pers [25]  [23].our contribution is that we are the ﬁrst to evaluate this idea over a long period of time in the context of a real large organization.1 our experiment shows that crowd- sourced phishing detection enables organizations to detect a large number of previously unseen real phishing campaigns with a short delay from the start of the campaign.the process- ing pipeline that we developed as part of our experiment also shows that the operational load of phishing report processing can be kept small  even in large organizations.our study also demonstrates that a sufﬁciently high number of employees report suspicious emails actively over long periods of time.in summary  we show that crowd-sourced phishing detection provides a viable option for many organizations.section vii provides full discussion of this topic.iii.experimental setup  in this section we explain how we performed this study in  collaboration with a partner company.a.study organization  partner company.for this study  we collaborated with a company which employs more than 56 000 people of diverse technical skills  age groups  and jobs.our partner company is a large public company  dealing in logistics  ﬁnance  transport  and it services.they employ people with different duties: ﬁeld workers  branch workers that work in front-end stores in contact with the general public  and ofﬁce workers of different qualiﬁcations  from it to marketing and accounting.1phishing reporting by users is a widely-used industry practice.for ex- ample  there are service providers who aggregate data from many of their business customers [24]  [19] and large email providers who collect reports to feed machine learning models [44].however  prior to our work  it has not been publicly evaluated whether the employee base of a single organization can be effectively leveraged as a phishing detection mechanism.as is common industry practice [19]  [17]  [20]  [18]  at the time of study planning our partner company was already run- ning a phishing awareness campaign which included simulated phishing emails and contextual (embedded) training.our role.for this study  we leveraged the already existing phishing awareness campaign as a testbed for our research questions.more precisely  we collaborated with our partner company in two ways.first  as a scientiﬁc advisor who helped in the design of the experiment.by deploying different tools and conditions to different employees  we were able to use the existing campaign to address our research questions.at the end of the study  we received anonymized data from the company in bulk and analyzed it.second  we took an active part by administering a questionnaire to randomly-selected employees and analyzing the responses.company’s role.the role of the company was three-fold: it designed all the simulated phishing emails; provided the infrastructure for sending the simulated phishing emails and measuring dangerous actions such as clicks; and hosted the embedded training resources (an educational webpage that was shown to those employees who performed a dangerous action).the company had a pre-existing collaboration with an external service provider that specializes in phishing awareness and education.this service provider assisted the company in phishing email and contextual training page design.the study was initiated and approved by the ciso of the company.b.measurement infrastructure  phishing exercise component.our partner company deployed a phishing exercise component  shown as  in figure 1  im- plemented by the service provider who specializes in phishing awareness and training  that sent simulated phishing emails  crafted by human experts.these emails could either link to a deceptive website (hosted by component ) or have a malicious ﬁle attached  with the goal of deceiving the participant to do a dangerous action  such as submitting their credentials or enabling macros on an attachment.(a) short warnings.fig.3: header of the contextual training awareness web page that was displayed after falling for a simulated phishing email.(b) detailed warnings.fig.2: warnings that we added to selected participants’ email clients on top of simulated phishing emails.deployed warnings.based on our recommendation  the com- pany deployed two types of warnings  that could be triggered to appear on top of the simulated phishing emails on the employees’ email client (outlook).as a baseline  we deployed short warnings (figure 2a)  visually identical to the standard outlook warnings that employees are used to  containing a similar generic sentence  warning the recipient to be careful because the email “looks suspicious”.we also developed and deployed detailed warnings  shown in figure 2b  again visually identical to outlook warnings  but adding a list of reasons why the email might be suspicious  e.g.mismatches between the email of the sender and the displayed name  or mismatches between the displayed link and the pointed domain.such information could be generated automatically in a deployment that adds warnings to emails that seem suspicious  when there is not sufﬁcient certainty to block the email.deployed training.the phishing exercise component also hosted a training web page on phishing  shown after some- one performed the dangerous action of a simulated phishing email.this internal corporate web page (part of it shown in figure 3) explained to the employee what happened in detail (i.e.that they failed a phishing exercise from their organization)  speciﬁc cues one should have paid attention to in the email  tips to avoid phishing in the future  an instructional video  and further quizzes and learning material on phishing.the training page was developed according to the best practices in academia [45]  [31] and industry [19]  [17] by the external service provider; we provide an excerpt in appendix a.the training page was delivered to employees such that there was no enforcement that the employee has to  fig.4: menu bar of the company’s email client (outlook)  modiﬁed to include a button to report suspicious emails.read the whole webpage or take the quizzes.reporting button.our partner company deployed a button  for reporting suspicious emails.this button was introduced in the outlook client  as shown in figure 4  and it was advertised in the internal news of the company before the start of the experiment.when reporting a suspicious email  employees could toggle a checkbox to report that they also opened the attachment or visited the link in the email  to notify the it department about a possible incident.reported email processing.all emails that were reported by our study participants were triaged by a commercial anti- phishing appliance  that ran a more-detailed secondary anal- ysis.the secondary analysis performed on the reported emails differed from the company’s primary inline ﬁlter in two ways: (a) more time consuming checks  such as following links  were performed  and (b) the analysis settings were tuned to be more aggressive  as at this point we did not need to avoid too many false positives.the results of the secondary analysis were presented to the company’s it department via a dashboard  where the appliance verdicts could be either conﬁrmed or subverted based on manual analysis .further  the appliance could return feedback to the employees  indicating whether the reported email was indeed malicious or not.c.study participants  the company enrolled 14 733 employees to be part of the experiment: we refer to them as participants.participants were selected uniformly at random from the whole company’s employee base  comprising many different job types  from accountants  it  marketing  and managerial roles  to less technical jobs (e.g.in logistics  or working in retail shops).in total  participants spanned 28 organizational groups of the company  and represented 3 827 different teams.for the purposes of our study  we classiﬁed the participants in terms of their age  gender and computer use in the day-  (a) use of computers in job.(b) age range.(c) gender.fig.5: demographics information of the study participants.age and gender are further divided by computer use in their job.to-day job.participants were divided by computer use (shown in figure 5a) in three different categories: (i) ofﬁce workers using computers daily  either in it-related jobs or in jobs that use computers such as marketing and accounting (frequent use); (ii) employment roles such as retail shop workers in contact with the general public  that mostly use a point-of- sales software and conﬁgure services from it (specialized use); and (iii) roles such as team leaders of ﬁeld workers in logistics who have a corporate email account but rarely use computers in their duties (infrequent use).study participants spanned an age range 18-73  with all age groups being well represented in our set of participants (figure 5b).the gender distribution (figure 5c) of our participants was balanced: 7 377 were male  and 7 356 were female  similar to the distribution of the company’s employee base.we observe an imbalance in the use of computers for gender and age: the majority of the branch workers that mostly use one specialized program are female  and are skewed to older ages  while users that work with or without computers are more uniformly distributed.d.study group sampling  following on our advice  the company assigned each of the 14 733 participants to one of 12 different user groups generated by combining the settings administered for different tested tools and mechanisms:  • warnings (3 settings): every participant received one of three possible settings on their simulated phishing emails: a simple warning; a detailed warning; or no warning  as a control setting.• training (2 settings): every participant that failed one simulated phishing attack by performing the dangerous action could either be redirected to the training page; or receive no such training  as a control setting.• report feedback (2 settings): after reporting a suspi- cious email  participants could always receive the result of their report as feedback; or they could receive the result only when they reported a legitimate email  as a control setting.for example  group 1 was administered simple warnings  training  and no feedback after correctly reporting phishing  while group 2 had the same conﬁguration except receiving complex warnings  and so on.each participant was randomly  assigned to one of the 3 × 2 × 2 = 12 groups  so that they were approximately of the same size: from 1 223 participants on the smaller to 1 231 participants on the larger.e.experiment execution  from july 2019 to october 2020   the company sent 8 different simulated phishing emails to each of the 14 733 participants.the participants received the ﬁrst 6 emails in random order and at random time intervals [16] during the ﬁrst 12 months of the experiment (july 2019–july 2020).they received the last two phishing emails2 from august 2020 to october 2020  again in random order and at random time inter- vals.participants were not aware of our study speciﬁcally  to not modify their behavior [47]  [48]; however  they were aware that the company may occasionally send phishing exercises to their employees.the 8 different email campaigns  of varying difﬁculty  were designed to simulate broad phishing campaigns targeted to the organization as a whole  rather than sophisticated  individually-crafted spear phishing.each different email rep- resented a typical phishing scenario  such as prompts to check their corporate credentials  migrate their email accounts to a new system  or parcel delivery notes as attachments  and used different triggers  such as a sense of authority or urgency or leveraging people’s curiosity [27].five emails contained a link to a phishing website  while three had an attached ﬁle.we provide the english version of selected emails in appendix b.during the experiment  our partner company recorded the  following interactions with the simulated emails: • clicks on the links contained in the email; • dangerous actions: further falling for the phish by  e.g.submitting credentials to the linked website  or enabling macros on the attached document.the company also recorded participants’ reports of suspi- cious emails.for each reported email  they stored whether it was one of our simulated phishing emails  the result of the secondary analysis by the anti-phishing appliance  and whether any employee from the it department looked at such result and conﬁrmed or subverted its verdict.during the last 5 months of  2the last emails were supposed to be three; however  a simulated ceo fraud phishing attack [46] caused unwanted confusion inside the company and this speciﬁc simulated phishing email was canceled.frequentinfrequentspecialized0200040006000#ofemployees18-1920-2930-3940-4950-5960+01000200030004000#ofemployeesfrequentinfrequentspecializedmf0200040006000#ofemployeesfrequentinfrequentspecializedthe experiment  the company also recorded how many inbound emails were similar to a reported one in a 20-days window around the date of the report.at the end of the experiment  we administered a question- naire with 27 closed-ended questions to 1000 randomly se- lected participants.participants that accepted to respond were informed that their replies were recorded anonymously and would further not be shared with their employer  to encourage honest answers.the ﬁrst questions asked participants about knowledge of phishing and other email threats  questions about email warnings  the button to report phishing  contextual train- ing  and whether they recalled falling for phishing.we report selected questions from the questionnaire in appendix c.we received 151 complete answers.f.ethics and safety  study approval.this study was initiated and approved by the ciso of our partner company.during the study  we never had access to any pii  and were only given access to anonymized data after collection by the company (see section iii-a).since the analysis of anonymized data does not require irb approval according to our institution’s guidelines  we did not submit a formal request.risks to participants.our partner company informs its employees about that includes phishing exercises.thus  our study participants were generally aware that the company may send them simulated phishing emails.the company did not speciﬁcally inform participants about the simulated phishing emails that were sent as part of this study (i.e.no informed consent or debrieﬁng).participants not in the embedded training group were not speciﬁcally informed about the simulated phishing emails  while participants in the embedded training group were informed that the email was phishing if they fell for it.their phishing awareness campaign   our participants were subject to minimal risk as part of this experiment: they were not exposed to greater risk than what they encounter as part of their normal daily life [49]  because they receive real phishing and other malicious emails regularly.experiments such as the one we conducted here can have negative impacts such as wasting employees’ time or creating distrust towards the company [49].this experiment took place as part of the company’s existing training program; given this context  we felt that the scientiﬁc impact of our experiment merited these potential negative impacts data collection and protection.during the study  our partner company collected data regarding clicks and dangerous ac- tions  and data on emails reported as phishing by participants.if a study participant entered their password on the simulated phishing web page  our partner company did not record the entered credentials nor checked if they were correct.the col- lected dataset was accessible to a small number of employees working in the it security department of our partner company and protected with two-factor authentication.the collected dataset was provided to us in anonymized format such that only attributes like gender  age  and level  of computer use were preserved.our partner company used the dataset internally to assess its overall exposure to phishing threats  and ensured us that the dataset will not be used for any other purpose  such as employee performance assessment.the reported emails did not carry any pii: every report recorded whether the reported email was a simulated one or not  and scores and verdicts of the anti-phishing appliance.none of these information can link to the original sender  subject  or content of the message.g.experiment statistics  overall   the study participants clicked on 6 680 out of 117 864 simulated phishes (5.67%).during the 15 months  4 729/14 733 participants (32.10%) clicked on at least one phish.the trend for dangerous actions is similar  with the numbers slightly lower: participants fell for 4 885 simulated phishing emails (4.14% of the total sent emails  and 73.13% of all the clicked simulated phishes)  and 3 747/14 733 partic- ipants (25.43%) users did at least one dangerous action.there were 4 260 study participants that reported at least one email.in total  the participants reported 14 401 emails  of which 11 035 were our simulated emails.the button to report phishing was also deployed to 6300 employees that were not part of the experiment but could report phishing: 1 543 of them reported at least one suspicious email  and they reported 4 075 emails.thus  the total number of reported emails we received during the 15 months was 18 476.iv.which employees fall for phishing?  in this section  we analyze the experiment data to understand which employees are the most likely to fall for phishing (rq1).recall from section iii that we classify participants based on frequent  infrequent and specialized use.we count the number of clicked links and dangerous actions based on demographics and job categories (see figure 6).for our following analysis  we deﬁne the following three hypotheses: • h1: employees’ use of computers in their job correlates  to falling for phishing.• h2: employees’ age correlates to falling for phishing.• h3: employees’ gender correlates to falling for phishing.to analyze the measured numbers  we ﬁt a linear model with type iii sum of squares to analyze both the demographic properties by themselves  and to capture the interactions among them.this statistical tool allows us to measure the impact of the independent variables (i.e.the demographic properties) on the dependent variables: number of clicked links and dangerous actions  that we use as proxies for phishing susceptibility.we ﬁt the model with all the combinations of demographic properties  and exclude the non-signiﬁcant factors until we obtain a ﬁnal model with following results.the results support h1: correlation with computer use.as can be seen from figure 6a  participants whose job type involve specialized computer use (e.g.branch workers who mostly use a single dedicated program) clicked on more links in phishing emails and performed more dangerous actions  (a) use of computers in job type.(b) age range.(c) gender by use of computers in job.fig.6: percentage of dangerous actions performed out of all phishing emails sent  divided by different demographics.frequent use of computers but in a very specialized setting  and young and older age all inﬂuence the susceptibility to phishing.than participants in the other comparable groups (frequent and infrequent use).our ﬁtted model shows that computer use is signiﬁcant (clicks: f (2  14710) = 11.01  p < 0.001; dangerous actions: f (2  14710) = 9.45  p < 0.001) and a tukey hsd post-hoc test conﬁrms that the difference between specialized use and the other two groups is signiﬁcant both for clicks and dangerous actions.however  the difference between frequent and infrequent use is not signiﬁcant.thus  while we support previous work that showed relationship between phishing susceptibility and knowledge of technology [11]  this last observation invites to caution  as this relationship seems more nuanced.while it is common to leverage the amount of computer use in participants’ jobs as a proxy for technological skills  our results suggest that the type of computer use and the expectations in one’s job might also inﬂuence phishing susceptibility.for example  specialized use participants in our partner organization may be expected to interact with emails more than infrequent use participants  who may  therefore  be more suspicious of incoming emails.the results support h2: correlation with age.the youngest employees clicked more and performed more dangerous ac- tions.our model conﬁrms the interaction between age and phishing susceptibility (click rate f (5  14710) = 4.70  p < 0.001); dangerous action rate (f (5  14710) = 3.84  p < 0.001).we ran a tukey hsd test to analyze which groups were more at-risk and conﬁrm what figure 6b shows: partici- pants aged 18–19 were much more likely to click on phishing links and perform the dangerous action than any other age group; participants in the 50–59 age range were also more at risk than the top performers aged 20–29 and 60+.this result supports previous literature [40]  [41]  [34].the results do not support h3: correlation with gender.our participants’ computer use w.r.t.their gender is not uniform (recall figure 5c).thus  further dividing interactions of both genders by use of computers shows a large difference among the same gender  shown in figure 6c and conﬁrmed by our model: the combination of gender and computer use is signif- icant (click rate f (2  14710) = 13.06  p < 0.001)  but gender by itself is not (click rate f (2  14710) = 0.23  p = 0.63).in- deed  figure 6c shows us that while frequent use females were  fig.7: number of simulated phishing emails that participants clicked or performed the dangerous action (8 being maximum; a missing bar denotes zero participants).more susceptible than frequent males  specialized use males were more susceptible than their female counterpart.thus  phishing susceptibility of participants can be better explained by considering the imbalance in job types  contradicting some previous studies [36]  [30].v.phishing vulnerability over time  in this section  we leverage our 15-month study to analyze how the phishing susceptibility of the organization evolves over time (rq2).to do so  we analyze trends of clicks and dangerous actions over time: how many times (out of maximum 8) participants interacted with the phishes  and how many participants over time eventually did so at least once.repeated clickers.we report in figure 7 the histogram of how many participants clicked or performed the dangerous action on the simulations a given amount of times.a total of 1 448 (30.62%) participants clicked on two or more phishes  and 896 (23.91%) performed the dangerous action on two or more— one participant even fell for 6 out of 8 simulations.thus  we observe that there will be a small number of employees that will click or fall for phishing emails multiple times  supporting a previous preliminary study [42].similarly to the raw amount of clicks and dangerous actions  we observe a correlation between age groups and clicking (welch-corrected anova  frequentinfrequentspecialized0%1%2%3%4%%ofdangerousactions18-1920-2930-3940-4950-5960+0%2%4%%ofdangerousactionsmf0%2%4%%ofdangerousactionsfrequentinfrequentspecialized0246actioncount101103#ofparticipantsclicksdangerousactionsf (5  4199) = 5.72  p < 0.001) or performing the dangerous action (f (5  4186) = 3.66  p = 0.002) on more than one simulated phishing emails.in both cases  a tukey hsd test shows that the younger group of participants aged 18-19 stands out as the one more likely to click more than once.many employees will eventually fall for phishing if con- tinuously exposed.in our experiment 4 729 out of 14 733 (32.10%) participants clicked on at least one link or attachment in our simulated phishing emails.a similar high number applies to dangerous actions: 3 747 out of 14 733 (25.43%) performed at least one.these results indicate that a rather large fraction of the entire employee base will be vulnerable to phishing when exposed to phishing emails for a sufﬁciently long time.we are the ﬁrst to show such result at scale.vi.effectiveness of warnings and training  in this section we analyze the data collected from our to answer rq3 related to the effectiveness of  experiment phishing warnings and training.a.effectiveness of warnings  recall from section iii that we experimented with two types of warnings (short and detailed)  and a control group that did not see any warnings.to analyze the effectiveness of these two warning types  we use the following hypotheses:  • h4: adding warnings on top of suspicious emails helps  users in detecting phishing.• h5: detailed warnings are more effective than short ones.the results support h4: warnings help users.figure 8 shows click and dangerous action rate for the different warning conﬁgurations.we observe that both types of warnings greatly helped participants both in avoiding clicking on links in our simulated phishing emails and not falling for the phish by performing the dangerous action.considering click rate  the group with no warnings clicked 3 964 times  compared to the lower 1 427 clicks for short and 1 289 clicks for long warnings (welch-corrected anova f (2  7485) = 564.71  p < 0.001).dangerous actions rate is similar: 2 994 dangerous actions for no warnings  compared to 998 and 893 dangerous actions  respectively (f (2  7461) = 392.58  p < 0.001).figure 9 shows the histogram of how many participants clicked on a simulated phish a given amount of times.we observe a strong correlation between receiving any of the warnings and not clicking or performing the dangerous action more than once (clicks: f (2  9287) = 358.88  p < 0.001  dangerous actions: f (2  9194) = 239.68  p < 0.001).our results support this widespread industry practice [43].the results do not support h5: detailed warnings are not more effective than short ones.to check whether there is any difference between short and detailed warnings  we ran a tukey hsd test between all groups and observed that  while both warnings correlate with lower total clicks and dangerous actions  there is no signiﬁcant difference between short and detailed warnings.thus  the way we provided additional infor- mation to users (by mimicking the current industry practices   fig.8: dangerous actions by administered warning.both warning types helped the participants signiﬁcantly.fig.9: number of different phishing emails that participants clicked on  by administered warning.missing bars denote a 0.rather than making radical changes to email warnings [37]) does not seem to provide better phishing protection.b.effectiveness of contextual training  recall from section iii that we tested the effectiveness of contextual training after falling for a simulated phishing email by administering it only to half of the participants—the other half was a control group for training and did not see the webpage.we formulate the following hypothesis:  • h6: receiving contextual training helps users improve in  future phishing detection.we analyze both the frequency at which participants clicked or performed the dangerous action  and the correlation between training and doing more than one click or dangerous action.the results do not support h6: voluntary contextual train- ing does not improve future phishing detection.surpris- ingly  we observe that both click and dangerous actions rates are higher for participants that received contextual training (i.e.participants who were forwarded to a training page) after falling for simulated phishes: for clicks  3 087 versus 3 593; for dangerous actions  2 155 versus 2 730.figure 10 shows the histogram of how many different phishing emails participants performed the dangerous action on.as expected  the number of participants that did not fall for any simulated phish  or fell only once  is similar among the two groups: such participants  nowarningsshortdetailed0100020003000#ofdangerousactions0246#ofclicks100101102103#ofparticipantsnowarningsshortdetailedthat were in the training group either never saw the training page  or saw it after performing their only dangerous action.however  if we focus on participants that fell two or more times (and thus  on participants in the training group that fell again for phishing after being shown the training page)  we see that the distribution is more skewed to the right for participants in the training group.indeed  participants that clicked on two or more phishing emails were 647 without training  and 801 with training.this shows a strong correlation between the provided training page and clicking on phishing emails or even performing the dangerous action more than once (welch- corrected anova for clicking: f (1  14592) = 18.37  p < 0.001; dangerous actions: f (1  14279) = 33.80  p < 0.001).this perhaps surprising result requires a careful interpre- tation.what our experiment showed is that this particular way of delivering voluntary training does not work.instead  such training method may cause unexpected and negative side effects  such as increased susceptibility to phishing.this ﬁnd- ing is signiﬁcant  because the tested phishing training delivery method is a common industry practice [19]  [17]  [20]  [18]  and the training material (refer to section iii) was designed by a specialized company according to known guidelines and best practices from previous work [31]  [34]  [45].it would be interesting to study whether other possible ways to deliver contextual training (e.g.ones where interaction with the provided training material is enforced) would work better.our study did not test the effectiveness of mandatory training.to gain some insights on why susceptibility to phishing increased among those participants who were forwarded to the training page  we analyzed the answers to our post-experiment questionnaire.one possible explanation that emerges from the questionnaire responses was a false sense of security that is related to the deployed training method: out of the respondents who remembered seeing the training page  43% selected the option “seeing the training web page made me feel safe”  and 40% selected the option “the company is protecting me from bad emails”.it remains an open question for future work to explore whether this is due to a misinterpretation of the training page (i.e.whether the participants thought they were protected from a real attack)  or if this is because of overconﬁdence in the organization’s it measures in general  as observed in similar settings in the past [39]  [50]  [51].ultimately  our result shows that organizations need to be careful when using this training method  and aware of possible unintended side effects.vii.can employees help the organization?  we now analyze the data collected from our experiment to answer rq4  related to crowd-sourced phishing detection in an organization.such method needs to fulﬁll the following requirements to be useful:  • sustainability: employees need to keep reporting suspi-  cious emails over long period of time.• effectiveness: employees’ reports need to be sufﬁciently accurate and timely so that the organization can stop new campaigns quickly enough.fig.10: number of different simulated phishing emails par- ticipants performed the dangerous action on  by administered contextual training.missing bars denote a 0.• practicality: the operational workload to process all the  reported emails needs to remain acceptable.a.reporting sustainability for employees  recall from section iii that we decided to experimented with two types of feedback: (i) always receive the result of their report; or (ii) receive the result only when (erroneously) reporting a legitimate (non-phishing) email.to investigate re- porting sustainability  we examine how the employees’ activity in reporting suspicious emails evolved over time  and whether the tested method of encouraging reporting worked.we ex- amine these questions using the following two hypotheses:  • h7: employees keep reporting over time at a steady rate • h8: providing feedback to reports encourages to report  again in the future  we count all reports and analyze their rate over time  and compare the number of participants that reported more emails after receiving the two different types of feedback.the results support h7: employees continue reporting emails.figure 11 shows the number of suspicious emails reported over the duration of the entire experiment.3 we observe a steady income of reports that does not slow down (and even increased when the two new phishing emails were released in august 2020)  as shown by the constant fraction of simulated emails reported daily.we further analyze the distribution of frequency of reports that is shown in figure 12.while 90% of the employees that reported suspicious emails reported 6 or less  there is a non-negligible amount of very active users.we conclude that in our experiment of 15 months  there was no signiﬁcant “reporting fatigue” suggesting that  if reporting is made easy  employees can actively keep on reporting suspicious emails for long periods of time.additionally  we examined whether any demographic inﬂu- ences the quantity of reports by ﬁtting a linear model with type iii sum of squares.similarly to phishing susceptibility  the combinations of age and computer use in job  and gender and computer use in job are signiﬁcant (age and computer  3these numbers include all 21 000 employees that received the button.0246#ofdangerousactions100101102103#ofparticipantsnotrainingcontextualtrainingfig.11: cumulative email reports over time.the dashed red line shows the percentage of simulated emails reported daily.fig.12: distribution of the number of reports per user.use f (10  14710) = 6.49  p < 0.001; gender and computer use f (2  14710) = 11.35  p < 0.001).considering the skewed distribution of computer use  we assume it is the main contributing factor.indeed  we ﬁnd that frequent computer use participants reported a very encouraging 22% of all the simulated emails that they received  while infrequent use participants reported only 10.20% and specialized use 7.60%.we conclude that  quite intuitively  employees with the best expected computer skills are also the most active reporters.however  interestingly  infrequent use participants were more active than the specialized ones.the results support h8: positive feedback encourages employees to report more.we ﬁnd a signiﬁcant interaction between the type of administered feedback type and the amount of reported emails.to measure this  we ﬁrst exclude all the participants that never reported any email.then we count how many emails were reported by the group that actually received positive feedback and by the one that only received feedback about false reports.the former (2 046 participants) is composed by participants in groups that always received feedback and that reported at least one malicious or simulated email (thus receiving the positive feedback).the latter (2 201 participants) is formed by those in groups that did not receive positive feedback  and by those in a group that could receive positive feedback but only reported legitimate emails (thus never receiving the positive feedback).we ran a welch-corrected anova (f (1  3224) = 31.62  p < 0.001) conﬁrming that participants that saw the positive feedback were more likely to report more emails.b.effectiveness of crowd-sourced phishing detection  to analyze the effectiveness of crowd-sourced phishing detection mechanism as a whole  we analyze timeliness and accuracy of reports.in addition to sufﬁciently high reporting activity  organizations need both quick and sufﬁciently accu- rate reporting to be able to detect and stop novel phishing campaigns that are often short-lived [14].we note that since we did not send thousands of copies of the same phishing email at the same time  we cannot directly measure how fast such mass phishing campaigns are reported  fig.13: average per–user–group cumulative distribution function of reported simulated emails as a function of the interval between email reception and report  with standard deviation as a colored region.and thus detected.instead  we measured how fast our randomly timed simulated phishing emails were reported by participants.based on these numbers  we can then estimate how quickly and accurately real mass campaigns could be detected.timeliness.we show in figure 13 the percentage of reports of our simulated emails that arrived shortly after their delivery.we can observe that the reaction time of the employee base as a whole is fast: on average around 10% of the reports arrived within 5 minutes; 20% within 15; and 30% to 40% within 30 minutes.we observe no signiﬁcant difference between the reporting times of different simulated email campaigns: despite all having different number of total reports (from 2 538 reports of the most reported simulation  down to 832 reports for the least reported)  all consistently see a similar amount of reports incoming within the ﬁrst 30 minutes.to apply these numbers to a hypothetical company of 1 000 employees where 100 of them are targeted by a phishing campaign  we would have between 8 and 25 reports of the email by employees—of which one within 5 minutes with high probability  and a larger number within 30 minutes.accuracy.the average accuracy of reports was good: 68%  up to 79% if spam emails should be reported as well.4 we observe  4as ground truth  we consider here the outcome of the secondary antiphish- ing appliance  corrected and validated by the it department of the company.0100200300400050001000015000#reportedallreportssimulatedemails0%20%40%60%80%100%%simulationsreported020406080100#ofreports100101102103104#ofusers030m1hr2hrs4hrs8hrsintervalbetweenreceptionandreport0%10%20%30%40%50%60%%ofreportsthat the distribution of employees’ accuracy in reporting is wide: while over 60% of the reporting employees have an accuracy of 80% or more  there is a non-trivial fraction that was always wrong (13% if spam should be reported; 22% otherwise)—however  it mostly comprises employees who reported only a single email.the accuracy of the top 10% of very active employees that reported 6 or more emails (recall section vii-a) is around 5% higher than considering all employees.we further note that very high reporting accuracy is not crucial.if using a secondary anti-phishing appliance to triage reports  as done in our experiment  employees can be encouraged to be overly-cautious and report emails when in doubt (not only when absolutely sure)  as the appliance can serve as a ﬁrst check on the email  and keep the operational workload acceptable  as discussed below.incident awareness.additionally  we analyzed the employ- ees’ awareness of being the victims of a security incident.we start by noticing that 6% of the participants who performed the dangerous action on our simulated emails immediately reported the email  thus realizing they were victims of a phishing attack.5 only 3.7% of these participants did not toggle the checkbox of the report button that allowed employ- ees to report whether they visited the link contained in the email  or opened its attachments.interestingly  we observe that some participants were overly-cautious: 13% of the reports of simulated phishing emails stated that they opened the link or attachment  despite not having done it.c.practicality for the organization  we observe that a secondary appliance triaging the reports makes the added workload reasonable: out of the 7 191 non- simulated reported emails in 15 months  only 689 (9%) of the decisions were taken by human administrators  and actually overturned the decision taken by the appliance only 50 times (7% of the total handled cases).the main goal of this secondary appliance is to ﬁlter out the reports of clear benign emails or minor threats such as spam  which include the majority of our collected reports: out of 7 191 reports of emails not part of our exercise  3 531 were benign  and 2 371 were spam or unwanted newsletters.thus  only roughly 1.5 emails per day needed manual handling from the it department—a clearly acceptable workload for a large organization that was collecting reports from over 21 000 users.d.finding real phishing campaings  we further validate our crowd-sourced phishing detection approach by analyzing whether we caught any real phishing campaigns delivered to employees of the company (in addition to our simulated phishing emails).we use the verdicts of the secondary ﬁlter and manual inspection by it specialists to ﬁnd reported phishing and other malicious emails.we observed 918 reports of real phishing emails during the last 5  months of our deployment.with email similarity techniques  we measured how many emails similar to the reported ones were incoming and found 252 large-scale phishing campaigns comprising 28 830 emails  and 1 534 emails with malware attached that our crowd-sourced approach would have detected in a short time span from their beginning.viii.study validity  simulated emails limitations.recall that 3 of our 8 emails had a malicious attachment where the dangerous action was to enable macros.while the company could monitor when macros were enabled  with a network call to the monitoring in- frastructure  it could not know when participants only clicked  i.e.simply opened and closed the attachment without enabling macros.thus  for attachments we underestimate the number of clicks by setting it to the number of dangerous actions.the company did not record when a simulated email was opened  thus we do not know the conversion rate from opening the email to clicking and dangerous actions.let  due to data protection concerns  it was not recorded whether employees submitted their valid credentials to the simulated phishing websites.therefore  the amount of employees that we recorded performing some of the dangerous actions (e.g.submitting credentials) could be overestimated  because we cannot ﬁlter out employees who submitted bogus credentials.email warnings limitations.our partner company added warnings on top of simulated phishing emails  but not on top of emails that the inline ﬁltering solution in use deemed suspicious but through anyway.this could lead some participants to fall once for our simulated phishing email  and subsequently associate the presence of warnings to surely suspicious emails  or even worse  to training exercises.further studies on this promising type of warnings when added on top of legitimate but suspicious-looking emails are needed  to remove the potential bias.campaigns success rates.as shown in figure 14  the differ- ent simulated phishing email campaigns had different success rates.such differences do not inﬂuence our analysis of rq1- rq4 due to two reasons.first  we always count the total number of clicks or dangerous actions for all campaigns and compare total counts.second  the order of administered cam- paigns was randomized for each participant in large groups.applicability to different companies.our partner company operates in numerous different sectors  has a diverse work- force  and large size.thus  we believe that our results can generalize to various similar-sized (large) companies.it is unclear whether our results generalize to companies with very specialized it workers  e.g.software engineering companies  or to very small organizations.ix.related work  5we only measure participants that did not receive training after falling for a simulated phish  because they had to understand by themselves what happened—the training material stated clearly that it was a simulated phishing attack.phishing and demographics.age is one of the most analyzed factors of phishing  as it intuitively often correlates with technological skills.studies showed that very young [40]   corporate setting [55].further  the business ecosystem that emerged around embedded phishing training [19]  [18]  [17]  [20] claims improvements due to the beneﬁts of training in a recent collaborative report [2]  but does not report results of experiment in controlled settings [56].phishing warnings have been studied extensively in the con- text of browsers (e.g.[57]  [58]).some recent works [37] also evaluate different kinds of warnings shown on the email client.while too frequent warnings can susceptible to habituation and lose some of their effectiveness over time [59]  the literature agrees that carefully-timed warnings are in general effective.crowd-sourced phishing detection.several companies al- ready provide tools to report phishing emails  to quickly detect new attacks using aggregate information across multiple customers [19]  [24].the same companies report that users are improving at reporting phishing attempts over time [2]  [1]  however  other work showed that users are reticent to report phishing to the it because of the lack of transparency in the process [60] and lack of fast responses from the system [39].prior to our work  it was not known if employees as a crowd- sourcing mechanism in a closed scenario  such as a corporation that manages reported phishing in-house  works effectively with acceptable operational workload.few recent works also suggest this concept  but do not evaluate it [25]  [61].x.conclusions and future work  thanks to our long-term and large-scale experiment  in this paper we supported several prior ﬁndings such as effectiveness of warnings with increased ecological validity.further  we found that embedded phishing training  as commonly deployed in the industry today  is not effective and can in fact have negative side effects.in this regard  our results contradict prior literature and common industry practices.finally  we are the ﬁrst to experimentally demonstrate that crowd-sourced phish- ing detection is effective and practical in a single organization.based on these results  we encourage organizations to adopt phishing prevention tools like warnings that have been extensively studied and where the available literature supports their effectiveness overwhelmingly.we call for caution in the deployment of methods like embedded phishing exercises and training  where the the existing literature is less unanimous about their effectiveness  and our research discovers potential negative side effects.we recommend organizations to consider crowd-sourced phishing detection as a new and complemen- tary way to improve the overall phishing prevention capabili- ties of the organization  since its effectiveness looks promising and operational workload remains low.our work also identiﬁes topics where more research is needed.our research shows that the effectiveness of phishing exercise and training has not been sufﬁciently measured  and it remains unknown what is the most effective way to deliver embedded phishing training.more research is also needed to better understand the (psychological) effects of phishing exer- cises and training that is embedded into the normal working context of employees  and how such effects may inﬂuence the employees’ future handling of real phishing emails.fig.14: cumulative count of dangerous actions per campaign during the experiment duration.[41]  [34]  and older people [36]  [35]  [29] are more at-risk for phishing.preliminary studies show that aging increases susceptibility to phishing [35]  but only the two extremes (very young and senior persons) were tested  not the full age spectrum.further  different age ranges are susceptible to different types of phishing emails [15]  [36].gender is a more divisive demographic  according to a recent literature survey [11]  but the studies that do ﬁnd an impact show that women are more vulnerable [36]  and can detect less phishing attempts [30].experience with computers [11]  experience of previous phishing attempts [29]  and seniority at an organiza- tion [26] also positively inﬂuence phishing immunity.phishing at workplace.some previous studies show that within the organization’s boundaries  employees feel safer and generally trust their company’s measures  thus lowering their attention [39]  [50]  and the existence of “repeated clickers” that are extremely at risk of being phished [42].other studies ﬁnd that helping employees in phishing prevention is made hard by the fact they struggle to comply with corporate security policies and often ignore them [52]  [53].phishing training and warnings.there is broad consensus that training should be active  e.g.with security games [54].a popular mechanism is running simulated phishing exercises  an approach adopted by several companies [19]  [18]  [17]  [20] following promising research results [32]  [33]  [34]  where (possibly unaware [47]  [48]) employees receive simulated phishing emails over time  ideally at random intervals [16].this practice is often combined with embedded training: immediately redirecting the employee that fell for phishing to a dedicated web page explaining the simulated attack they just fell for and providing information about phishing [34].previous studies suggest that training should be continuous  as knowledge retention spans from a few days [33]  [45] to a few months at most [34].however  research effort has unclear external validity  because most work employed small populations [33]  [31]  [28]  [16]  [32]  [27]  was shorter in time [16]  [33]  [27]  had populations with little diversity [31]  [28]  [27] or tested a role-playing setting only [32]  [31]—and a recent study questioned whether these results transfer to a  0100200300400days050010001500# of dangerous actionsc8c6c4c7c3c1c2c5acknowledgments  this research has been partially supported by the zurich  information security and privacy center (zisc)
62,2022-Poisoning Attacks to Graph-Based Recommender Systems.pdf, in recent years  studies have revealed that introducing knowledge graphs (kgs) into recommendation systems as auxiliary information can improve recommendation accuracy.however  kgs are usually based on third-party data that may be manipulated by malicious individuals.in this study  we developed a poisoning attack strategy applied on a kg-based recommendation system to analyze the inﬂuence of fake links.the aim of an attacker is to recommend speciﬁc products to improve their visibility.most related studies have focused on adversarial attacks on graph data; kg-based recommendation systems have rarely been discussed.we propose an attack model corresponding to recommendations.in the model  the current recommended status and a speciﬁed item are analyzed to estimate the effects of different attack decisions (addition or deletion of facts)  thereby generating the optimal attack combination.finally  the kg is contaminated by the attack combination so that the trained recommendation model recommends a speciﬁc item to as many people as possible.we formulated the process into a deep reinforcement learning method.conducting experiments on the movie and the fund data sets enabled us to systematically analyze our poisoning attack strategy.the experimental results proved that the proposed strategy can effectively improve an item’s ranking in a recommendation list.keywords poisoning attacks  adversarial attacks  knowledge graph-based recommendation  deep reinforcement learning  1,  recently  recommendation systems have become increas- ingly valuable.recommendation systems are commonly used in real-world applications such as music  ﬁlm  and online shopping.these systems have attracted attention  & szu-hao huang  szuhaohuang@nycu.edu.tw  zih-wun wu zihwun07534.iim07g@nctu.edu.tw  chiao-ting chen rsps971130.cs09@nycu.edu.tw  1  institute of information management  national yang ming chiao tung university  hsinchu  taiwan 30010  ,  we devised a poisoning attack strategy using deep q-learning that transforms the process of selecting the most effective attack combination into step-by-step training involving deep reinforcement learning.subsequently  we proposed two reward mechanisms to identify the optimal combination of perturbations.attackers inﬂuence kg- based recommendation systems by polluting the corre- sponding kg  thereby recommending speciﬁc items to more people than before the attack.experimental results obtained using data sets with different attributes demon- strate that in the kg-based recommendation system  the evaluation value of the poisoning attack strategy proposed  table 6 performance comparison of different attack strategies applied to fund transaction record data  attack  2018/ 07 (%)  2018/ 08 (%)  2018/ 09 (%)  hit@1  hit@3  hit@5  hit@10  hit@1  hit@3  hit@5  hit@10  hit@1  hit@3  hit@5  hit@10  none  0.2329  0.4976  1.2738  4.5432  0.7830  1.0735  1.8505  6.0607  0.4355  1.8436  3.3128  6.9499  random  0.4970  1.4622  2.3707  4.3045  0.8618  2.0715  3.0406  5.5285  0.5385  1.8358  3.1467  5.9618  zhang [46]  dqn-rank  0.4628  0.5310  1.4383  1.5486  2.4596  2.5691  4.9810  5.4148  0.6188  1.4502  1.8391  2.9073  2.9157  4.2074  5.8013  7.4981  0.9224  1.3204  2.2455  3.0829  3.4269  4.2809  6.2198  6.6658  attack  2018/ 10 (%)  2018/ 11 (%)  2018/ 12 (%)  hit@1  hit@3  hit@5  hit@10  hit@1  hit@3  hit@5  hit@10  hit@1  hit@3  hit@5  hit@10  none  random  zhang [46]  dqn-rank  0.7674  1.9678  3.4135  6.6922  0.3588  1.6122  2.9557  5.6723  0.6083  1.6027  2.6739  6.0431  0.4575  1.5157  2.6203  5.6173  0.5021  1.3461  2.2765  4.6982  0.5973  2.0918  3.6480  7.2666  0.8546 0.8956  2.1114 2.2594  3.2968 3.5338  6.4979  6.6017  0.5574 1.0752  1.6205 2.3697  2.9860 3.6212  6.4356 6.5537  0.7491 1.6355  2.5908 4.1913  4.3579 6.1398  8.3397 10.0971  123  3114  neural computing and applications (2022) 34:3097–3115  in this paper was higher than that before the attack by 1.6921%.this strategy has validity and practicability advantages over other methods discussed in counterattack research.black box experiments showed that the adver- sarial examples of kgs are transferable.results from our mutual fund experiments reveal that the manipulation of a kg can increase the visibility of speciﬁc products in practice and bring commercial beneﬁts to attackers.some possible future directions are worth discussing.in addition  certain aspects of require enhancement.for example  in attack strategies  more realistic attack methods can be examined  such as adding new products.in particular  future research can develop effective defense algorithms on the basis of the poisoning attack strategy proposed by the present study.for example  defense mechanisms such as using adversarial training modules can improve the robustness of kg-based recom- mendation systems to prevent malicious attacks.attack strategy still  the  acknowledgements this work was supported in part by the ministry of science and technology  taiwan  under contract most 110-2221-e-a49 -101 and contract most 110-2622-8-009 -014 - tm1; and in part by the financial technology (fintech) innovation research center  national yang ming chiao tung university.declarations  conflict of interest all authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript
63,2022-yoU aRe a Liar-_A Unified Framework for Cross-Testing URL Parsers.pdf,—a variety of attacks  including phishing  remote-code execution  server-side request forgery  and hostname redirection  are delivered to users over the web.the beginning of most of the web exploits is an innocent-looking url.malformed or misinterpreted urls can lead to remote code execution attacks as well.the ietf and whatwg standards organizations define the components of a url and act as an implementation guide for url parsers.they state which characters are allowed in each portion of the url and loosely suggest what to do in case an undefined character is present in the url.the existence of two standards is the first concern  and the addition of server-side request forgery in the latest version of owasp top 10  suggests that neither of these standards is being followed accurately and concisely.moreover  neither of these specifications describe an exact implementation standard  causing inconsistencies in the way the various parsers interpret the same url.for example  malicious users can find ways to craft urls to look like they are pointing to one resource but actually direct the user to different one.this problem is worsened when one application uses two separate parsers for validation and resource fetching.in this paper  we design a framework that unifies the testing suites of 8 url parsers from popular web-related projects and highlights the inconsistencies between them.we examine and dive deep into the url parser implementation across the most popular libraries  browsers  and command-line tools  and discover many open areas for exploitation.our findings include identifying categories of inconsistencies  developing proof-of- concept exploits  and highlighting the need for a comprehensive implementation standard to be developed and enforced at the earliest.index terms—url  parser  spoofing  web security  ssrf  i,  to make it possible for an individual to record the location of a document  the world wide web associates each page of information with a unique identifier.the identifier consists of a string of characters that can be recorded in a computer file  written on a piece of paper  or sent to another person.an identifier used to specify a particular page of web information is called a uniform resource locator (url).when a browser displays a page of information  it also displays the url for the page.in other words  urls are the standardized names for the internet’s resources.they are the resource locations that the browser needs to find pieces of electronic information.urls are the first human access point to the internet: a user points a browser at a url  the browser sends the appropriate protocol messages to get the resource that is requested.these are a subset of a more general class of resource identifier called a uniform resource identifier (uri).1  fig.1.how urls relate to browser  machine  server  and location on the server’s file-system  a url may seem like a nonsensical collection of letters and punctuation.however  the precise syntax conveys meaning that a browser can use to retrieve a particular page  with- out passing through other documents.each url uniformly identifies a unique page of information by giving the name of a remote computer  a server on that computer  and a specific page of information available from that server.rfc 1738 [1] specifies the syntax and semantics of urls  which is updated by rfc 3968 [,
